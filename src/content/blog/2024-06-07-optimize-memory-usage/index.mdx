---
slug: optimizing-memory-usage
title: 'How to Get the Most out of Postgres Memory Settings'
authors: [shaun]
description: |
    Best practices for configuring Postgres to maximize RAM use without risking process termination.
tags: [postgres, alerts, cloud, database, status, memory]
date: 2024-06-07T09:00
image: './header.png'
planetPostgres: true
---

import Callout from '../../../components/Callout.astro';

It's no secret that databases use _a lot_ of RAM. When Postgres needs to build a result set, a very common pattern is to match against an index, retrieve associated rows from one or more tables, and finally merge, filter, aggregate, and sort tuples into usable output. Every one of these steps relies on memory, and Postgres may handle thousands of such queries simultaneously.

Given the memory-hungry nature of databases like Postgres, how can we manage that voracious appetite to best utilize what's actually available? How can we prevent the operating system from defensively terminating a Postgres process that is using _too much_?

Read along to learn how Postgres uses memory, and what steps you can take to get the best bang for your RAM buck. It's extremely tempting to simply reach for that "upgrade instance" button, but there may be _another_ way.

## Sharing is Caring

The first and by far the largest segment of RAM associated with a Postgres service is called [shared buffers](https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-SHARED-BUFFERS). It represents the most frequently retrieved rows from all tables and indexes, backed by a scoring heuristic based on use frequency. Essentially the more often a page is referenced, the more likely it is already located in shared buffers and does not need to be requested from the operating system. It tends to look something like this:

![Segment of Shared Buffers](./shared-buffers.png)

It's a static value that must be allocated when Postgres starts, and it won't get any bigger, so we don't have to worry about this contributing to _unexpected_ memory problems. It's important to size this properly, because it can only be changed when Postgres starts or restarts.

<Callout variant='warning'>
While Postgres considers this memory allocated up-front, the operating system might not. As a result, it's possible to specify extremely high values up to the amount of RAM in the instance. This kind of dangerous over-allocation implicitly over-commits RAM since it does not account for other usage described in this article. Doing so will eventually lead to an out-of-memory scenario and a subsequent crash.
</Callout>

The default Postgres [page size](https://en.wikipedia.org/wiki/Page_\(computer_memory\)) is 8kb, and each page represents several index records or table rows. Before Postgres can use an index or row, it must first retrieve the corresponding page into RAM. If those pages are in shared buffers, their "score" increases and they're less likely to be evicted later. Given that we want as much data as possible to be cached by Postgres, it may seem that higher settings are _always_ better.

The default value Postgres uses for this parameter is 128MB. The most frequently cited recommendation for production systems is to use 25% of available RAM, which also happens to be what we use here at Tembo. This rule of thumb is an excellent starting point and works for most systems because it scales with hardware. A large database is likely exceptionally busy and requires many CPU threads, and these are usually accompanied by lots of RAM, especially in cloud environments. The busiest portion of the database tends to fit comfortably in this 25% in most cases, and everyone is happy.

What if we want a more precise value, however? How do we find the amount of RAM used by the most active tables and indexes? This is where the [`pg_buffercache`](https://www.postgresql.org/docs/current/pgbuffercache.html) extension comes in handy. It provides a view into the shared buffers so we can figure out exactly which tables and indexes are allocated, including the relative score of every page. For the purposes of tuning `shared_buffers`, we only really need to know how much of the buffer is full using a query like this:

```sql
WITH state AS (
  SELECT count(*) FILTER (WHERE relfilenode IS NOT NULL) AS used,
         count(*) FILTER (WHERE relfilenode IS NULL) AS empty,
         count(*) AS total
    FROM pg_buffercache
)
SELECT *, round(used * 1.0 / total * 100, 1) AS percent
  FROM state;
```

Such a query shows how many pages of the buffer are used out of the total. If our database has been running for a while and the buffer cache isn't 100% utilized, our setting may be too high and we can reduce the size of the instance or the value of `shared_buffers` to compensate. If it's at 100% and only small fractions of many tables are cached, it may be beneficial to assign sequentially higher values until we see diminishing returns. It's possible to perform this tuning without the aid of `pg_buffercache`, but it certainly eases the process!

If we start crossing past 50% of system RAM, we should consider increasing the size of the instance instead. Postgres still needs memory for user sessions and associated queries after all. Speaking of...

## Working Memory

The other half of the memory balancing act falls under the memory Postgres uses to actually get things done, known as work memory. The parameter that controls this is&mdash;appropriately enough&mdash;named [`work_mem`](https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-WORK-MEM). The default is 4MB and this is one of the first values a user might modify in an attempt to make a query run faster.

However, this name is actually something of a misnomer. Many interpret "work memory" as a single allocation assigned to all work Postgres might perform while working on a query, but it can actually be _far more than that_. Consider the simplified execution diagram for a fairly basic query which is retrieving all products ever ordered by a certain user, and then sorting the results by order line item:

![Example of query nodes](./query-nodes.png)

Each step requires combining only two items, and the product of that combination is then sent through further steps. Most databases refer to each of these steps as a "node", and queries are commonly comprised of multiple nodes. This example in particular requires four nodes to produce the end result. When considering memory usage in a Postgres server, it's **critically important** to understand that each node is allocated a **separate** instance of `work_mem`. So if we were using the `work_mem` default of 4MB, this query could consume up to 16MB of RAM.

If we have a moderately busy server with 100 of these queries running simultaneously, we could use up to 1.6GB of RAM simply to calculate results. More complex queries would require even more RAM depending on how many nodes are necessary to execute the query. The [`max_connections`](https://www.postgresql.org/docs/current/runtime-config-connection.html#GUC-MAX-CONNECTIONS) parameter also plays a part here, as it defines how many concurrent sessions may be executing a query.

<Callout variant='info'>
Use the [`EXPLAIN`](https://www.postgresql.org/docs/current/sql-explain.html) command to see the execution plan of a query. It will show how Postgres will execute the query and all of the nodes necessary to produce output. Used in conjunction with the `pg_stat_statements` extension, it becomes possible to isolate the most active queries and estimate overall memory use due to `work_mem`.
</Callout>

This is one of the major reasons behind the 25% recommendation for `shared_buffers`. Until a database is closely profiled for query concurrency and complexity, it may not be safe to allocate a higher value. To do so could risk the operating system rejecting memory requests or terminating Postgres itself.

Conversely, if `work_mem` is set too low, any rows or intermediate results that won't fit in RAM will spill to disk, which is orders of magnitude slower. Thankfully this is easy to detect by checking the `pg_stat_database` view with a query like this:

```sql
SELECT datname, pg_size_pretty(temp_bytes / temp_files) AS overflow
  FROM pg_stat_database
 WHERE temp_files > 0;

 datname  | overflow 
----------+----------
 mytestdb | 6017 kB
```

Postgres keeps track of the cumulative size and count of all temporary files written to disk. We can use that to find a coarse average and if the size is reasonable, we can increase `work_mem` by this amount. Afterwards, the majority of queries will no longer require disk storage as temporary working space. It's not uncommon for `work_mem` to be 2-4MB short of pure memory execution, resulting in queries that are much slower than they could be.

## Ongoing Maintenance

The last of the tunables for Postgres RAM usage is similar to work memory, but associated specifically with maintenance. Consequently it has a similar parameter name in [`maintenance_work_mem`](https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-MAINTENANCE-WORK-MEM). This specifies the amount of RAM dedicated to perform operations such as `VACUUM`, `CREATE INDEX`, and `ALTER TABLE ADD FOREIGN KEY`, with a default of 64MB.

Because this is limited to one operation per session, and it's unlikely many concurrent such actions will occur, it's considered safe enough to use higher values. It's very common to set this as high as 1GB or even 2GB, as these maintenance operations can be very memory intensive and complete much faster if they can operate entirely in RAM.

One important caveat here is the Postgres autovacuum process which marks dead tuples for later reuse. Autovacuum launches background tasks until the limit of [`autovacuum_max_workers`](https://www.postgresql.org/docs/current/runtime-config-autovacuum.html#GUC-AUTOVACUUM-MAX-WORKERS), and _each of these_ may use a full instance of `maintenance_work_mem`. Most servers with RAM to spare will be safe with 1GB of maintenance work memory, but if RAM is scarce, we should be more judicious. There's a specific [`autovacuum_work_mem`](https://www.postgresql.org/docs/current/runtime-config-autovacuum.html#GUC-AUTOVACUUM-MAX-WORKERS) parameter if we want to restrict the autovacuum workers in particular.

## Session Pooling

The easiest way to reduce memory consumption is to put a logical cap on potential allocations. Postgres is currently a process-based engine, meaning every user session is assigned a physical process rather than a thread. This means every connection comes with a certain amount of RAM overhead, and contributes to context switching. As a result, one common recommendation is to set `max_connections` no higher than 4x the amount of available CPU threads. This minimizes the amount of time spent switching active sessions between CPUs and naturally limits the amount of RAM sessions can collectively consume.

Remember, if every session is executing a query, and each node represents one allocation of `work_mem`, our theoretical maximum work memory usage is: `connections * nodes * work_mem`. It's not always possible to reduce query complexity, but we can _usually_ reduce connection count. In cases where applications always open a certain inflated amount of sessions, or a multiple separate microservices rely on Postgres, this may be easier said than done.

<Callout variant='info'>
Remember this formula: `work_mem * max_connections * 5`

It's a rough estimate of the maximum amount RAM a Postgres instance might allocate to user sessions simply for processing basic queries, assuming all connections are active. If the server doesn't have enough RAM for this value, consider decreasing one of the factors, or increasing RAM.
</Callout>

That means the next step is to introduce a [connection pooler](/docs/product/cloud/apps/connection-pooler/) like PgBouncer. This decouples client connections from the database and reuses the expensive Postgres sessions between them. When properly configured, several hundred clients can share a few dozen Postgres connections with no impact to the application.

We're left with something like this:

![Pgbouncer connection multiplexing](./pgbouncer-stack.png)

We've seen PgBouncer multiplex over 1000 connections to 40-50 this way, dramatically reducing the overall amount of memory consumption caused by process overhead.

## Reducing Bloat

Quite possibly the most difficult aspect of memory usage to track is that of table bloat. Postgres uses Multi-Version Concurrency Control ([MVCC](https://www.postgresql.org/docs/current/mvcc.html)) to represent data in its storage system. This means any time a table row is modified, Postgres creates another copy of the row somewhere in the table, marked with a new version number. The Postgres [`VACUUM`](https://www.postgresql.org/docs/current/routine-vacuuming.html) process marks old row versions as "unused" space so that new row versions can be placed there.

Postgres has an automatic vacuum background process that continuously finds these reusable allocations and tries to make sure tables don't grow without bound. Sometimes the default configuration for this is not sufficient for especially high volume systems however, and such maintenance can fall behind. As a result, tables may slowly fill with more _dead_ rows than _live_ ones, resulting in a table that is "bloated" full of old data. 

<Callout variant='info'>
We have an article on [optimizing autovacuum](/blog/optimizing-postgres-auto-vacuum/) for information on addressing this issue. Properly tuning autovacuum is an incredibly subtle topic that deserves an equally thorough explanation. Even experienced Postgres DBAs could do with an occasional refresher.
</Callout>

Consider what this means for our shared buffers if a table is extremely bloated. Remember our earlier depiction of shared buffers? What happens if each page only contains one live row and several dead rows? We'd end up in a situation like this:

![Bloat Tables in Shared Buffers](./shared-buffers-bloat.png)

If a particular query requires 10 rows in this scenario, we'd have to fetch 10 pages into shared buffers and end up wasting a lot of memory that could have been used for something else. If these rows are in particularly high demand, the usage count would keep them in shared buffers and make our cache much less efficient.

There are a lot of queries floating around the internet which can estimate table bloat, but the only way to get a concrete view of how pages look in a table is to use the [`pgstattuple`](https://www.postgresql.org/docs/current/pgstattuple.html) extension. Once that extension is installed, we can see _exactly_ how bloated a particular table may be using a query like this:

```sql
SELECT tuple_percent, free_percent
  FROM pgstattuple('my_table');
```

If the `free_percent` is greater than 30%, it may be necessary to modify autovacuum to be more aggressive. If it's _significantly_ greater than 30%, it's probably a good idea to remove the bloat entirely. Unfortunately there's currently no easy way to do this. Currently the only supported method is to essentially rebuild the table using a command like this:

```sql
VACUUM FULL my_table;
```

A `VACUUM FULL` rewrites the entire table by relocating all live rows to a new location and discards the old bloated copy. This process allocates an access exclusive lock for its duration, so requires some kind of downtime in almost all cases.

An alternative to this is the [`pg_repack`](https://reorg.github.io/pg_repack/) extension supported by Tembo. This command-line tool can also reorganize a table to remove bloat, but does so without an exclusive lock and in an entirely online manner. Because this tool exists outside the Postgres core and modifies table and index storage, it's often considered advanced usage. We recommend copious tests in a non-production environment before using it.

## The Balancing Act

Getting all of these parameters and resources configured properly is both an art and a science. We've seen how we can measure the actual usage of our shared buffers, and we can determine if work memory is too low. But what if&mdash;as in most cases&mdash;we're constrained by our available hardware or budget? As a good example of this, what happens if we encounter a memory usage alert or the operating system terminated our service in the past? This is where the "art" comes in.

In some situations where memory is scarce, we may need to reduce `shared_buffers` slightly to make room for more `work_mem`. Or perhaps we must reduce both. If our application requires high session counts, it may make more sense to reduce `work_mem` or introduce a connection pool to prevent concurrent sessions from racking up extensive RAM allocations. It may make more sense to reduce `maintenance_work_mem` if we'd increased it in the past under the assumption we had sufficient RAM for everything.

We recommend following this order of operations to both maximize memory use and avoid resource exhaustion:

1. Add a connection pooler and reduce `max_connections`. It's often the quickest and easiest way to cut maximum resource consumption.
2. Reduce `work_mem` by 10%, and repeat as necessary, but no lower than 2MB. This parameter is multiplicative, and can quickly account for more RAM usage than any other setting.
3. Reduce `maintenance_work_mem` back to the default of 64MB and consider adding increments of 8MB at a time. This may slow down maintenance slightly, but is preferable to the alternative of exhausting system RAM.
4. Use the `pg_buffercache` extension to see how much of which tables are stored in `shared_buffers`. Examine each table and index closely and see if there's a way to reduce this by archiving data, revising queries to use less information, and so on. This may also include `VACUUM FULL` or `pg_repack` to compact pages used by active bloated tables.
5. Reduce `shared_buffers` by 10%, and repeat as necessary. The common recommendation of 25% of system RAM may simply be too much for certain workloads. Shared buffers work alongside operating system and filesystem caches, but can't be purged in the same way when memory pressure is high. Rather than Postgres having direct knowledge of cached pages, smaller shared buffers rely on the operating system to cache frequently used data. As such, it can be a good option when RAM is scarce.
6. Provision more capable hardware. This could be purchasing a larger server or moving up to the next larger cloud instance size. If the database truly needs more RAM for its current workload and reducing the above parameters would adversely affect system performance too dramatically, it usually makes more sense to upgrade.

As you can see, upgrading doesn't have to the only, or even the _first_ option to consider when configuring a Postgres cluster for optimal memory use. A busier and more popular application will require more database resources in general, but in many cases there is plenty of runway to leverage between hardware upgrades.

In the end, we hope you learned a bit about how Postgres uses memory, and the best ways to ensure a well-balanced database instance on _any_ hardware allocation. Postgres performance is great out of the box, but it's even better with a bit of informed TLC!


