slug: open-source-tiering
title: 'Open and Accessible Data Tiering'
authors: []
description: |
	Open source data tiering project. 
tags: [postgres, workloads]
image: ./community-tiering.png 
date:
planetPostgres: 
---

While appraising the value of data, one mustn't only consider their literal measurements, but the resources dedicated to their lifecycle management as well. Today, users with large data volumes find themselves incurring high costs as their data scales; and what might have started as an attractive subscription fee, quickly becomes difficult to justify. One answer to such a problem focuses not so much on the amount of data, but how the data is stored.

At Tembo, we've heard this pain point come up frequently, both in internal and community conversations. This was such a pervasive topic, in fact, that we decided to build and open source a community solution, `pg_tier`; a PostgreSQL extension designed to streamline the developer interaction with AWS S3 and other object stores.
With this tool, users have the power to push a Postgres table to an S3 bucket, for example, while retaining the ability to query it as if it were still in Postgres.

## A primer on data lifecycle management

While we find this exciting in its own right, it's also important to appreciate the context within which this extension operates. That is to say, the fundamentals of data lifecycle management and some example use cases.

As data progresses through the various stages of its lifecycle, so too do its access patterns change.
While not always the case, it's safe to assume that as data ages, its frequency of access decreases.

Pg_tier is envisioned to provide a substantial solution towards data management when it enters the less accessed usage stage (aged data stage) to rarely accessed stage (archival stage).

### Aged Data Stage:

Older data sets that are not frequently accessed, but are expected to be available for immediate access. Pg_tier moves the data to the object store as a parquet file, generates foreign data wrapper meta to enable transparent access through sql queries. Parquet file is compressed and analytical friendly format.

### Archival Stage:

Object store tiering has evolved as an alternative or complement to traditional data archiving methods. Traditional archive solutions often offer lower storage costs compared to standard storage options, as they are designed for long term retention of data. However, these solutions incur higher cost of data retrieval or access. On the other hand, pg_tier provides bottomless storage with low cost of data access.

## Everyone can have bottomless storage on Postgres

We love Postgres, but while storage volumes such as Elastic Block Store (in the case of AWS) might appear convenient, they are very costly relative to their object storage alternatives (such as S3). Not only that, but they are difficult to scale (and typically cannot scale down).

As an organization, it's important to distinguish between data that needs to be "hot and ready" versus tucked away into "colder" storage.
Indeed, this is already common practice; engineers working with Postgres are archiving data via simple copying to S3 and deleting from Postgres.
Great! Storage utilization is decreased and data is retained in a more cost-effective storage solution.

But what about when you want to query that data?
There are certainly tools that allow you to do this, such as [DuckDB](https://duckdb.org/), [Apache Pinot](https://pinot.apache.org/), or [ClickHouse](https://clickhouse.com/).
That said, users will find that they will have to build a significant amount of the pipeline themselves, including moving the data to S3, to then integrate the S3 data into their tool of choice.
The goal of `pg_tier`is to make this a standardized process, across all object storage formats and cloud providers, with a first class experience on Postgres.

## Contributing to parquet_s3_fdw for a touch-free experience

While pg_tier is a relatively new project, it heavily depends on long-standing and popular project: parquet_s3_fdw. pg_tier uses parquet_s3_fdw to create a foreign data wrapper around data in S3.
This is the key piece of technology that allows you to query the data in S3 as if it were in Postgres.

In order to make the experience best for cloud users, Tembo contributed the ability for parquet_s3_fdw to fetch cloud credentials from the environment.
With this feature, you can inject AWS credentials into the Postgres environment, then parquet_s3_fdw will be able to use those credentials to authenticate with and configuring object storage.
We are currently running a public fork of parquet_s3_fdw in Tembo Cloud so that you can use this feature today.

Here's how you do this on Tembo Cloud:

Create an example table, and give it some data

```sql
CREATE table people (
    name text not null,
    age numeric not null
);
```
```sql
INSERT into people values ('Alice', 34), ('Bob', 45), ('Charlie', 56);
```

Call call tier.table() on the table:

```sql
SELECT tier.table('people');
```

That table is now a foreign table, with the FDW storage in S3:

```sql
\d+ people
```
```text
                                      Foreign table "public.people"
 Column |  Type   | Collation | Nullable | Default | FDW options  | Storage  | Stats target | Description
--------+---------+-----------+----------+---------+--------------+----------+--------------+-------------
 name   | text    |           | not null |         | (key 'true') | extended |              |
 age    | numeric |           | not null |         | (key 'true') | main     |              |
Server: pg_tier_s3_srv
FDW options: (dirname 's3://cdb-plat-use1-prod-instance-storage/v2/reservedly-lovable-sheepdog/public_people/')
```

Every Tembo Cloud instance has it's own private and dedicated S3 bucket.
When you run tier.table(), the table is written to this bucket in parquet format and the current table converted into a foreign table (via parquet_s3_fdw).
When you're running on Tembo Cloud, this means you don't need to bring your own AWS account.

## Bring your own bucket

If you're running this extension on your own or want to use your own bucket, you have just one extra-step: set the credentials and bucket configuration.

```sql
select tier.set_tier_config(
	'my-storage-bucket',
	'AWS_ACCESS_KEY', 
	'AWS_SECRET_KEY',
	'AWS_REGION'
);
``
```sql
select tier.table('my-table');
```

```sql
\d people
```
```text
                  Foreign table "public.people"
 Column |  Type   | Collation | Nullable | Default | FDW options
--------+---------+-----------+----------+---------+--------------
 name   | text    |           | not null |         | (key 'true')
 age    | numeric |           | not null |         | (key 'true')
Server: pg_tier_s3_srv
FDW options: (dirname 's3://my-storage-bucket/public_people/')
```

## Try it on Tembo Cloud

pg_tier is PostgreSQL licensed and available today on Tembo Cloud in the Data Warehouse Stack. Try anywhere with Docker, or install it with Trunk or from source, and via .
