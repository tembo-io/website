"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[7250],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>g});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},p=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},d="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),d=c(n),m=o,g=d["".concat(l,".").concat(m)]||d[m]||h[m]||r;return n?a.createElement(g,s(s({ref:t},p),{},{components:n})):a.createElement(g,s({ref:t},p))}));function g(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,s=new Array(r);s[0]=m;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i[d]="string"==typeof e?e:o,s[1]=i;for(var c=2;c<r;c++)s[c]=n[c];return a.createElement.apply(null,s)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},12376:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>c});var a=n(87462),o=(n(67294),n(3905));const r={slug:"pgvector-and-embedding-solutions-with-postgres",title:"Unleashing the power of vector embeddings with PostgreSQL",authors:["rjzv"],tags:["postgres","extensions","embedding","vector","pgvector"],image:"./RAG.png"},s=void 0,i={permalink:"/blog/pgvector-and-embedding-solutions-with-postgres",editUrl:"https://github.com/tembo-io/website/blob/main/blog/2023-10-18-pgvector-and-embeddings/index.md",source:"@site/blog/2023-10-18-pgvector-and-embeddings/index.md",title:"Unleashing the power of vector embeddings with PostgreSQL",description:"Language models are like the wizards of the digital world, conjuring up text that sounds eerily human. These marvels of artificial intelligence, such as GPT-3.5, are sophisticated algorithms that have been trained on vast swathes of text from the internet. They can understand context, generate coherent paragraphs, translate languages, and even assist in tasks like writing, chatbots, and more. Think of them as your trusty digital scribe, ready to assist with their textual sorcery whenever you summon them.",date:"2023-10-18T00:00:00.000Z",formattedDate:"October 18, 2023",tags:[{label:"postgres",permalink:"/blog/tags/postgres"},{label:"extensions",permalink:"/blog/tags/extensions"},{label:"embedding",permalink:"/blog/tags/embedding"},{label:"vector",permalink:"/blog/tags/vector"},{label:"pgvector",permalink:"/blog/tags/pgvector"}],readingTime:7.955,hasTruncateMarker:!1,authors:[{name:"Binidxaba",title:"Community contributor",url:"https://github.com/binidxaba",email:"noreply@tembo.io",imageURL:"https://github.com/binidxaba.png",key:"rjzv"}],frontMatter:{slug:"pgvector-and-embedding-solutions-with-postgres",title:"Unleashing the power of vector embeddings with PostgreSQL",authors:["rjzv"],tags:["postgres","extensions","embedding","vector","pgvector"],image:"./RAG.png"},prevItem:{title:"Hacking Postgres, Ep. 2: Adam Hendel",permalink:"/blog/hacking-postgres-ep2"},nextItem:{title:"Hacking Postgres Ep. 1: Marco Slot",permalink:"/blog/hacking-postgres-ep1"}},l={image:n(61493).Z,authorsImageUrls:[void 0]},c=[{value:"From words to vectors",id:"from-words-to-vectors",level:2},{value:"Postgres meets Language Models",id:"postgres-meets-language-models",level:2},{value:"pgvector: Postgres as a vector database",id:"pgvector-postgres-as-a-vector-database",level:2},{value:"Are these sentences similar?",id:"are-these-sentences-similar",level:2},{value:"Pgvector(ize)?",id:"pgvectorize",level:2},{value:"To wrap up...",id:"to-wrap-up",level:2},{value:"Disclaimer",id:"disclaimer",level:2}],p={toc:c},d="wrapper";function h(e){let{components:t,...r}=e;return(0,o.kt)(d,(0,a.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"Language models are like the wizards of the digital world, conjuring up text that sounds eerily human. These marvels of artificial intelligence, such as GPT-3.5, are sophisticated algorithms that have been trained on vast swathes of text from the internet. They can understand context, generate coherent paragraphs, translate languages, and even assist in tasks like writing, chatbots, and more. Think of them as your trusty digital scribe, ready to assist with their textual sorcery whenever you summon them.")),(0,o.kt)("p",null,"If you have used ChatGPT in the past, you probably were able to suspect that the previous paragraph was generated using it. And that's true. See the prompt ",(0,o.kt)("a",{parentName:"p",href:"https://chat.openai.com/share/9fab8ac9-6e34-481d-a281-db2f00b0f7f5"},"here"),"."),(0,o.kt)("p",null,"From the example above, you can witness the eloquence LLMs are capable of. Some people have been shocked so much that they became convinced that these ",(0,o.kt)("a",{parentName:"p",href:"https://www.scientificamerican.com/article/google-engineer-claims-ai-chatbot-is-sentient-why-that-matters/"},"models were sentient"),". However, in the end, they are nothing but a large, complex series of ",(0,o.kt)("a",{parentName:"p",href:"https://www.youtube.com/watch?v=bCz4OMemCcA"},"matrix and vector operations"),". These matrices and vectors have been trained to represent the semantic meaning of words."),(0,o.kt)("p",null,"In today's post, we will explore these meaning vectors and how they are related to Postgres. In particular, we are going to play with sentence transformers, vectors, and similarity search. All of that with the help of the pgvector Postgres extension."),(0,o.kt)("p",null,"Let\u2019s go!"),(0,o.kt)("h2",{id:"from-words-to-vectors"},"From words to vectors"),(0,o.kt)("p",null,"Like we said, a vector can represent many things, for example, the position of a character in a 3D video game, the position of a pixel in your screen, the force applied to an object, a color in the RGB space, or even the meaning of a word\u2026"),(0,o.kt)("p",null,"Word embedding refers to the technique by which words can be represented as vectors. These days, the embeddings offered by ",(0,o.kt)("a",{parentName:"p",href:"https://openai.com/blog/new-and-improved-embedding-model"},"OpenAI")," are very popular. However, other alternatives exist, like ",(0,o.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1301.3781"},"word2vect"),", ",(0,o.kt)("a",{parentName:"p",href:"https://aclanthology.org/D14-1162.pdf"},"Glove"),", ",(0,o.kt)("a",{parentName:"p",href:"https://fasttext.cc/docs/en/support.html"},"FastText"),", and ",(0,o.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1802.05365v2"},"ELMo"),"."),(0,o.kt)("p",null,"Similarly, entire sentences can be represented as vectors using ",(0,o.kt)("a",{parentName:"p",href:"https://platform.openai.com/docs/guides/embeddings/what-are-embeddings"},"OpenAI embeddings")," or ",(0,o.kt)("a",{parentName:"p",href:"https://sbert.net/"},"SentenceTransformers"),", for example."),(0,o.kt)("p",null,"These models can be accessed through libraries for different languages. The following Python snippet shows how to obtain the vector embeddings of three sentences using SentenceTransformer:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nsentences = ['SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings.',\n             'Pgvector is postgres extension for vector similarity search.',\n             'Tembo will help you say goodby to database sprawl, and hello to Postgres.']\n\nsentence_embeddings = model.encode(sentences)\n\nfor sentence, embedding in zip(sentences, sentence_embeddings):\n    print(\"Sentence:\", sentence)\n    print(\"Embedding:\", embedding)\n    print(\"\")\n")),(0,o.kt)("admonition",{type:"note"},(0,o.kt)("p",{parentName:"admonition"},"The code used in this blog post can be found in ",(0,o.kt)("a",{parentName:"p",href:"https://gist.github.com/binidxaba/2eb3bff573c6be700e4391d650a302db"},"this gist"),".")),(0,o.kt)("p",null,"The mind-blowing part is that words and sentences with a similar meaning will have similar vectors. This characteristic is the basis of a search technique called similarity search, where we simply find the nearest embedding vectors to find texts that are similar to our query."),(0,o.kt)("h2",{id:"postgres-meets-language-models"},"Postgres meets Language Models"),(0,o.kt)("p",null,"Models are great at generating content that seems credible, as shown earlier. However, you may have experienced cases where ChatGPT hallucinates answers or delivers out-of-date information. That's because LLMs are ",(0,o.kt)("strong",{parentName:"p"},"pre-trained")," using ",(0,o.kt)("strong",{parentName:"p"},"general data"),". And, because of that, creating a chatbot based only on the pre-trained data wouldn't be helpful for your customers, for instance."),(0,o.kt)("p",null,"The concept of ",(0,o.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2005.11401"},"RAG (Retrieval-Augmented Generation)")," acknowledges this limitation. "),(0,o.kt)("p",null,"One way of overcoming these problems is to store your company's knowledge base in a database.... preferably in a vector database. You could then query related content and feed that content to the LLM of your preference."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"RAG with pgvector",src:n(61493).Z,width:"450",height:"288"})),(0,o.kt)("p",null,"Specialized vector databases include ",(0,o.kt)("a",{parentName:"p",href:"https://milvus.io/"},"Milvus"),", ",(0,o.kt)("a",{parentName:"p",href:"https://qdrant.tech/"},"Qdrant"),", ",(0,o.kt)("a",{parentName:"p",href:"https://weaviate.io/"},"Weaviate"),", and ",(0,o.kt)("a",{parentName:"p",href:"https://www.pinecone.io/"},"Pinecone"),". However, you probably want to ",(0,o.kt)("a",{parentName:"p",href:"https://www.amazingcto.com/postgres-for-everything/"},"stick to your Postgres database"),". "),(0,o.kt)("p",null,"Postgres is not in itself a vector database, but extensions can come to the rescue one more time... This time with ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/pgvector/pgvector"},(0,o.kt)("strong",{parentName:"a"},"pgvector")),"."),(0,o.kt)("p",null,"Let's use it and explore how we would query related content from a Postgres database."),(0,o.kt)("h2",{id:"pgvector-postgres-as-a-vector-database"},"pgvector: Postgres as a vector database"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://github.com/pgvector/pgvector"},"pgvector")," is a Postgres extension that helps work with vectors and stores them in your postgres database. It offers functions for calculating the distance between vectors and for similarity search."),(0,o.kt)("p",null,"For the following demo, I converted all of Tembo\u2019s blogs into document vectors using the following Python script that uses the ",(0,o.kt)("a",{parentName:"p",href:"https://python.langchain.com"},"langchain framework"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.document_loaders import TextLoader\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores.pgvector import PGVector\n\nimport os\n\n\nCONNECTION_STRING = \"postgresql+psycopg2://postgres:password@localhost:5432/vector_db\"\nCOLLECTION_NAME = 'my_collection'\n\nembeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 20)\n\nfiles = os.listdir('./corpus')\n\nfor file in files:\n    file_path = f\"./corpus/{file}\"\n    print(f\"Loading: {file_path}\")\n    loader = TextLoader(file_path)\n    document = loader.load()\n    texts = text_splitter.split_documents(document)\n    sentence_embeddings = embeddings.embed_documents([t.page_content for t in texts[:5]])\n\n    db = PGVector.from_documents(\n            embedding=embeddings,\n            documents=texts,\n            collection_name=COLLECTION_NAME,\n            connection_string=CONNECTION_STRING)\n")),(0,o.kt)("p",null,"It basically loads each document and then inserts them into Postgres using the ",(0,o.kt)("a",{parentName:"p",href:"https://python.langchain.com/docs/integrations/vectorstores/pgvector"},(0,o.kt)("inlineCode",{parentName:"a"},"PGVector")," class"),". As a result, in my Postgres database called ",(0,o.kt)("inlineCode",{parentName:"p"},"vector_db"),", I got two tables:"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Show tables",src:n(46783).Z,width:"434",height:"177"})),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"langchain_pg_collection"),": contains information about all collections."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"langchain_pg_embedding"),": contains all the resulting vectors.")),(0,o.kt)("p",null,"The following picture shows part of the contents of (2):"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Show vectors",src:n(77793).Z,width:"1470",height:"465"})),(0,o.kt)("p",null,"The resulting vectors have ",(0,o.kt)("a",{parentName:"p",href:"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"},"384 dimensions"),".  "),(0,o.kt)("h2",{id:"are-these-sentences-similar"},"Are these sentences similar?"),(0,o.kt)("p",null,"Let\u2019s now play with these vectors."),(0,o.kt)("p",null,"Using pgvector we can search content that is similar to a query. For example, we can find content related to ",(0,o.kt)("inlineCode",{parentName:"p"},"postgres 16"),"."),(0,o.kt)("p",null,"First, we can obtain a vector that represents a query:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from langchain.embeddings import HuggingFaceEmbeddings\n\nembeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\nprint embeddings.embed_query(\u201cWhat is new in postgres 16\")\n")),(0,o.kt)("p",null,"Then we can search vectors stored in the database that are similar to the query vector. The tool for that is ",(0,o.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Cosine_similarity"},(0,o.kt)("inlineCode",{parentName:"a"},"cosine distance")),", which in pgvector is represented with the ",(0,o.kt)("inlineCode",{parentName:"p"},"<=>")," operator:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT document, 1-(embedding <=> '[<your_vector_here>]') as cosine_similarity\nFROM langchain_pg_embedding\nORDER BY cosine_similarity DESC\nLIMIT 2;\n")),(0,o.kt)("p",null,"The above query retrieves vectors/chunks of text ordered by how close they are (in terms of ",(0,o.kt)("inlineCode",{parentName:"p"},"cosine distance"),") to the query vector. In my case, the most similar chunk of text was:"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"In case you missed it, Postgres 16 came out last week - and this year it\narrived earlier than the last few years. There are many features that\nI\u2019ve been looking forward to for the last few months and I\u2019m excited to\nsee them get into the hands of users. Before we dive into the specific\nfeatures of this release, let\u2019s discuss what a Postgres major release\nactually means."),(0,o.kt)("p",{parentName:"blockquote"},"[postgres-16]"),(0,o.kt)("p",{parentName:"blockquote"},"Postgres Releases"),(0,o.kt)("p",{parentName:"blockquote"},"The PostgreSQL Global Development Group releases a new major version\nevery year with new features."),(0,o.kt)("p",{parentName:"blockquote"},"In addition, Postgres releases minor versions of each major release\nevery 3 months or so with bug fixes and security fixes. No new features\nare released in minor versions, and that\u2019s what makes major version\nreleases so exciting as it\u2019s the culmination of about a year\u2019s worth of\ndevelopment work on the project.")),(0,o.kt)("p",null,"Which is an excerpt from ",(0,o.kt)("a",{parentName:"p",href:"https://tembo.io/blog/postgres-16"},"Postgres 16: The exciting and the unnoticed"),"."),(0,o.kt)("p",null,"Let us look at what Postgres is doing behind the scenes, using ",(0,o.kt)("inlineCode",{parentName:"p"},"explain analyze"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-console"},"Limit  (cost=28.07..28.08 rows=2 width=641) (actual time=1.069..1.071 rows=2 loops=1)\n   ->  Sort  (cost=28.07..28.53 rows=181 width=641) (actual time=1.067..1.068 rows=2 loops=1)\n         Sort Key: ((embedding <=> '[<your_vector>]'::vector))\n         Sort Method: top-N heapsort  Memory: 28kB\n         ->  Seq Scan on langchain_pg_embedding  (cost=0.00..26.26 rows=181 width=641) (actual time=0.036..0.953 rows=181 loops=1)\n Planning Time: 0.079 ms\n Execution Time: 1.093 ms\n(7 rows)\n\n\n")),(0,o.kt)("p",null,"We can observe that Postgres is sequentially scanning all rows. Then it computes the ",(0,o.kt)("inlineCode",{parentName:"p"},"cosine distance")," for all those rows and sorts them. Finally,  it takes the first two rows."),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"sequential scan")," could be avoided if we had an index. Indeed, we can create one thanks to pgvector, for example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sql"},"alter table langchain_pg_embedding alter column embedding type vector(384);\n\nCREATE INDEX ON langchain_pg_embedding  USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-console"}," Limit  (cost=5.01..5.11 rows=2 width=641) (actual time=0.175..0.179 rows=1 loops=1)\n   ->  Index Scan using langchain_pg_embedding_embedding_idx2 on langchain_pg_embedding  (cost=5.01..13.49 rows=181 width=641) (actual time=0.172..0.175 rows=1 loops=1)\n         Order By: (embedding <=> '[<your_vector>]'::vector)\n Planning Time: 0.154 ms\n Execution Time: 0.224 ms\n(5 rows)\n")),(0,o.kt)("p",null,"One thing to keep in mind is that these indexes are used for ",(0,o.kt)("inlineCode",{parentName:"p"},"approximate nearest neighbor search"),". We\u2019ll explore what that means in a future blog post. ",(0,o.kt)("a",{parentName:"p",href:"https://twitter.com/tembo_io"},"Let us know")," if that would be interesting for you."),(0,o.kt)("h2",{id:"pgvectorize"},"Pgvector(ize)?"),(0,o.kt)("p",null,"Ok, at this point you should now have a sense of what pgvector is, and how to use it together with Python. However, wouldn't it be great if the vectorizing step could happen all within Postgres?"),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://github.com/tembo-io/pg_vectorize"},(0,o.kt)("strong",{parentName:"a"},"Pg_vectorize"))," is an extension being developed by ",(0,o.kt)("strong",{parentName:"p"},"Tembo")," that intends to streamline the process of generating vectors from the data in your Postgres tables. It uses a background worker to generate and update the embeddings in batches every ",(0,o.kt)("em",{parentName:"p"},"N")," seconds. Also, if you need to find similar vectors, the extension can do that. All within Postgres. Isn't that a cool idea? "),(0,o.kt)("p",null,"I invite you to check out the repository and stay tuned."),(0,o.kt)("h2",{id:"to-wrap-up"},"To wrap up..."),(0,o.kt)("p",null,"In this post, we briefly discussed the concept of ",(0,o.kt)("inlineCode",{parentName:"p"},"embeddings"),", why they are important, and how they can be generated using one of the multiple available libraries. We also explored how to store and query the resulting vectors using Postgres and the pgvector extension."),(0,o.kt)("p",null,"These concepts are relevant to leveraging a knowledge base in conjunction with LLMs in an emerging technique called RAG. Of course, when implementing a real-life solution, ",(0,o.kt)("a",{parentName:"p",href:"ttps://medium.com/@neum_ai/retrieval-augmented-generation-at-scale-building-a-distributed-system-for-synchronizing-and-eaa29162521"},"more factors need to be considered"),", and this post was just an introduction."),(0,o.kt)("p",null,"I invite everyone to try out pgvector (e.g. using the scripts in this post), and the different operations that it offers. Also, can you think of other uses of pgvector? Let us know your thoughts in ",(0,o.kt)("a",{parentName:"p",href:"https://twitter.com/tembo_io"},"@tembo_io"),"."),(0,o.kt)("h2",{id:"disclaimer"},"Disclaimer"),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"The first paragraph in this blog post was generated using ChatGPT. ",(0,o.kt)("a",{parentName:"em",href:"https://chat.openai.com/share/9fab8ac9-6e34-481d-a281-db2f00b0f7f5"},"Here\u2019s the prompt"))))}h.isMDXComponent=!0},61493:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/RAG-cd4c28ec5c956f11e8de9ccf5bde2db2.png"},77793:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/select-vectors-8523ebc09c243b1bc0987332c2225b8b.png"},46783:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/table-list-3c5b0f371564f4dd0af86bcf7d14e209.png"}}]);