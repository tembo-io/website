"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1477],{30010:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"hacking-postgres-ep8","metadata":{"permalink":"/blog/hacking-postgres-ep8","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-11-10-hacking-postgres-ep8/index.md","source":"@site/blog/2023-11-10-hacking-postgres-ep8/index.md","title":"Hacking Postgres, Ep. 8: Philippe No\xebl","description":"Search is simple in theory. In practice? Anything but. In today\u2019s Episode 8 of Hacking Postgres, Ry sits down with Philippe No\xebl of ParadeDB to talk about how search is evolving, the influence of AI, and the lessons you\'d tell your younger self.","date":"2023-11-10T00:00:00.000Z","formattedDate":"November 10, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"hacking_postgres","permalink":"/blog/tags/hacking-postgres"}],"readingTime":31.5,"hasTruncateMarker":false,"authors":[{"name":"Eric Ankenman","url":"https://github.com/ericankenman","email":"noreply@tembo.io","imageURL":"https://github.com/ericankenman.png","key":"eric"}],"frontMatter":{"slug":"hacking-postgres-ep8","title":"Hacking Postgres, Ep. 8: Philippe No\xebl","authors":["eric"],"tags":["postgres","hacking_postgres"]},"nextItem":{"title":"PGMQ: Lightweight Message Queue on Postgres with No Background Worker","permalink":"/blog/pgmq-self-regulating-queue"}},"content":"Search is simple in theory. In practice? Anything but. In today\u2019s Episode 8 of Hacking Postgres, Ry sits down with Philippe No\xebl of ParadeDB to talk about how search is evolving, the influence of AI, and the lessons you\'d tell your younger self.\\n\\nWatch below, or listen on Apple/Spotify (or your podcast platform of choice). Special thanks to Regina and Paul for joining us today!\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/o73EMG1c3cA?si=ypM2zuSnQD-elBSB\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n\\nWant to know more about something they mentioned? Here\u2019s a starting point:\\n\\n\\n\\n* ParadeDB - [https://www.paradedb.com/](https://www.paradedb.com/)\\n* pg_bm25 - [https://docs.paradedb.com/blog/introducing_bm25](https://docs.paradedb.com/blog/introducing_bm25)\\n* pgvector - [https://github.com/pgvector/pgvector](https://github.com/pgvector/pgvector)\\n* pgrx - https://github.com/pgcentralfoundation/pgrx\\n* Elastic - [https://www.elastic.co/](https://www.elastic.co/)\\n* pg_search - [https://github.com/Casecommons/pg_search](https://github.com/Casecommons/pg_search)\\n* zombodb - [https://github.com/zombodb/zombodb](https://github.com/zombodb/zombodb)\\n* Algolia - [https://www.algolia.com/](https://www.algolia.com/)\\n* InterDB - https://www.interdb.jp/\\n\\nDid you enjoy the episode? Have ideas for someone else we should invite? Let us know your thoughts on X at @tembo_io or share them with the team in our [Slack Community](https://join.slack.com/t/tembocommunity/shared_invite/zt-23o25qt91-AnZoC1jhLMLubwia4GeNGw).\\n\\n\\n## Transcript\\n\\n\\n##### [00:00:12] - Ry\\n\\nWelcome to Hacking Postgres, an interview podcast with the people working on open source projects around Postgres, which in my opinion is the world\'s best open source database. I\'m Rye Walker, founder of Tembo, a managed Postgres company, and today I have Phil Noel from ParadeDB, working on ParadeDB as my guest. Phil, welcome to the show.\\n\\n\\n##### [00:00:37] - Phil\\n\\nThanks. Yeah, thanks. Nice to meet you. Thanks for having me.\\n\\n\\n##### [00:00:42] - Ry\\n\\nGreat to actually meet you. I\'d like to start...maybe you could give us a quick background, like maybe where\'d you grow up in the world? And what were you doing before?\\n\\n\\n##### [00:00:53] - Phil\\n\\nYeah, absolutely. Happy to. So I\'m originally from Quebec City, the French part of Canada. I actually grew up in this small town called [inaudible 01:02] , which is 2 hours East. Spent most of my life there, and then eventually I left to go to university. I stayed in Boston, studied computer science and neuroscience, did a few things. I started a browser company before ParadeDB, which I ran for about three years, and then after that got acquaintance with the joys of Postgres, you could say through that experience. Sounds somewhat similar to yours, maybe, although not to the same scale. And my previous co founder and I only started ParadeDB.\\n\\n\\n##### [00:01:35] - Ry\\n\\nNice. Tell the browser company, what was that called and how far did you get in that project?\\n\\n\\n##### [00:01:43] - Phil\\n\\nOh, that is both a short and a long story. It was called Wisp. We were building like a cloud based web browser, so the initial idea was to offload heavy processing to the cloud by streaming, which is like an age old idea, but we were young and dumb, so we thought we had invented something. We did that for a while, raised a couple of rounds of funding, grew a team to like 25 people, but we never quite found PMF and eventually pivoted to cybersecurity. Did that for a while, but it was a much better market, but not our battle to win. So we closed down the company. Yeah.\\n\\n\\n##### [00:02:21] - Ry\\n\\nOkay. I\'m sure we both have a lot of battle stories we could commiserate over. Do you remember when you first started using Postgres? I can\'t remember. That\'s why I ask this question, because it kind of just happened at some point in my career. But do you remember specifically when you started with Postgres?\\n\\n\\n##### [00:02:45] - Phil\\n\\nI don\'t know. Actually. That\'s a good question. I don\'t know. As a user? I don\'t know. As a developer since earlier this year or last year.\\n\\n\\n##### [00:02:58] - Ry\\n\\nWell, cool. Obviously as a user, you\'ve used it for years, I\'m sure. Tell me about your Postgres work. What have you built in the past and what are you building now? Or is this all just the first thing?\\n\\n\\n##### [00:03:12] - Phil\\n\\nI guess I would say it\'s the first major thing. So we can keep it to that, what we\'re doing. So we\'re building ParadeDB. ParadeDB is like a Postgres database, essentially, or Postgres extension, where we\'re integrating native full text search within Postgres, which is something that some people are quick to jump to say this already exists, and some people are quick to jump to say the one that exists is very bad. And so they\'re excited about what we\'re doing. So it depends which camp you fall on. But the first thing we released is this extension is called pg_bm25, where we\'re essentially integrating proper full text search within Postgres and then combine this with some of the existing innovations that\'s come, like pgvector to build hybrid search. And our goal is to build this Postgres type database that is sort of the go to choice for companies where search is critical to the product they\'re building. \\n\\n\\n##### [00:04:08] - Ry\\n\\nNice. Why do you think you\'re working on this? What led you here?\\n\\n\\n##### [00:04:14] - Phil\\n\\nYeah, that\'s a good question, actually. This is a problem we face ourselves and that\'s kind of why we wanted to solve it. So after my first company, my co founder and I, we were trying to decide what we\'re going to do next. So we started doing some contracting work sort of left and right. I was living in Paris at the time and working with a lot of French and German company. And when we were working with this German automaker, they just really needed high quality full text search within Postgres. So the promo we were building for them, and instead we had to glue Elasticsearch or some vector database on top. And it was just a nightmare. And we were very frustrated how bad it was and decided, you know what, maybe we should just fix it ourselves. And so that kind of led us to today. \\n\\n\\n##### [00:05:00] - Ry\\n\\nGot it. Yeah, it\'s interesting. I don\'t know. There was a famous tweet that I can\'t remember the exact wording on, but it basically said something to the effect that, hey, tech founders, you do yourself well to pivot to building something that would support whatever you would be building today. Basically go one level deeper than your current idea is similar to what we did at Astronomer, because we were basically building a competitor to segment clickstream data processing. And then we started using Airflow. Then we said, hey, let\'s just provide airflow as a service. And kind of made that switch to go down a level and that was the winning pivot for that company. So it sounds like you\'ve kind of jumped down to more of an infrastructural issue, which I think is good. Well, what challenges? Obviously there\'s a lot of ground to cover, but what are your biggest challenges, would you say you face so far? Building this thing.\\n\\n\\n##### [00:06:12] - Phil\\n\\nYou mean like business wise or technical wise or process wise?\\n\\n\\n##### [00:06:16] - Ry\\n\\nActually, yeah. I\'d love to hear all categories of challenge. Yeah, spill the beans. What\'s hard about this? I imagine fundraise. I mean, it\'s a tough time to fundraise, so that\'s probably not awesome. But yeah, maybe start technically.\\n\\n\\n##### [00:06:33] - Phil\\n\\nYeah, I can start technically. I think software is the easiest part of building a company, so I would say these are our simplest set of challenges. Not to say it\'s know, I was actually talking to Eric that I listened to the Hacking Postgres episode with Eric and I talked to him afterwards. At first we didn\'t know pgrx was a thing, and we were like, oh my God, how on earth is this even going to be a thing? And then we found out pgrx was a thing and we\'re like, wow, it went from near impossible to quite doable, really. So, like on the technical front, I mean, there\'s just so much you need to do when you\'re building search, right? We have covered a small ground of features of what people expect, and there are so many more that we need to build, and so we\'re trying to prioritize that.\\n\\n\\n##### [00:07:25] - Ry\\n\\nObviously Elastic is probably the big competitor. Can you estimate what percentage of their functionality you aim to replace? Is it essentially 100% and what\'s your progress towards that?\\n\\n\\n##### [00:07:41] - Phil\\n\\nYeah, that\'s a good question. I mean, our progress was small. It\'s small enough that you can use it, but Elastic is enormous, right? I don\'t think we aim to replace or not to replace, sorry. To match 100% of their features, at least for transactional data, which is what we need today. I think we actually had a decent bit of the way done and definitely enough to keep us busy for a couple of months. But I think that\'s sort of the first step of what we\'re looking to match beyond that, like what everything elastic does for observability, that\'s sort of another story, and that\'s another milestone that we will see depending on what our customers want.\\n\\n\\n##### [00:08:29] - Ry\\n\\nObviously I\'m building a fresh new tech startup as well, and I understand you got to match the functionality of, I call it the table stakes features. But then you also have to come up with some differentiators or else no one will care. So what are the biggest differentiators you\'re thinking about with ParadeDB?\\n\\n\\n##### [00:08:49] - Phil\\n\\nYeah, so that\'s a good question. I think for ParadeDB right now, the differentiator has been, ironically, I would say the combination of two non differentiated things, if that makes sense, which is like, if you want to do search over transactional data, ParadeDB is sort of the best solution today because it\'s the only thing that actually truly combines the two. It wasn\'t done before. And so sort of our initial interest, I would say, has come from that. So typically when people wanted to do this, they would have some managed Postgres provider, of which there are many, and then they would use Elastic and then they would need to combine the two and then run the two and so on. It can be a ton of work. And so ParadeDB sort of makes that a single system and especially for small companies, which is where you\'ve been really, it\'s been a lot easier to manage, I would say. And so that really draws them to us. That\'s sort of like our initial point initially, but we have some other things that will be coming up which hopefully will expand on this sort of initial interest.\\n\\n\\n##### [00:09:52] - Ry\\n\\nYeah, it seemed to me like as I think about building my next search bar in a product, I have to decide do I want to go what I can so call like the old school way versus vector search? Maybe I can get away with some sort of AI integration for search. Are people asking that question or do they have the perception that maybe like vector search somehow gets, replaces or displaces in some way traditional full text search type features?\\n\\n\\n##### [00:10:28] - Phil\\n\\nYeah, that\'s a very good question. A lot of people have, I\'ve talked to a lot of people who weigh in on both sides of the coin. I would say my personal take is both will remain quite important just because at the end of the day people will still have data and we\'ll still need to be able to find instances of those data. Right. And there\'s one way to find it via similarity, but there\'s also sometimes you just want to start from a clean place. And so I think both are quite valuable. Most of the people we talk to, hybrid search is very important to them, like the combination of the two, both big and small companies. And that\'s one of the reasons why we built Parade the way we did today, where pgvector exists and so many others. And I would almost say vector search is kind of table stake now. We don\'t really have to innovate on that so much ourselves, I would say. But the traditional search is kind of the piece that\'s been missing, which is why we released this first extension, pg_bm25, and then we offered the combination of both as well [inaudible 11:29]  with another extension called pg_search, which we\'re going to be publicizing more heavily soon.\\n\\n\\n##### [00:11:35] - Ry\\n\\nYeah, I think it seems like, well, it seems to me like everything has to have an AI story or else perhaps it\'s been outdated. Right. So I think even search, even full tech search, you can say I like that the hybrid answer is a good answer. Obviously then you have to figure out how to hybridize it all. And that\'s not necessarily trivial. But yeah, I\'m curious to see what you come up with on this.\\n\\n\\n##### [00:12:07] - Phil\\n\\nYeah, it takes some effort. It takes some effort. I would say we definitely have an AI story, but our personal take on the whole AI world and fade is like AI is sort of a step function improvement on existing things being done, but you need a foundation of value proposition for customers on the fundamental non AI part of the product, if that makes sense. And that\'s kind of how we think about it. There are so many people that are trying to bring AI when there was nothing useful to begin with. Well, I really think of AI as taking something that was useful to begin with and making it that much better. And so it kind of plays quite well actually with the way we\'ve been going about delivering this product and saying like, hey, we\'ll just give you really good full text search and obviously so much more can be done. But if that is not useful in the first place in Postgres, then who cares that I can add AI to.\\n\\n\\n##### [00:13:01] - Ry\\n\\nYeah, yeah, I agree. Yeah, I think about it. Know you can\'t build AI without training data, right? And you can\'t, you can\'t have training data without a product. You know, to collect some base level data. If you have a base level of product without AI, basically you have to start without AI, don\'t have data at all. And I think that\'s a challenge. A lot of founders, maybe they\'re thinking they\'re going to go straight to AI. They\'re like, there\'s nothing. You got to start basic. Yeah, I agree. What are some big milestones? How many big milestones do you have envisioned for ParadeDB at this moment?\\n\\n\\n##### [00:13:44] - Phil\\n\\nThat\'s also a good question.\\n\\n\\n##### [00:13:46] - Ry\\n\\nI mean you probably have a next one, right? You usually know your next big milestone.\\n\\n\\n##### [00:13:50] - Phil\\n\\nHopefully you\'d have that. Yeah, of course.\\n\\n\\n##### [00:13:53] - Ry\\n\\nAnd you also tell me what it is. But I\'m curious.\\n\\n\\n##### [00:13:58] - Phil\\n\\nHappy to share. I mean, to be honest our big milestones was to get a thousand stars in new Hub and then some stranger posted our repo on Hacker News and somehow that happened in a couple of days, which was really exciting and rather unexpected. Now our big milestone is we\'re about to release our own small version of managed Postgres ish specifically for ParadeDB to test how else we can deliver value to people. Like many that have been using the product so far have been using our self hosted version, which I\'m sure you\'re extremely familiar with how people don\'t want to do this, doing Tembo. So that\'s the next big milestone that\'s coming, trying to learn to manage this. And then after that we have a couple of, I think after that we\'ll see the exact number. We\'re just trying to get more people on it and see what they think. Really.\\n\\n\\n##### [00:14:53] - Ry\\n\\nYeah, just grow users. This is another thing I\'ve been thinking about around just even like we have stacks at Tembo with different use cases for Postgres, and then some things ought to be workloads separated from other things. Like for example, do you keep your analytical workload separate from transactional? That\'s clear. But search is interesting, whether that\'s like an add on. Is that just work that the transactional database should be doing, or is it somehow, do you think the workload for search should be isolated from traditional transactional workloads? From an application standpoint, that\'s a good question.\\n\\n\\n##### [00:15:36] - Phil\\n\\nI think it depends what product you\'re building, to be honest, because you don\'t want to slow down transactions. That\'s something that\'s very important to us. For example, I think it\'s possible to integrate it, actually. I think it\'s up to you, the way we\'re thinking about it. So the way we\'ve built ParadeDB is we offer weak consistency over search. So what this means is the way we build searches, we have inverted indices that store the tokenized data that you want to search over, but those are weakly consistent. So just like if you use Elastic, for example, and something like Zombodb, and then you synchronize it, there will be some lag between transactions being updated in your database and eventually being reflected in your search engine, the same thing happens, but it happens automatically for ParadB, instead of being it can be minutes sometimes here will be like a few seconds at the very most, but it ensures that your transactions are never slowed down, which is quite critical for people. So I think if you build in that way, it\'s actually quite possible to centralize it and have it be integrated with the actual database. If you don\'t use a product where search is built that way, then I don\'t think you want to slow down your transaction so you have to isolate them.\\n\\n\\n##### [00:16:48] - Ry\\n\\nYeah, I think a lot of people just obviously just do it inside their transactional database because it seems like overkill. Spin up a new Postgres, just especially like traditional shitty full tech search that\'s available. But if we want to get good search, if I wanted to build really good search into an application, I would go buy Algolia or something like that. And that clearly is a microservice at that point, right? I\'m sending data to them to index and they\'re sending me back results really quickly that are happening. It\'s not taxing my main database as a part of that. So I kind of like thinking of search as, let\'s call it great search as a microservice candidate for sure, because the better you make it, the more compute it requires. The more compute it requires, the more it\'s competing. Right? So I don\'t know, I like the idea of just taking that worldview. That great search needs to be separated from the transactional or analytical.\\n\\n\\n##### [00:18:03] - Phil\\n\\nYeah, I think you\'re right in a lot of ways. You do need to be able to orchestrate it properly at scale. This is definitely something that we will ponder more and more as we have more people using it and using it at greater scale. I do think even if it needs to be solely separate from what we have done today, one of the great things and where DBMS and Postgres managed companies like ParadeDB or Tembo show up is in the ability to just make it transparent to the customer. The actual complexity of the orchestration behind the.\\n\\n\\n##### [00:18:37] - Ry\\n\\nSo, you know, you\'ve been writing code for a number of years now. What would you say your most important lesson has been? Kind of switching from the technical to the interpersonal or just like professional. Do you have any top lesson? If you\'re going to meet with a young developer, you\'d say.\\n\\n\\n##### [00:19:00] - Phil\\n\\nSpecifically on writing code? Yeah, specifically on writing code. I would say only write the code that you need to write. I think that\'s like a big one. Our previous company, maybe I\'ll bother with this one. I like to think of products and startups now as like science experiments, as works of art. And I think our previous company, we thought of it as a work of art. So we came out of college, we were so incredibly into it and we wanted to be perfect. We cared so tremendously about what we did, that we always went above and beyond and it always made the product better and it involved writing more code, but it did not always make the product better in a way that meaningfully improved the experience of the user or brought us closer to product market fit. Right. And so I think if you think of a work of art like it\'s never finished, it needs to be perfect, versus a science experiment is more like, what\'s the bare minimum I can do to validate or invalidate this hypothesis I have or deliver value to a customer. And so what I would say, what I wish I could say to my younger self was, if no one has asked for this, I don\'t think you need to do it.\\n\\n\\n##### [00:20:12] - Ry\\n\\nYeah, it\'s a good point. So you\'re saying be more of a scientist than an artist, perhaps to some degree. When you\'re starting on something and it\'s not validated.\\n\\n\\n##### [00:20:26] - Phil\\n\\nI think a lot of great products are almost like work of arts. Like, people were so obsessed over it and you can feel that. And so I don\'t think you want to leave all types of esthetic approach or care to it, but I think definitely early on, before you even sink so much time into it, you just want to get feedback quickly. Right. And that involves just only writing the code you need to write. Yeah.\\n\\n\\n##### [00:20:49] - Ry\\n\\nI\'ll tell you a quick story. Like when we started astronomer, I have a good friend, Greg Neiheisel. He was CTO and basically my partner of crime. And we just very much mind melded on a lot of things. But our attitude following your principle was really what we were building was this beautiful lattice of glue, and everything was open source inside. So we used, for example, we used Google\'s material design, which had just come out, and we built a UI, like, adhering to it perfectly, and ended up being a beautiful UI because guess what? Those guys built a great framework and we had no designer on the team. Google was our designer. We would joke about it all the time, and we actually chose a color scheme for the company that was kind of Googley, had the same four main colors. So it was really easy to build something great. But that we really built the first version of our product was entirely open source components just glued together in a unique way. And that was our art. It was like this inner skeleton of orchestration, or just like I said, I can think of it like a lattice of glue that\'s holding all these things together. And, yeah, it was a very successful product. And it looks basically how you arrange open source components is art. It\'s a creative endeavor, I think, and it\'s enough for a brand new product. It\'s enough art. You don\'t have to go and reinvent, like, for example, the airflow UI during this really hard early project. And you don\'t have to go reinvent the Postgres UI. Maybe you want to, but you\'re like, let\'s handle the first things first. So did you think of a lesson that was non technical that you learned that you would tell your younger self or not really?\\n\\n\\n##### [00:22:49] - Phil\\n\\nOh, I have so many. I spent a lot of time thinking after our previous companies didn\'t work out specifically. I don\'t know. This is a broad question. I can go in every direction. Let me think, which one is the best one to share? Yeah, I would say, okay. One interesting lesson is I\'ll focus on the youth as well. And what you mentioned. When we were building our first company, our first product, we felt very righteous. We were doing something very difficult and objectively, technically difficult. And a lot of people have tried before, and we were like, they must have done it wrong. Who are they? People with more experience and context than us to know how to do this really well. And so we always started every premise of what we were doing from the premise that we were correct. Right. And then we were strived to further validate the fact that we were correct. And I think this leads to a lot of cognitive dissonance when you turn out to be wrong. Right. Because you had this idea of what you were thinking, and it\'s now being challenged versus now the way we think about it.\\n\\n\\n##### [00:24:07] - Phil\\n\\nAnd I mean this from a personal standpoint, from the way we talk and hire people, from the way we talk to customers, from the way we try to build product now, we just think we\'re wrong. I always assume I\'m wrong in what I think, and then I strive to prove myself correct. Right. And if we do manage to prove ourselves to be correct, then it\'s like a very positive moment. You\'re like, that\'s amazing. I found this great surprise that I wasn\'t fully convinced of before. And if you turn out to be wrong, you\'re like, well, kind of thought I could be wrong all along, not a big deal, and then sort of drop this and move to something else. And I think it\'s a mental framework that has served us a lot better in everything that we do now.\\n\\n\\n##### [00:24:45] - Ry\\n\\nYeah, I love that.\\n\\n\\n##### [00:24:47] - Phil\\n\\nYeah.\\n\\n\\n##### [00:24:48] - Ry\\n\\nI think it\'s a tricky catch 22. Like, you have to, as a founder, live in a reality distortion field just to change the world. You have to have a crazy idea that most people disagree with. Those are the best ideas. Unfortunately, they\'re also the worst ideas.\\n\\n\\n##### [00:25:05] - Phil\\n\\nExactly.\\n\\n\\n##### [00:25:06] - Ry\\n\\nJust accept that, hey, you might be. At the same time, you have to also fight for your idea. Even if you hear negative, it\'s like, oh, man, I built all this infrastructure for it to be righteous, and now it\'s hard to give up on it. I think it\'s wrong to give up on it a lot of times, because, again, I think that some of the best ideas almost everyone would disagree with as it starts. I agree. It\'s a much better mindset to understand that it\'s a lottery ticket sort of odds not, hey, well, if you were a good student, you know you can get an A in a class, right? You know you can get an A. It\'s just a matter of doing the right amount of work, and you\'ll get an A. But it doesn\'t work that way with startups. Exactly.\\n\\n\\n##### [00:25:59] - Phil\\n\\nYeah.\\n\\n\\n##### [00:26:00] - Ry\\n\\nCool. So, well, I\'m curious, like, are you paying attention to the Postgres ecosystem and other developments maybe that you\'re excited about that you\'ve seen since you started working on.\\n\\n\\n##### [00:26:13] - Phil\\n\\nHave. I mean, there\'s, there\'s just so much that\'s happening in Postgres, obviously pretty excited about Tembo. I\'ve been following you guys\' journey. Just like, recently, there are people that I forget the name of the developer. That\'s very embarrassing. But someone released PG branching, which maybe you\'ve seen, which is like, so I was talking to Nikita, the CEO of Neon, right? And he was, like, raving about their branching system, which I\'m sure is world class and amazing. But it turns out other people might be able to do this someday, which is good for the community, perhaps. So I think that\'s quite an exciting extension that hasn\'t been made by us, for example, that we\'re very excited about. And there\'s many others that I could list.\\n\\n\\n##### [00:26:56] - Ry\\n\\nYeah, it\'s interesting. My first reaction I saw, like, AWS announce, oh, we have active, active replication available on RDS. It\'s a new extension, you know. And I responded on Twitter, is it going to be open source? And crickets. So it makes me feel like time to. We\'ll study that extension, and the community needs to just build the open source version of it if they won\'t do it. I\'m not salty about how the big clouds operate. Extracting value from open source. Because I\'m extracting value from open source, and you are, too. We all are. All the way down. Like I said, it\'s like value extraction all the way down to Linux, but they could be. Obviously their goal is to build a proprietary alternatives and I really wish there was a Postgres company out there that was big and was worrying about the whole open source ecosystem. That\'s what we\'re trying to do. Yeah, there\'s so much going on. I don\'t know if it\'s because I\'m looking at it now or if things have changed in 2023 versus 2022. It\'s great that it\'s happening too during what I would consider like a law in the venture ecosystem as well. So I think the stuff that\'s happening now will be strong. They always say like the startups that start during the slow parts of venture usually end up being more successful than the ones at the hype cycle. So you and I are both lucky we\'re starting here. Even those. It might not feel that way sometimes.\\n\\n\\n##### [00:28:39] - Phil\\n\\nI think it does feel that way though. We raised our seed round, things went quite well. It was a challenging raise, but challenging because people were very critical about what we\'re doing and I think that\'s good, right? Like the first company, we raised money on a terrible idea and we thought raising money was the good part. But actually if you have a terrible idea and people convince you of that early on, they\'re actually doing you a favor, right? Yeah, I think it\'s good. I\'m very bullish on the companies that are going to be founded, that have been founded this year, that will be found next year and so on, because I think you\'re right. Overall, typically they turn out to be quite good. Yeah.\\n\\n\\n##### [00:29:22] - Ry\\n\\nSo do you have a couple of favorite people that you\'ve met in Postgres Land? Give them a shout out if you do.\\n\\n\\n##### [00:29:29] - Phil\\n\\nYes, sure. I mean I met Eric, which I think will take the prize for my favorite. He. He\'s very humble about it. He keeps saying TCDI needed pgrx. I think they really did it beyond what was needed for the benefit of the community. So I think he\'s definitely gone above and beyond and he continues to go above and beyond. Another one maybe you know him or not. He\'s the writer of one of the PG internals book called Hironobu, who\'s this Japanese man who lives in Europe now I haven\'t engaged with him very much, but he\'s made this resource called I think InterDB.jp or something like that. I could send you the link if you\'re curious, which goes into Postgres internals at a great detail. And it\'s been like a good resource for us in it. So shout out to that guy.\\n\\n\\n##### [00:30:23] - Ry\\n\\nYeah, I definitely read the website, but I haven\'t spoken to the man.\\n\\n\\n##### [00:30:29] - Phil\\n\\nMaybe we should try to email him a few times. Yeah, I tried to convince him to join us, but it was too difficult to sell.\\n\\n\\n##### [00:30:37] - Ry\\n\\nYeah, maybe he, I mean, I tried to convince Eric Ridge to join Tembo in the early on and he was too hard to sell.\\n\\n\\n##### [00:30:43] - Phil\\n\\nI\'m sure you did. Of course, I would have loved to convince, um, the naive part of me was like, I can\'t wait to convince Eric to join Parade. And then I talked to him, realized he basically is the co founder and CTO of TCDI. And I was like, okay, this isn\'t happening.\\n\\n\\n##### [00:31:01] - Ry\\n\\nYeah, I was probably the 25th person to go after him. And you were probably the 27th person to go after him. Whatever.\\n\\n\\n##### [00:31:10] - Phil\\n\\nExactly.\\n\\n\\n##### [00:31:11] - Ry\\n\\nYeah, I think it\'s great. I\'m kind of excited that these people, I\'ll call them like the gods of Postgres, are kind of locked up up on Mount Olympus. We don\'t get to see them or talk to them very much. That\'s exciting to me, to know that someday we\'ll get there. Someday, if our companies are strong enough, maybe we can get to work with those people. But it\'s great to be in an ecosystem where there is a tier of people that are just amazing. I was talking to Samay, who\'s the CTO here at Tembo, and he did the quick analysis. To see that most people become a committer at Postgres takes like around seven or eight years of contributing to Postgres before you get the commit bit, which is like, wow, that is an amazing. You think of venture backed startups, it\'s like two startups worth of effort sticking around for your full vesting before you get to be up. So it has nothing to do with the company that you\'re working for, it\'s just you\'re going to dedicate yourself to an open source project for the core of your professional life. Because once you get there, you probably want to stick around for another eight years, I\'m sure.\\n\\n\\n##### [00:32:27] - Ry\\n\\nYeah, it\'s pretty cool.\\n\\n\\n##### [00:32:28] - Phil\\n\\nYeah. But these people are great, and I think that\'s one of the things that\'s so great about Postgres community. Even though Eric and so on, they\'re amazing people and they\'re doing great things and they\'re very busy and so on. This are quite accessible, really. Eric is very accessible in the pgrx discord and so on. So I do feel like by building open source, we still get to work with them, even though not in the same organization. And some other people I forgot to shout out, which perhaps deserve a lot of shout out, is maybe like Umur and Ozgun from Citus, right? They were like truly visionaries of the Postgres world, building extensions when Postgres was a toy database, as people would like, critic being critical of.\\n\\n\\n##### [00:33:17] - Ry\\n\\nYeah, yeah, we\'re definitely standing on their shoulders. They\'ve, they did a lot of hard work early on and yeah, it\'s great. If you had a magic wand, you could add any feature to Postgres tomorrow and it existed. Is there anything that you would use that wand on? Do you have any things you wish Postgres was that it isn\'t?\\n\\n\\n##### [00:33:40] - Phil\\n\\nThat\'s very interesting. I think the answer would be better distributed support. Actually, what Citus has done is amazing. But the whole reason Yugabyte exists today, for example, is because there\'s only so much you can do when you\'re an extension like Citus did, right? And like Yugabyte is basically trying to be the Postgres equivalent of CockroachDB. And I think if that was baked in natively to Postgres, it would be good for a lot of people that are building on Postgres included.\\n\\n\\n##### [00:34:13] - Ry\\n\\nYeah, that\'s a good one. Do you listen to podcasts very much?\\n\\n\\n##### [00:34:19] - Phil\\n\\nNot very much, not very much. Only yours.\\n\\n\\n##### [00:34:22] - Ry\\n\\nOkay, good, thank you. No, I listen to PostgresFM all the time. I\'m huge fans of those guys. I do listen to a lot of podcasts. But yeah. Just going to ask you what your favorites are. But you already said. All right, well cool. It was great talking to you. Where can listeners find you online? Maybe just tell us about yourself and then ParadeDB url and so on.\\n\\n\\n##### [00:34:49] - Phil\\n\\nYeah, so you can find us on paradedb.com or on a GitHub is ParadeDB as well. It\'s open source. We welcome contributions and users and so on. Of course we\'re very responsive. As for me specifically, I think you\'re going to link my Twitter in the bio, but my Twitter is PhilippeMNoel. It\'s basically my handle across every social media. I feel like when you can find one, you sort of keep it. There\'s so many people now on the internet, so that\'s my handle everywhere. People can find me there and I love chatting.\\n\\n\\n##### [00:35:20] - Ry\\n\\nAwesome. Well, thanks for joining. Love to have you on again in the future. We\'ll see again, it\'s super early in this podcast life, but track what\'s going on with ParadeDB and if there\'s more stuff to talk about, we\'ll get back together.\\n\\n\\n##### [00:35:36] - Phil\\n\\nOf course. Yeah, I would love to be there. Thank you for having me."},{"id":"pgmq-self-regulating-queue","metadata":{"permalink":"/blog/pgmq-self-regulating-queue","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-11-07-pgmq-self-regulating/index.md","source":"@site/blog/2023-11-07-pgmq-self-regulating/index.md","title":"PGMQ: Lightweight Message Queue on Postgres with No Background Worker","description":"image","date":"2023-11-07T00:00:00.000Z","formattedDate":"November 7, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"message-queue","permalink":"/blog/tags/message-queue"},{"label":"pgmq","permalink":"/blog/tags/pgmq"}],"readingTime":5.705,"hasTruncateMarker":false,"authors":[{"name":"Adam Hendel","title":"Founding Engineer","url":"https://github.com/ChuckHend","email":"noreply@tembo.io","imageURL":"https://github.com/chuckhend.png","key":"adam"}],"frontMatter":{"slug":"pgmq-self-regulating-queue","title":"PGMQ: Lightweight Message Queue on Postgres with No Background Worker","authors":["adam"],"tags":["postgres","message-queue","pgmq"]},"prevItem":{"title":"Hacking Postgres, Ep. 8: Philippe No\xebl","permalink":"/blog/hacking-postgres-ep8"},"nextItem":{"title":"Hacking Postgres Ep. 7: Burak Yucesoy","permalink":"/blog/hacking-postgres-ep7"}},"content":"<div style={{width: \'75%\'}}>\\n\\n![image](./tembo-queue.png)\\n\\n</div>\\n\\nYour app needs a message queue. Simple enough\u2014until you try to do it, anyway.\\n\\nGo set it up on Kafka? Sure...but now you have a Kafka cluster to manage.\\nRedis could work, but now you\'re just managing Redis instances instead.\\nSQS? That means you have to reconfigure your application to talk to AWS, and you also get an extra external bill as icing on the cake.\\nLet\'s build it on Postgres! However, if you follow most blogs and guides, you\'ll probably end up building an agent, watcher process, or background worker to make sure the queue stays healthy. It\'s better, but if the background worker fails, you could end up with bigger problems.\\n\\nFortunately, there\'s a better way.\\n\\nBy designing with a visibility timeout, we remove the need for external processes for queue management. PGMQ is a Postgres extension built following exactly this sort of self-regulating queue. Today we\'re going to combine PGMQ with pair of core Postgres features\u2014FOR UPDATE and SKIP LOCKED\u2014to cover all the needs of our message queue. FOR UPDATE helps ensure that just a single consumer receives a message in the queue. And SKIP LOCKED is required if you want to have multiple consumers on the same queue \u2013 without it each consumer would wait for the others to remove their locks. Before we put all the pieces together, let\'s do a quick refresher on how each of these works.\\n\\n## FOR UPDATE: Ensuring Exclusive Access\\n\\nImagine a crowded store with a single cashier. As customers approach the counter, the cashier must serve them one at a time, ensuring each transaction is complete before moving on to the next. Similarly, when working with a queue, it\'s essential to ensure that only one process or transaction works with a particular row of data at a time. This is where FOR UPDATE comes into play.\\n\\nThe FOR UPDATE clause is used to lock selected rows, preventing other transactions from modifying or locking them until the current transaction ends. This ensures that once a task (or row in our queue) is picked up, it isn\'t grabbed by another worker or process simultaneously. It guarantees exclusive access, much like how our lone cashier attends to a single customer at a time.\\n\\n## SKIP LOCKED: Keeping the Line Moving\\n\\nBack to our store analogy, if a customer isn\'t ready to check out and holds up the line, it can cause unnecessary delays for the other customers. What if, instead, those who aren\'t ready simply step aside, allowing others to continue? That would undoubtedly speed up the checkout process. This is the concept behind SKIP LOCKED.\\n\\nWhen combined with FOR UPDATE, the SKIP LOCKED clause ensures that if a row is locked by another transaction, it gets skipped over, and the next available row is selected. This way, other workers or processes don\'t get stuck waiting for a locked row to be released; they simply move on to the next available task.\\n\\nBy using these two clauses together, you can ensure that tasks in your queue are processed smoothly and efficiently, with each task getting picked up by a single worker and other workers moving seamlessly to the next available task.\\n\\nHowever, how can we take this one step further? Many queue implementations have us using FOR UPDATE to mark a message as \u201cin progress\u201d which is great. However, that typically requires us to have a service external to postgres which monitors the queue to check for messages which have been \u201cin progress\u201d for too long.\\n\\n## Self-Regulating Queues in PostgreSQL\\n\\nIf you\u2019re using FOR UPDATE SKIP LOCKED, and setting messages \u201cin progress\u201d, you most likely need a process to watch your queue and check for messages that have been processing for too long. Rather than running a background worker or external process, PGMQ implements a Visibility Timeout (VT). A VT is a designated period during which a message, once read from the queue, becomes invisible to other consumers or workers. This ensures that once a worker picks up a task, other workers won\'t attempt to process the same task for the duration of this timeout. If the original worker fails to complete the task within the specified timeout, the task becomes visible again by nature of time elapsing past the specified VT, which means PGMQ still provides an at-least-once delivery guarantee even when the VT has elapsed. The task is ready for reprocessing by the same or a different worker.\\n\\nIn essence, the visibility timeout provides a grace period for tasks to be processed. It becomes a buffer against potential failures, ensuring that if a task isn\u2019t completed due to any unforeseen reasons, it doesn\u2019t get lost but rather re-enters the queue. Without something like the VT, queue systems will need to run a process to watch the queue. If that watcher crashes, or loses connection, then messages will stay unavailable until the watcher is recovered.\\n\\n## Queue regulation isn\u2019t just for error modes\\n\\nA common use case is for a consumer that needs to process a long-running I/O bound task. Let\u2019s say there is a message with a task to create some infrastructure in your favorite [cloud provider](https://cloud.tembo.io), e.g. create an EC2 instance if it doesn\u2019t already exist. That could take minutes to start up. Your consumer can submit the request to provision EC2, and then instead of waiting for EC2 to create, it can set the VT on that message to 60 seconds from now, then move on to read the next message. In 60 seconds, the message will become visible again and can be picked up.  That can look something like the follow pseudo-code:\\n\\n```text\\n# read a message, make it invisible for 30 seconds\\nselect pgmq.read(\u2018task_queue\u2019, 30, 1)\\n\\n...check if S3 bucket already already created\\n...request to create S3 bucket if not exists\\n\\n# set message VT to 60 seconds from now If not exists: \\nselect pgmq.set_vt(\u2018task_queue\u2019, <msg_id>, 60)\\n\\n# consumer archives or deletes the message when its finished with its job\\nselect pgmq.archive(\'test_queue\', <msg_id>)\\n```\\n\\nWith PGMQ\u2019s design, messages do not leave the queue until they are explicitly archived or deleted, so you could think of using the VT directly as \u201creturning the message to the queue\u201d, even though it technically never left the queue. Rather, returning it to a state that can be read by other consumers.\\n\\n## Choose the simplest queue architecture with PGMQ\\n\\nUse `FOR UPDATE` so that you ensure messages are only read by one consumer at time. `SKIP LOCKED` so that you can have multiple workers processing messages concurrently. Finally, implement a visibility timeout so that you no longer need to rely on an external process to handle messages that have failed to process. Install the open-source [PGMQ extension](https://github.com/tembo-io/pgmq) for Postgres or try it on [Tembo Cloud](https://cloud.tembo.io) today and immediately benefit from all of these design decisions."},{"id":"hacking-postgres-ep7","metadata":{"permalink":"/blog/hacking-postgres-ep7","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-11-03-hacking-postgres-ep7/index.md","source":"@site/blog/2023-11-03-hacking-postgres-ep7/index.md","title":"Hacking Postgres Ep. 7: Burak Yucesoy","description":"Postgres for everything? How about building a new cloud provider? In episode 7 of Hacking Postgres, Ry talks with Burak Yucesoy of Ubicloud about new clouds, getting attached to extensions, and the future potential of Postgres.","date":"2023-11-03T00:00:00.000Z","formattedDate":"November 3, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"hacking_postgres","permalink":"/blog/tags/hacking-postgres"}],"readingTime":24.815,"hasTruncateMarker":false,"authors":[{"name":"Eric Ankenman","url":"https://github.com/ericankenman","email":"noreply@tembo.io","imageURL":"https://github.com/ericankenman.png","key":"eric"}],"frontMatter":{"slug":"hacking-postgres-ep7","title":"Hacking Postgres Ep. 7: Burak Yucesoy","authors":["eric"],"tags":["postgres","hacking_postgres"]},"prevItem":{"title":"PGMQ: Lightweight Message Queue on Postgres with No Background Worker","permalink":"/blog/pgmq-self-regulating-queue"},"nextItem":{"title":"Application Services: Helping Postgres Do More, Faster","permalink":"/blog/tembo-operator-apps"}},"content":"Postgres for everything? How about building a new cloud provider? In episode 7 of Hacking Postgres, Ry talks with Burak Yucesoy of Ubicloud about new clouds, getting attached to extensions, and the future potential of Postgres.\\n\\nWatch below, or listen on Apple/Spotify (or your podcast platform of choice). Special thanks to Regina and Paul for joining us today!\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/_f4WZwkrXQw?si=V2H6vs_e86Aq2xJt\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n\\nWant to know more about something they mentioned? Here\u2019s a starting point:\\n\\n\\n\\n* Ubicloud - https://www.ubicloud.com/\\n* Citus Data - https://www.citusdata.com/\\n* PostgreSQL HLL - [https://github.com/citusdata/postgresql-hll](https://github.com/citusdata/postgresql-hll)\\n* JSONB - [https://www.postgresql.org/docs/current/datatype-json.html](https://www.postgresql.org/docs/current/datatype-json.html)\\n* hstore -https://www.postgresql.org/docs/current/hstore.html\\n\\nDid you enjoy the episode? Have ideas for someone else we should invite? Let us know your thoughts on X at @tembo_io or share them with the team in our [Slack Community](https://join.slack.com/t/tembocommunity/shared_invite/zt-23o25qt91-AnZoC1jhLMLubwia4GeNGw).\\n\\n\\n## Transcript\\n\\n\\n##### _[00:00:12] - Ry_\\n\\nHello. I\'m Ry Walker, founder of Tembo, a managed Postgres company. And today I have Burak from Ubicloud. How do you say UB? Ubi.\\n\\n\\n##### _[00:00:23] - Burak_\\n\\nUbicloud.\\n\\n\\n##### _[00:00:24] - Ry_\\n\\nYeah, UB. Welcome to the podcast Burak.\\n\\n\\n##### _[00:00:33] - Burak_\\n\\nYeah, thanks a lot for having me here.\\n\\n\\n##### _[00:00:36] - Ry_\\n\\nYeah, well I\'d love to start with giving us a quick background. I\'m curious where\'d you grow up and what were you doing before?\\n\\n\\n##### _[00:00:48] - Burak_\\n\\nWell, well hello everyone, this is Burak and I work as a software developer and I guess in reverse chronological order I worked at Microsoft, Citus Data and SAP on distributed databases in all three and currently I worked at a startup called Ubicloud. We are basically building a new cloud provider and the primary thing we do differently than other providers is that well all the code is open. You can think it as open alternative to existing cloud providers. You can go to GitHub, check out the code, set it up on a bare meta server and then you can have your own cloud or you can use our managed offering of course. So yeah, this is brief history of me working professionally.\\n\\n\\n##### _[00:01:46] - Ry_\\n\\nYeah, nice. So you\'re open sourcing, essentially open sourcing and what AWS and GCP and Azure have done.\\n\\n\\n##### _[00:01:55] - Burak_\\n\\nYeah, definitely, I\'m seeing it what Linux is to proprietary operating systems. This is what we are doing for the cloud providers.\\n\\n\\n##### _[00:02:07] - Ry_\\n\\nYeah, and you only have to build 300 different services on top of it. Right, but luckily a lot of those are open source too.\\n\\n\\n##### _[00:02:15] - Burak_\\n\\nKind of. Well, I guess our current plan is not building all 200 render services because most of the time people use only ten at most 20. If you implement 20 of them, I guess you have 80% of the use cases. So I guess this is our initial plan but we never know what would feature show.\\n\\n\\n##### _[00:02:42] - Ry_\\n\\nYeah, I think it\'s a great idea. I\'m excited to watch it evolve and hopefully partner with you guys at some point.\\n\\n\\n##### _[00:02:50] - Burak_\\n\\nYeah, that would be awesome.\\n\\n\\n##### _[00:02:52] - Ry_\\n\\nDo you remember when you first started using Postgres?\\n\\n\\n##### _[00:02:57] - Burak_\\n\\nYeah, I do. Well to be honest, when I start programming I started as a web developer and at that time Lamp stack was very common. So my first database was MySQL but then I started working at Citus Data and which is the place that I started working development part of the Postgres. So basically for people who don\'t know Site, the Sitestata was the company behind many popular extensions such as Citus or Pgcron or PostgreSQL HLL. So when I first joined Citus Data, I initially worked on building Citus extension and while doing that you need to dig to the Postgres code first you need to understand and then you build your extension on top of it. And then we built our own managed service. So I switched to that team to build a Postgres managed service and some of our customers were well, they were heavily using PostgresQL HLL extension and at that time the original authors of the PostgreSQL HLL extension, they went through an acquisition process and they didn\'t have enough time at their hand to maintain the extension. And well, we know the people and at that time PostgreSQL Community was much smaller.\\n\\n\\n##### _[00:04:37] - Burak_\\n\\nSo we just called them and said that, hey, we want to maintain this extension, what do you think about? And they were pretty happy to find a new maintainer. So, long story short, I found myself as the maintainer of the PostgreSQL extension and then the Citus Data got acquired by Microsoft and then well, I continued my Postgres journey on Microsoft. Like we also build a managed server stash. Yeah, I guess that\'s how I start with Postgres development. Well, not just start like till the almost whole journey.\\n\\n\\n##### _[00:05:18] - Ry_\\n\\nYeah. And are you taking a little bit of a break from it right now or are you working on Postgres stuff over at Ubicloud?\\n\\n\\n##### _[00:05:31] - Burak_\\n\\nYeah, well, first of all, we use Postgres in Ubicloud as like most of my professional community, like the friends, and they are from Postgres Community, so there\'s no way I can leave that community at all. But as a cloud provider, Ubicloud also needs to offer a Postgres service. So we are planning some things like it\'s not very well defined yet, but I think eventually we will have something for Postgres as well.\\n\\n\\n##### _[00:06:12] - Ry_\\n\\nOut of curiosity, are you guys I\'m sure this is a hotly debated topic, but Kubernetes or no Kubernetes inside of UBI Cloud?\\n\\n\\n##### _[00:06:22] - Burak_\\n\\nWell, in our case right now, no Kubernetes. We have pretty simple control plane, which if you want, I think you can move it to Kubernetes. But right now we don\'t use it. Not that we have anything against Kubernetes, it\'s just I guess right now we don\'t need that complexity, I believe.\\n\\n\\n##### _[00:06:50] - Ry_\\n\\nYeah, well, and I imagine you\'ll have managed Kubernetes. You\'d have to have that. That\'ll be one of your 1st 20 most likely.\\n\\n\\n##### _[00:06:57] - Burak_\\n\\nYeah, definitely. Because managed Kubernetes is quite one of the most demanded products, so it needs to be one of the first.\\n\\n\\n##### _[00:07:11] - Ry_\\n\\nWell, so again, you were working on Citus...When you started working on it, was it an extension or was it, I don\'t know the full history? Was it a fork at any point and then became extension?\\n\\n\\n##### _[00:07:25] - Burak_\\n\\nYeah, at the beginning it was a fork and just before I joined the team it become an extension. So basically Citus become an extension and become an open source at the same time. And I think I joined Citus team in like one month after that. \\n\\n\\n##### _[00:07:45] - Ry_\\n\\nGot it. So you never were part of that previous era. Were a lot of the hard problems already solved, would you say, with Citus? Usually. I just did a talk with the PostGIS team and they said early on is where they solved the most of the problems and it was more gradual after that. Is that the case with Citus too, or was there a big project that happened after you joined well, I think.\\n\\n\\n##### _[00:08:19] - Burak_\\n\\nMost of the difficult problems were already solved and to be honest, I think one of the problems with the extension development is that there isn\'t good documentation about it. Like for Postgres has pretty good documentation for user facing features. But if you want to as a developer, there isn\'t that much resources so you usually need to read lots of Postgres code. And to be honest, I think Postgres code is pretty readable for a project at its size and that big and that old, so I think that\'s a huge plus. But still you need to read lots of code to understand what you need to do. And in our case, thankfully, Citrus hired one of the most prominent contributors of the Postgres Andres Freund and he primarily lead the effort to make Citus an extension. And I think at that time I believe Postgres extension framework also didn\'t have some of the features we need. So we had to do some hacky workaround. But eventually Postgres extension framework also got improved and we had chance to remove those hacky workaround.\\n\\n\\n##### _[00:09:48] - Ry_\\n\\nYeah, that\'s great. Yeah, it sounds like that\'s happening now with lots know a lot of the innovation in Postgres happens as a fork with the hope that in a version or two can become an extension and then maybe a couple of versions. After that it becomes a less hacky extension. Right? You can streamline it, but it\'s a four year journey or so. Tell me about PostgreSQL HLL. Tell me what\'s HLL stand for.\\n\\n\\n##### _[00:10:19] - Burak_\\n\\nYeah, well, HLL stands for Hyperlog log and it is an extension to make a cardinality estimation, which is a fancy way of saying doing count distinct but approximately. Let me first explain why approximately. The reason is doing count distinct as an accurate number is pretty difficult. Well, not difficult, but maybe unfeasible. If your data size is small, that\'s okay. But if you have lots of data, the usual way is keeping a hash map or hash set. Every time you see an item you put it to hash set and at the end you count number of items in it. But if you have lots of data that becomes unfeasible. If you have a distributed system, like if you are doing count testing in two different nodes, that becomes even more difficult. Because let\'s say you bite the bullet and calculate the counter stick in one node and the other node, it\'s not possible to merge the result. Because there might be common elements, you cannot just sum them up. So what Hyperlog log does is it uses an approximation algorithm. I can go into detail of it as well to have an internal representation of the number of unique elements which is both memory efficient compared to doing a hash map which is also easy to merge.\\n\\n\\n##### _[00:12:04] - Burak_\\n\\nSo it allows you to do like the parallel computing. And the only gotcha is it is not an exact number, it\'s an approximation, but it turns out that we don\'t need exact number most of the time. Like for example, especially in analytical use cases, let\'s say you want to count the number of unique users that visit your website. It doesn\'t matter if they are 4 million or 4.1 million, like you want a ballpark number. And also the good thing is the error rate of hyperlog log is quite small. It is usually around 2% and you can make it even smaller if you give it a bit more memory like you can make it more accurate while this hyperlog log algorithm is just out there. And what PostgreSQL hyperlog log does is it implements this algorithm for PostgreSQL.\\n\\n\\n##### _[00:13:09] - Ry_\\n\\nSo how much of a resource reduction would you estimate that using an approximation? So you lose a percentage or two of accuracy, but you get how much less compute required?\\n\\n\\n##### _[00:13:24] - Burak_\\n\\nWell, usually it\'s about 1.5 KB. So the hyperlogo data structure on default it takes about 1.5 KB memory.\\n\\n\\n##### _[00:13:38] - Ry_\\n\\nOrders of magnitude smaller.\\n\\n\\n##### _[00:13:40] - Burak_\\n\\nYeah, actually log log parts come from that. So if you are dealing with 32 bit integers, it can go up to two to the 32. You get the log of that, it is 32. You get another log that you get five. So you need five bits of memory to be able to store one bucket. And then what hyperlog log does is it keeps multiple buckets to increase the accuracy. So at the end it end up about like 1.5 kilobyte.\\n\\n\\n##### _[00:14:17] - Ry_\\n\\nGot it. So how involved were you in that extension? Did you start it or did you inherit it?\\n\\n\\n##### _[00:14:25] - Burak_\\n\\nI inherited it. So actually another startup called Aggregate Knowledge built that extension. Then I think they got acquired by New Star and at that time the project was not maintained frequently. So there were some boxes we need to be merged in and our customers were also using it. So we contacted the original authors and said that hey, we want to maintain this. And they were happy to hand over the maintainership to us. And then after that we did bug fixes, we did regular releases. I presented a few conference talks about hyperlog log  in Pgcomp EU and Pgcomp US. Yeah, that\'s the overall story.\\n\\n\\n##### _[00:15:24] - Ry_\\n\\nI\'m curious, have you been able to disconnect from I imagine it\'s easier to disconnect from Citus as an extension after leaving Microsoft, but disconnecting from this extension PostgreSQL HLL. Are you still kind of watching that because you have that knowledge?\\n\\n\\n##### _[00:15:47] - Burak_\\n\\nYeah, I have a little bit of emotional bond to that extension. Well, for example, there were few improvements that I wanted to do, but I didn\'t have time while working at Microsoft and it just, itched me from time to time and it is open source. So I guess at some point in near future I\'ll open a pull request and hopefully it would get merged. I hope.\\n\\n\\n##### _[00:16:19] - Ry_\\n\\nYeah, but Microsoft team controls that extension as it sits.\\n\\n\\n##### _[00:16:25] - Burak_\\n\\nYeah, right now Microsoft team controls, they continue to do regular releases and every time new PostgreSQL version comes up they ensure that it works well and they update the packages, release new packages. If there\'s a bug report, they are the one who fixes it.\\n\\n\\n##### _[00:16:45] - Ry_\\n\\nHow many extensions, I mean, obviously there\'s the Citus extension, this one. How many total extensions would you say like Microsoft has in Postgres? I know maybe it\'s hard to nail down a number, but there are a bunch of others too.\\n\\n\\n##### _[00:17:01] - Burak_\\n\\nThere are there\'s Citus, there\'s PostgresQL HLL. There is Pgcron which is also quite popular. It is a Chrome based job scheduler.\\n\\n\\n##### _[00:17:10] - Ry_\\n\\nYeah, I just started using that.\\n\\n\\n##### _[00:17:12] - Burak_\\n\\nYeah, that\'s pretty cool. It is primarily developed by Marco Slot. There is one extension to ensure that Postgres works. Postgres is well integrated with Azure. So there\'s like an extension called PG Azure. I think it\'s not open source but if you start a Postgres instance from Azure and check the extensions, if there\'s that extension there is TopN which is also approximation based extension. It gives you top N elements of a sorted list. And if you think about it is also expensive operation to do on big data set because you need to sort them first and take the top N. And I think there are more optimized algorithms that where you can keep heap which is more memory efficient but you still need to go over lots of data at that time. At Citus we also developed this TopN extension. Actually, if you look at it, the general team is about being able to do things at scale because Postgres is already pretty good at doing things in a single node. And like the title primary use case was make. A distributed PostgreSQL and we developed few extensions to make some operations that are not feasible to do at scale and find a ways to make them more feasible.\\n\\n\\n##### _[00:19:06] - Ry_\\n\\nSo I\'m curious, are there some big milestones in Postgres that you\'re looking forward to?\\n\\n\\n##### _[00:19:14] - Burak_\\n\\nYeah, well, actually I was looking forward for Postgres 16 release mostly because of the logical replication improvement. There are few and I think there will be more upcoming because I think logical replication is a very powerful concept but it\'s still a bit cumbersome to use it with PostgreSQL, especially when there\'s a change in the data, like if when you run a DDL command or when you have a failover. So there are few gotcha and I think with the Postgres 16 some of these are less problematic and maybe I hope in the upcoming versions it would be even easier to use. Well, when you have a very solid logical replication, it opens up lots of cool features. Well, one thing that I personally invested in is being able to do zero downtime failovers and when I say zero time downtime, I mean like the real zero downtime. Not just like 1 second downtime, but real zero downtime. And I think logical replication, solid logical replication would open up that yeah, I agree.\\n\\n\\n##### _[00:20:45] - Ry_\\n\\nIt\'s one thing to do that too with a single node versus a Citus cluster too right. Zero downtime gets complicated the more complicated your deployment is. But I agree on a single deployment I have this idea where we could basically build scaffolding around the existing thing, whatever it takes, like get another one working. In other words, temporarily have replication happening and then somehow seamlessly up above you have to have some fulcrum that you can so it\'s a complicated thing to figure out but I think it\'d be great for a managed service provider to basically build up temporary infrastructure that helps the zero downtime thing happen. If that\'s true, then you can restart for all kinds of reasons with impunity like auto scaling is possible, stuff like that.\\n\\n\\n##### _[00:21:50] - Burak_\\n\\nYeah, and one reason I especially interested in was that in our previous managed service we do lots of operations via failover. Like for example, if you want to scale up what we would do is we create another server with higher number of cores and then we would do failover. Or if you want to like for example, we might need to do maintenance like maybe we found a security vulnerability or there is the regular maintenance. What we would do is instead of going and patching the existing server we would create a new one and then we do a failover to that one and each of those failovers. It takes some amount of downtime which is not obviously not preferable and not a good experience for customers but if they were virtually free from the perspective of customer if they don\'t even notice that there\'s a failover then you can do as many failovers as you want.\\n\\n\\n##### _[00:22:55] - Ry_\\n\\nYou share the same vision as I do there. I think it would be exciting to get there. So I\'m curious though, if there was one thing that you could if you had a magic wand and this weekend something new would be in Postgres core, what would it be? What would you use that wand on?\\n\\n\\n##### _[00:23:11] - Burak_\\n\\nYeah, that\'s difficult. I want lots of things like picking one is difficult but I guess one thing that bothers me is that for high availability and backups you always need to depend the third party tool I would really love that to be sold in Postgres. Like for example, for Redis it comes with very good default settings that you can use for high availability. But for Postgres there are solutions. There are good solutions but I would love them to be in the core.\\n\\n\\n##### _[00:24:03] - Ry_\\n\\nYeah, I get that for sure. It\'s tricky when you have to buy a product or let\'s say you adopt a product and you immediately have to adopt x additional products right off the bat and that\'s not a good feeling. It feels complex, right? Yeah, that\'s cool, I would say. Do you have any opinions about Postgres that almost nobody agrees with you about? Are you a contrarian in any area that you can think of?\\n\\n\\n##### _[00:24:35] - Burak_\\n\\nLet me see. I don\'t think so. Well.\\n\\n\\n##### _[00:24:45] - Ry_\\n\\nMaybe that\'s one of the areas that put that you think backrupt should be inside.\\n\\n\\n##### _[00:24:48] - Burak_\\n\\nThere is that. But I know there are people who also share that opinion. Yeah, I\'m not sure it\'s okay.\\n\\n\\n##### _[00:25:04] - Ry_\\n\\nYeah, I was just curious. It\'s always fun to talk about those things, if they exist. Give you a soapbox.\\n\\n\\n##### _[00:25:14] - Burak_\\n\\nActually, there is one thing, but that\'s also even I think Tembo kind of agrees with me on that one, is that I think many different use cases can be implemented in Postgres. So instead of having a lot of specialized databases, you can have Postgres. And with some configuration and maybe few extensions, you can implement, like you can implement Kafka in Postgres, you can implement Redis, you can implement like the NoSQL in Postgres. I guess if I said this maybe two, three years ago, probably I would get more raised eye growth. But now I think more people start thinking to think like that. I think Tembo is also thinking things along similar lines, right?\\n\\n\\n##### _[00:26:14] - Ry_\\n\\nYeah, I think there\'s openness to it. I talked to a lot of people, and the question is how people are Postgres for everything. Or question is what percentage of developers are actually on that bandwagon? Obviously on Twitter, it just takes one person, and it\'s a big community, too, especially if it\'s a contrarian view. But I\'m kind of curious. One of the things I want to find out over the next few months is what percentage of developers would actually if this Postgres for Everything was real, would they actually use it versus still saying \\"Ehh.\\" And I think it all comes down to like I think, yeah, you can do Kafka, like work on Postgres right now, but it doesn\'t feel as clean as buying Confluent. That seems like a very safe decision. And doing something like Kafka on Postgres seems like you\'re just kind of stringing together a Rube Goldberg machine and it doesn\'t feel like a solid. But the question is, if those solutions were solid, would people use them? And that\'s our big thesis, is that if they were solid, people would use them. But I just don\'t know what percentage of people would do that.\\n\\n\\n##### _[00:27:38] - Ry_\\n\\nA big percentage or a small percentage?\\n\\n\\n##### _[00:27:40] - Burak_\\n\\nYeah, I\'m not sure. But there is one interesting thing that come into my mind, is, well, today Postgres supports JSONB type, but it was not like that all the time. So in the earlier days, if you want to store JSON data, we had an extension called hstore, which we still have, but not as commonly used as before. And what hstore does is, and this is one of the very powerful part of PostgreSQL extension framework, hstore defined the data type, and on top of it, they defined how you can hash this data type and how you can compare this data type. And when you do this in Postgres, Postgres allows you to create index on that data type. So suddenly you are not only able to store JSON data, but you can index it. And at that time this is kind of rare things even for NoSQL database. So I think it\'s a bit funny. And also it shows the power of PostgreSQL extension framework is that suddenly you are able to do what NoSQL database does but better. I mean, not in all perspectives, like connection scaling was still a problem, but being able to index NoSQL data, being able to index JSON, it was a rare feature even for NoSQL databases.\\n\\n\\n##### _[00:29:19] - Burak_\\n\\nBut you had it in Postgres. I don\'t know, maybe some of these other databases or other use cases Postgres might have something unexpected that would make it better.\\n\\n\\n##### _[00:29:36] - Ry_\\n\\nAn unexpected advantage. Yeah, it\'s the same way with pgvector right now the great thing about doing vector embeddings inside of Postgres is that you don\'t have to move the data out of Postgres as part of the process. Right. You can just add a column and keep it where it is, whereas anybody else has if it\'s an external vector database that\'s specific for that use case, you have to have data pipelines and all that kind of machinery. Which that\'s to me, one of the big benefits of keeping it all in Postgres is less data movement. And less data movement can mean much like no data delays and all that kind of stuff go away. So yeah, I agree with you, there\'s a lot of unexpected benefits for keeping things together.\\n\\n\\n##### _[00:30:25] - Burak_\\n\\nYeah, I guess since Postgres provides pretty strong asset guarantees, it allows you to build things on top of that. And when you have asset, then you can be much more free to develop complex features. Because what I realize is like while developing software, most of the time as a developer, I try to ensure that hey, what I\'m doing is atomic or what I\'m doing does not it is isolate. So it\'s not caused any problems if something is coming and not in the expected order. But you have chance to delegate all this to Postgres, I think that gives you quite a bit of advantage.\\n\\n\\n##### _[00:31:21] - Ry_\\n\\nWell, one of the things I love too is that because the Postgres core team is quite separated from the commercial products, is that I just think it seems like a very stable chassis to build these things on top of. And you really can\'t if it\'s more of a captive, open source project, say, like what Kafka is to Confluent. They can move Kafka quickly if they need to for commercial to help their commercial product, but that could introduce more instability. I just don\'t see this Postgres team doing anything very risky, which to me is a great counterbalance to people developers trying to move fast and build crazy cool new things. It\'s just nice to have that as a stability factor, I think, inside the product.\\n\\n\\n##### _[00:32:17] - Burak_\\n\\nI think so, yeah. Well, I guess historically I think the Postgres community kind of divided into camps and some of them would want to implement new shiny thing and some of them would try to hey, just let\'s get stabilized. And I believe this JSONB support comes from the part who wants to innovate and try new things. And at the beginning, I think they got some hesitation from the other company. But at the end, I guess what they do proved itself to be very valuable. And then now JSONB support is one of the most widely used features of Postgres. Yeah, so I guess there is some sort of balance to try risky things and also try being stabilized.\\n\\n\\n##### _[00:33:10] - Ry_\\n\\nIf you look at the number of the change log for Postgres 16 they did a lot of things. It\'s almost more than anyone can keep in their head. That\'s what I\'m saying. The good stuff gets through, it\'s just the bar is high and then with a lot of assurances that they didn\'t break any part of Postgres in the process. I really appreciate that part of this thing and it\'s one of the reasons why I\'m so excited to be building a product on top of it.\\n\\n\\n##### _[00:33:41] - Burak_\\n\\nYeah, well, at the same time it is sometimes bit frustrating because sometimes you have a feature you want it to be merged in like you might be author or you might be just someone watching from the site and desperately need that feature. And then you see that there is a huge discussion going on and people cannot convince each other and it falls to the next cycle, which is like the one years later. So that\'s a bit frustrating but I guess yeah, it is kind of cost of having a quite stable system.\\n\\n\\n##### _[00:34:18] - Ry_\\n\\nIt\'s the cost. And like I said, well, obviously I haven\'t been here for the 26 years watching the mailing lists and maybe I\'m jumping in here relatively late in a cycle and I just appreciate all the efforts and all the debates and all the fights that have happened there because I think it\'s created such a great core engine. All right, well, so where can listeners find you online? I imagine you\'re on X. Yeah, they.\\n\\n\\n##### _[00:34:50] - Burak_\\n\\nCan find me at X, at LinkedIn. Yeah, I guess those two would be the places they could find me, but mostly at X. My alias is at BYucesoy. So basically my first letter of my name and my last name. Great.\\n\\n\\n##### _[00:35:10] - Ry_\\n\\nWell, we\'re excited to see the outcome of as you guys are shipping Ubicloud. Excited to see that. And yeah, appreciate you joining us today.\\n\\n\\n##### _[00:35:24] - Burak_\\n\\nThanks a lot for having me here, it was great talk, I enjoyed a lot and I\'m definitely looking for the other episodes to release so that I can listen."},{"id":"tembo-operator-apps","metadata":{"permalink":"/blog/tembo-operator-apps","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-11-01-tembo-operator-apps/index.md","source":"@site/blog/2023-11-01-tembo-operator-apps/index.md","title":"Application Services: Helping Postgres Do More, Faster","description":"alttext","date":"2023-11-01T00:00:00.000Z","formattedDate":"November 1, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"kubernetes","permalink":"/blog/tags/kubernetes"},{"label":"rust","permalink":"/blog/tags/rust"}],"readingTime":5.005,"hasTruncateMarker":false,"authors":[{"name":"Adam Hendel","title":"Founding Engineer","url":"https://github.com/ChuckHend","email":"noreply@tembo.io","imageURL":"https://github.com/chuckhend.png","key":"adam"}],"frontMatter":{"slug":"tembo-operator-apps","title":"Application Services: Helping Postgres Do More, Faster","authors":["adam"],"tags":["postgres","kubernetes","rust"]},"prevItem":{"title":"Hacking Postgres Ep. 7: Burak Yucesoy","permalink":"/blog/hacking-postgres-ep7"},"nextItem":{"title":"Hacking Postgres, Ep. 6: Regina Obe and Paul Ramsey","permalink":"/blog/hacking-postgres-ep6"}},"content":"![alt_text](./tembo_ele.png \\"Tembo the Operator\\")\\n\\nSo you have a database, and that database does something. (Probably several somethings, if we\'re honest). However, today, you need it to do something else. \\n\\nSimple enough...you just create a tool to give it that new functionality. Job done.\\n\\nExcept it isn\'t. Because the requests for new \\"things\\" never stop. They get complicated. They slow things down. They conflict with one another. Sometimes they even screw up your database along the way. And if we\'re honest, you don\'t really want to be constantly building new tools for every new need anyway. Before you know it, all of these \\"things\\" you\'re asking the database to do are starting to get in the way of its core performance.\\n\\nThe good news is that Postgres has a rich ecosystem of tools and services built by the community. Many of these run in the database as Postgres extensions, while others run outside the database as external services. Some of the most well known examples are [PostgREST](https://postgrest.org/en/stable/), an out-of-the box REST API for Postgres and [pgbouncer](https://www.pgbouncer.org/usage.html), a production ready connection pooler. A scalable way to run these pieces of software with the Tembo operator is to utilize a new feature called Application Services, which runs these applications in containers next to postgres.\\n\\n## Understanding the Essence of Tembo Operator\\n\\nWe have developed the Tembo Operator looking to add new capabilities for developers and enterprises. We believe that it stands out from other techniques in many ways, and in particular, one important feature is that it lets users deploy applications in containers right alongside their PostgreSQL instances, ensuring efficient connection to Postgres with very low latency.\\n\\n## The Advantage of Running Applications in Separate Containers\\n\\nPostgreSQL is a very powerful piece of software. When running in Kubernetes, a useful pattern is to run important applications in a separate container, in the same namespace. By running these applications in separate containers, we isolate application requirements, such as resource allocations. This means that each container can have dedicated CPU and Memory, ensuring there\'s no competition with the resources reserved for running PostgreSQL. This segregation ensures that the Postgres database and other applications can function and scale efficiently.\\n\\n## Spotlight on PostgREST: A Prime Example\\n\\nPostgREST is a perfect example where an application can run with this pattern. PostgREST serves as a standalone web server that turns your database directly into a RESTful API. The immediate advantage? Developers can use the auto-generated API to build robust applications without writing any code. By simplifying the process and reducing the need for middleware, PostgREST has become a popular tool in the Postgres ecosystem.\\n\\nHowever, let\u2019s remember that  the main advantage of this method is not just resource allocation. It\'s about ensuring the optimal performance of Postgres without bogging it down with additional tasks. \\n\\nLet\u2019s look at an example of a spec that would run the workload that we just described. This will look familiar if you\u2019ve worked with the [Kubernetes Pod spec.](https://kubernetes.io/docs/concepts/workloads/pods/)\\n\\n```yaml\\napiVersion: coredb.io/v1alpha1\\nkind: CoreDB\\nmetadata:\\n  name: my-postgres-deployment\\nspec:\\n  image: \\"quay.io/tembo/standard-cnpg:15.3.0-1-1096aeb\\"\\n  appServices:\\n    - name: postgrest\\n      image: postgrest/postgrest:v10.0.0\\n      routing:\\n        - port: 3000\\n          ingressPath: /\\n      env:\\n        - name: PGRST_DB_URI\\n          valueFromPlatform: ReadWriteConnection\\n        - name: PGRST_DB_SCHEMA\\n          value: public\\n        - name: PGRST_DB_ANON_ROLE\\n          value: postgres\\n```\\n\\nThe Tembo operator always starts postgres with default configurations, so let\u2019s focus on the `appServices` section of the spec, which will tell us what and how we\u2019ll run an application container.\\n\\nWe can run any number of applications in containers near the Postgres instance. Only the `name` and `image` parameters are required, but you can configure commands, arguments, environment variables, CPU and memory, and readiness and liveness probes for the application.\\n\\nIf you need network communication, you can also configure the ingress by specifying a port and a path. In our example, postgREST runs on port 3000 and expects traffic routed to the root path. appServices also support various Middleware configurations, but we will cover those in a future blog.\\n\\nEnvironment variables also have some advanced configuration. The Tembo operator creates a few roles in Postgres, and tracks those credentials in a Kubernetes secret. If we want to pull those credentials into an application, we can do so by using the `valueFromPlatform` option. The service currently supports pulling in credentials for `ReadWriteConnection`, `ReadOnlyConnection` but we\u2019ll be building out more role assignments soon.\\n\\n## Arbitrary container deployments\\n\\nThe Tembo operator is not limited to postgREST; it can run nearly any containerized application. For example, run your Rails application, or FastAPI web server. Specify the image, and other configurations as necessary and the Tembo operator will provision the resources.\\n\\n```yaml\\napiVersion: coredb.io/v1alpha1\\nkind: CoreDB\\nmetadata:\\n  name: my-postgres-deployment\\nspec:\\n  appServices:\\n    - name: my-app\\n      image: quay.io/myImage:latest\\n      routing:\\n        - port: 3000\\n          ingressPath: /\\n      command: [ \\"python\\", \\"-m\\", \\"myapp.py\\" ]\\n      env:\\n        - name: MY_ENV\\n          value: my_value\\n```\\n\\n**Kubernetes Footprint**\\n\\nLet\u2019s take a look at what actually gets created when you specify an appService; we\u2019ll use the postgREST example as an illustration.\\n\\nEvery application service gets its own Kubernetes Deployment. If the appService has any ingress requirements, then a Kubernetes Service and an ingress resource (Tembo currently uses [Traefik](https://doc.traefik.io/traefik/middlewares/overview/)) is created for that appService. Middleware is also created (also Traefik) if the appService has any middleware configuration specified. Here\u2019s an image of the pods you\u2019d likely see in the namespace you create for Postgres.\\n\\n\\n![alt_text](./kube.png \\"image_tooltip\\")\\n\\n\\n## More to follow\\n\\nThe Tembo operator is under continuous development and runs the entire Postgres Stack, which is not limited to just the core Postgres engine. It also includes auxiliary container services that run outside of Postgres. These auxiliary services augment the Postgres experience by running complex applications outside of Postgres which isolates their workload from your database. Running this with the Tembo Operator makes it simple to get these services up and running.\\n\\nAnd if you want to try out the full power of Postgres without being concerned with how it will run, try out [Tembo Cloud](https://cloud.tembo.io/).Drop into our [Slack](https://join.slack.com/t/tembocommunity/shared_invite/zt-20dtnhcmo-pLNV7_Aobi50TdTLpfQ~EQ) channel to ask questions and get help from the Tembo team and other community members."},{"id":"hacking-postgres-ep6","metadata":{"permalink":"/blog/hacking-postgres-ep6","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-10-31-hacking-postgres-ep6/index.md","source":"@site/blog/2023-10-31-hacking-postgres-ep6/index.md","title":"Hacking Postgres, Ep. 6: Regina Obe and Paul Ramsey","description":"In this episode, Ry, Regina, and Paul talk about geospatial development, the challenges of creating and maintaining an extension across multiple Postgres development cycles, and what they\u2019re hoping for in the future of Postgres.","date":"2023-10-31T00:00:00.000Z","formattedDate":"October 31, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"hacking_postgres","permalink":"/blog/tags/hacking-postgres"}],"readingTime":30.215,"hasTruncateMarker":false,"authors":[{"name":"Eric Ankenman","url":"https://github.com/ericankenman","email":"noreply@tembo.io","imageURL":"https://github.com/ericankenman.png","key":"eric"}],"frontMatter":{"slug":"hacking-postgres-ep6","title":"Hacking Postgres, Ep. 6: Regina Obe and Paul Ramsey","authors":["eric"],"tags":["postgres","hacking_postgres"]},"prevItem":{"title":"Application Services: Helping Postgres Do More, Faster","permalink":"/blog/tembo-operator-apps"},"nextItem":{"title":"Tembo Operator: A Rust-based Kubernetes Operator for Postgres","permalink":"/blog/tembo-operator"}},"content":"In this episode, Ry, Regina, and Paul talk about geospatial development, the challenges of creating and maintaining an extension across multiple Postgres development cycles, and what they\u2019re hoping for in the future of Postgres. \\n\\nWatch below, or listen on Apple/Spotify (or your podcast platform of choice). Special thanks to Regina and Paul for joining us today!\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/q3TXM6Nu7Aw?si=El0OVHhXKXkXAoQ1\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n\\nWant to know more about something they mentioned? Here\u2019s a starting point:\\n\\n\\n\\n* SQL Server - [https://www.microsoft.com/en-us/sql-server](https://www.microsoft.com/en-us/sql-server)\\n* PostGIS - https://postgis.net/\\n* Gist - https://www.postgresql.org/docs/current/gist.html\\n* R-tree - https://www.postgresql.org/docs/current/indexes-types.html\\n* KNN - https://postgis.net/workshops/postgis-intro/knn.html\\n* Parallelism - https://www.postgresql.org/docs/current/parallel-query.html\\n* GEOS - https://libgeos.org/\\n* CartoDB - https://carto.com/\\n* Crunchy Data - https://www.crunchydata.com/\\n* OGR - https://github.com/pramsey/pgsql-ogr-fdw\\n* Paragon Corporation - https://www.paragoncorporation.com/\\n\\nDid you enjoy the episode? Have ideas for someone else we should invite? Let us know your thoughts on X at @tembo_io or share them with the team in our [Slack Community](https://join.slack.com/t/tembocommunity/shared_invite/zt-23o25qt91-AnZoC1jhLMLubwia4GeNGw).\\n\\n\\n## Transcript\\n\\n\\n##### _[00:00:12] - Regina_\\n\\nHi, I\'m Ry Walker, founder of Tembo, a managed Postgres company. And today I have Regina Obe and Paul Ramsey, two who are working and have been working on PostGIS for quite some time. Welcome to the show, both of you.\\n\\n\\n##### _[00:00:28] - Regina_\\n\\nThanks, Ry.\\n\\n\\n##### _[00:00:28] - Paul_\\n\\nThanks, Ry. Nice to be here. Great. Yeah.\\n\\n\\n##### _[00:00:32] - Ry_\\n\\nSo I think you guys have all been using Postgres for quite some time. Maybe. Paul, when did you start using Postgres?\\n\\n\\n##### _[00:00:42] - Paul_\\n\\nI started using Postgres, and I\'m sure I can give you a version number, I think like 6.4, which I think puts me in like the \'98, \'99 era. And yeah, I was using it as a consultant. My first career was in consulting and working for the provincial government. And we were doing a big data analysis project and it was a geospatial project, but all the geospatial processing was being done by an external piece of software and we used Postgres as the backing store for what was a very long, like, 20 day compute cycle to spit out all the results for the entire province. \\n\\n\\n##### _[00:01:17] - Ry_\\n\\nNice. How about you, Regina?\\n\\n\\n##### _[00:01:18] - Regina_\\n\\nI started out as a SQL Server person, so yeah, Paul dragged me into Postgres. So my first introduction was via PostGIS in 2001, I think it was. So it was like 7.1 or 7.3, I can\'t remember somewhere between 7.1 and 7.3.\\n\\n\\n##### _[00:01:39] - Paul_\\n\\nIt was 7.1 because that was the first version of Postgres that you could actually do a geospatial extension. Oh, nice.\\n\\n\\n##### _[00:01:45] - Ry_\\n\\nAnd do you regret leaving SQL Server?\\n\\n\\n##### _[00:01:49] - Regina_\\n\\nWell, I still consult for SQL Server, and what\'s interesting about that is I think I\'ve gotten to be a better SQL Server expert knowing Postgres because Postgres would always introduce things first, like all the lead functions, all the window functions, Postgres introduced those before SQL Server had them and the CTEs and everything.\\n\\n\\n##### _[00:02:18] - Ry_\\n\\nYeah, got it. What\'s the state of geospatial in Microsoft SQL Server, would you say compared to Postgres? I don\'t know too much about that ecosystem, but I\'m curious, how much parity have they achieved?\\n\\n\\n##### _[00:02:32] - Regina_\\n\\nWell, I think their geography is still better than ours, but in terms of the geometry support, they\'re way behind, which is what most of the state people care about.\\n\\n\\n##### _[00:02:42] - Ry_\\n\\nAwesome. Cool. All right, well, I\'m going to kind of go off my standard script because how long have you guys been working on PostGIS for? This is one of the oldest extensions, or is it the oldest extension?\\n\\n\\n##### _[00:02:59] - Paul_\\n\\nYeah, well, I mean, so when we started extension, this didn\'t exist. It\'s a pre extension as a package concept. It\'s just like the idea of runtime, adding things at runtime to the database which was there from the very beginning. That was a stonebreaker original. Like, this is one of the things Postgres can do. PostGIS is from 2001, as Regina mentioned, she started using it within three months of the initial release. So, yeah, may of 2001. So yeah, we\'re at 22 years.\\n\\n\\n##### _[00:03:28] - Ry_\\n\\nSo how has it, has it changed much? Would you say since? Has it been pretty steady progress towards what it is now? Or were there any big milestones, big changes that you can remember that are super noteworthy?\\n\\n\\n##### _[00:03:42] - Paul_\\n\\nIt has not been like one linear rise. It\'s definitely been punctuated equilibrium. And as these things go, the most new features and new capabilities happen right at the start, because we started with nothing, right? We started with first release. It had geospatial types, it had a geospatial index. It had eight functions, I think only one of which was analytical in any way. But that was like release 0.1 in 2001. I think we got to 0.8 in two years, like 2003. And that had the first full set of spatial functions where you could really test all the relationships between the geometries. You could do constructive geometry operations like buffering intersections and unions and so on in the geometries, that was the really big like, you could have stopped there in many respects. If you look at what SQL Server has, that\'s kind of what they did until 2008. They had no spatial at all. And then they came out with their spatial extension, which was basically what they have now because it was a capable, complete vector spatial database, full stop. But since then, we\'ve kept adding stuff. Maybe Regina will take another tour of what she thinks the biggest things to happen since 0.8 were yeah, so I.\\n\\n\\n##### _[00:05:08] - Regina_\\n\\nThink PostGIS just improved as Postgres improved. So they introduced Gist and all our indexes changed from the old R-tree to the Gist index. They improved aggregation in Postgres, which I think was a huge milestone for us because a lot of the processing we do involves aggregating geometries together. So we would see something like from a tenfold speed improvement in terms of aggregation of geometries. And then the other things. I think CTEs were pretty useful. Now, nobody does any queries in Spatial without using a CTE anymore.\\n\\n\\n##### _[00:06:01] - Ry_\\n\\nWould you say, like those advances in Postgres? I don\'t know. Let\'s say that they did 80% of the work towards the thing working ten times faster, and you guys had to do 20%. Or what was the ratio of effort from where were you getting great improvements virtually for free without any effort on your side?\\n\\n\\n##### _[00:06:21] - Regina_\\n\\nYeah, I think we were getting great improvements for free, pretty much.\\n\\n\\n##### _[00:06:25] - Ry_\\n\\nOh, that\'s great.\\n\\n\\n##### _[00:06:25] - Regina_\\n\\nAnd then there\'s the whole KNN thing, which they drastically improved from 9.2 to yeah, I was making fun of Bruce, and then Bruce said, I can\'t believe you\'d have to do that. We need to fix it. And so after 9.2, the KNN became real KNN instead of just box KNN. But yeah, in all those cases, there wasn\'t that much we needed to do to get the improvement.\\n\\n\\n##### _[00:06:49] - Paul_\\n\\nParallelism.\\n\\n\\n##### _[00:06:50] - Regina_\\n\\nOh, yeah, parallelism.\\n\\n\\n##### _[00:06:51] - Paul_\\n\\nAnother one of which is like a huge freebie. Like, look at that. We do parallel processing.\\n\\n\\n##### _[00:06:57] - Regina_\\n\\nYeah, the only thing we needed to do is tell people how to configure their Postgres comps to take advantage of parallelism.\\n\\n\\n##### _[00:07:05] - Ry_\\n\\nCurious, what kind of challenges did you guys face? I\'d say early on building this.\\n\\n\\n##### _[00:07:14] - Paul_\\n\\nThe biggest thing? Well, I don\'t know. There are a lot of things. As a geospatial extension, our code isn\'t packed like into one hunk of code that we control. We use a lot of library dependencies. And so we end up having, and we still do having a really complex build almost right out of the gate compared to other extensions, because other extensions like, hey, we\'re a self contained thing. You just type, make and you\'re done. Whereas with us it was always like, hey, guess what? You get to track down three or four different other libraries in addition to the thing that we have and make sure they\'re the right versions, and here\'s the configuration, so on. So we end up with this really naughty configuration setup. And initially, like, if you\'re going to start using this stuff back in the day, step one was always to build it. They weren\'t prepackaged. Postgres itself wasn\'t prepackaged, everything was from source, so it put this fairly steep entryway in for new users.\\n\\n\\n##### _[00:08:14] - Regina_\\n\\nYeah, although early on, we didn\'t have any other dependencies except Postgres, so it was much easier.\\n\\n\\n##### _[00:08:19] - Paul_\\n\\nWell GEOS after two years and then, well, proge at the same time. I mean, we had GEOS and proj almost from the start, and then started picking up format libraries after that.\\n\\n\\n##### _[00:08:28] - Regina_\\n\\nDo we have proj?\\n\\n\\n##### _[00:08:29] - Paul_\\n\\nYeah, I don\'t remember projection because proj already existed, so tying it in was a pretty straightforward thing to do.\\n\\n\\n##### _[00:08:35] - Regina_\\n\\nOkay, I don\'t remember that. Or maybe it was an optional dependency, so I just never built with it.\\n\\n\\n##### _[00:08:43] - Paul_\\n\\nEverything was optional.\\n\\n\\n##### _[00:08:44] - Ry_\\n\\nTell me what GEOS is. I\'ve seen that mentioned, but I didn\'t dive in. Is that powering other geospatial solutions besides PostGIS?\\n\\n\\n##### _[00:08:55] - Paul_\\n\\nYeah, it is. GEOS is an acronym of course, stands for Geometry Engine open source. It provides the computational geometry underpinnings for a bunch of sort of the key functions. I mentioned a buffer, that\'s a GEOs function at the back end. Intersection is a GEOS function at the back end. The Boolean predicates intersects contains within those are all GEOS functions at the back end. Some of the fancier stuff that we\'ve added to GEOS is now exposed in PostGIS. So if you ask for Delaunay triangles or [...] polygons, you get those from GEOS and then GEOS, because it\'s like this useful Swiss Army knife of computational geometry is used by other programs in the geospatial ecosystem. So if you use the Google library, it backs out of GEOS for its geospatial operations. And like most prominently, if you use the QGIS desktop GIS, you\'ll find that the Q just backstops its geospatial algorithms.\\n\\n\\n##### _[00:09:54] - Ry_\\n\\nSo did you guys refactor code into that, or did you just end up replacing some stuff you had built early on with that library later on?\\n\\n\\n##### _[00:10:01] - Regina_\\n\\nWell, Paul started GEOS too.\\n\\n\\n##### _[00:10:04] - Ry_\\n\\nOkay. All, it\'s okay. So it sounds like refactored into that.\\n\\n\\n##### _[00:10:09] - Regina_\\n\\nWell, not so much refactor. I think it was always kind of a separate thing. Right. It was always intended to do more.\\n\\n\\n##### _[00:10:15] - Paul_\\n\\nAnd as an external library, there\'s actually a slight penalty to using it because you got to take your data out of the Postgres memory space and copy it over into the JS memory space to work with it. And that\'s just expensive enough that for simple things, we still actually kept the original native post, just side implementations for things like area or length or the most complex one that we kept is distance. Yeah.\\n\\n\\n##### _[00:10:45] - Ry_\\n\\nAre there any milestones going in the future for PostGIS that you\'re looking forward to or is it just kind of stability and continuous improvement?\\n\\n\\n##### _[00:10:56] - Paul_\\n\\nYou go, Regina.\\n\\n\\n##### _[00:10:58] - Regina_\\n\\nOh, no, you\'re not going to ask me that. I think speed is always good, and my concern, I think, is mostly improving Raster. And I\'m looking forward to Toast\'s API changes that are coming along and how we could leverage those.\\n\\n\\n##### _[00:11:16] - Paul_\\n\\nThat\'s funny you bring up raster. Maybe we should talk about this in the part we have a little get together, because it\'s one of the things which in front of me lately is like the underlying infrastructure we have for handling rasters was built well, no, 2010, anyways. It was built, like, in an era when the idea of, say, doing cloud based raster processing was kind of not what would be done. It was built around the idea that you would store your own rasters kind of locally in your local area network, that increasingly organizations just don\'t do that. They still want to have raster access. And while you can do remote raster access with what we have, it\'s kind of clunky. It\'s not optimized for that at all. I feel like just like a relook at what the raster use cases are and kind of a reevaluation of whether how we handle rasters is right is due. I was looking at the API that Alibaba offers on their cloud databases and thinking, yeah, that\'s kind of an interesting way of tackling rasters.\\n\\n\\n##### _[00:12:17] - Ry_\\n\\nYeah, you\'re talking about like Alibaba is like a they have a proprietary database or not, their Postgres.\\n\\n\\n##### _[00:12:25] - Paul_\\n\\nThey don\'t say. They\'re really cagey about it. So I\'m not sure whether it\'s back then Postgres or not, but their take on rasters is very much all their own. I haven\'t seen anyone else go at it that way.\\n\\n\\n##### _[00:12:35] - Regina_\\n\\nOh, I haven\'t seen that. I should check that out. But yeah, that\'s one of the complaints that people have, at least the clients I have, that the outdb is much slower and yeah, that could be improved.\\n\\n\\n##### _[00:12:48] - Paul_\\n\\nYeah, great.\\n\\n\\n##### _[00:12:51] - Ry_\\n\\nI\'m curious if you\'re paying attention. I don\'t know the difference. I\'ve not studied how PostGIS works or raster, and it is but I\'m curious, is the vector search stuff happening that\'s happening in the ML space? How closely related is the math of that to the math of I\'m sure you\'ve kind of paid a little bit of attention to that space. Is it wildly different or is it kind of remarkably similar or neither of those?\\n\\n\\n##### _[00:13:23] - Paul_\\n\\nYeah, well, I mean, insofar as a 2D vector is the same as a [...] vector. At a conceptual level, they\'re the same. But from a practicality point of view, the practicalities of handling super high dimensional stuff are just different. Like one of the first things you learn, even like we go to four dimensions, even at four dimensions, the indexing properties start to break down if the kind of sort of standard R-tree stuff just doesn\'t work as nicely. You don\'t have to get very high up into a complex dimensional space for that form of indexing to be like, it doesn\'t work, it\'s not doing what we need to do. And you can really see that in just how different the indexing approaches are for ML vectors compared to sort of 2D vectors.\\n\\n\\n##### _[00:14:10] - Ry_\\n\\nSo, like, high dimensionality just requires an entirely different mindset and solution set. So I\'m hearing you say.\\n\\n\\n##### _[00:14:17] - Paul_\\n\\nYes, totally.\\n\\n\\n##### _[00:14:18] - Ry_\\n\\nJust curious if it somehow scales into that or not. That\'s cool. Yeah. Well, tell me, I guess, real quick, I\'d love to learn a little bit more about the commercial products, I guess. How does this manifest? How does PostGIS manifest commercially for both you and Regina?\\n\\n\\n##### _[00:14:41] - Regina_\\n\\nFor my side, it\'s mostly consulting. Yeah, I don\'t have any commercial things around.\\n\\n\\n##### _[00:14:52] - Paul_\\n\\nI make sideline in talking about and studying the economics of open source. One of the things that\'s kind of obvious once you start looking at this stuff is that there\'s like a project size threshold before you start to see enough money around a project to support full time maintainership or people whose jobs are mostly around the project. And PostGIS is interesting in being like one of the few Postgres extensions which has received achieved that level. But even at that level, it\'s quite small. So you got Regina, who has a good consulting business. I work for Crunchy Data, which is a professional open source support company, which is to say they sell support contracts to Fortune 100 companies and US Federal government and increasingly a number of international organizations, but of similar size and scale, big organizations. And then also has a software as a service called Crunchy Bridge, which is basically database in the cloud of the sort that everyone\'s gotten used to. So, I mean, in that respect, I\'m kind of like Regina. I work for Crunchy because they value my expertise as a PostGIS committer and my ability to help their customers who deploy PostGIS.\\n\\n\\n##### _[00:16:20] - Paul_\\n\\nSo it\'s still very much like skills for hire. No one has wrapped it up, has wrapped PostGIS itself in particular up as a specific product.\\n\\n\\n##### _[00:16:28] - Regina_\\n\\nYeah, I mean, others have, it\'s just.\\n\\n\\n##### _[00:16:30] - Paul_\\n\\nWe haven\'t and then yeah, other members of the development team are also still sort of on the consulting bandwagon and that\'s how it bundled.\\n\\n\\n##### _[00:16:41] - Ry_\\n\\nI\'m not familiar with anyone who\'s bundled it up as a product per se, who\'s done that?\\n\\n\\n##### _[00:16:47] - Regina_\\n\\nI mean, it\'s not so much a product, but like all the cloud providers so Amazon has it, Microsoft has an installable extension. Yeah. As an installable.\\n\\n\\n##### _[00:16:59] - Paul_\\n\\nFrom the point of view of ubiquity and market spread.\\n\\n\\n##### _[00:17:03] - Regina_\\n\\nYeah. CartoDB used to be the closest though. But do they still use Postgres or did they switch to something else?\\n\\n\\n##### _[00:17:09] - Paul_\\n\\nWhich DB?\\n\\n\\n##### _[00:17:10] - Regina_\\n\\nCartoDB. Carto.\\n\\n\\n##### _[00:17:12] - Paul_\\n\\nThey still use Postgres. Yeah. They haven\'t moved up. That would be a good sort of like example of a productization of PostGIS. Certainly in their earliest incarnation they had a software as a service which did a very good job of allowing you to put data in, visualize it in a whole bunch of ways. And that exposed like SQL as the language for customization of what you were seeing. And it was all sitting on top of PostGIS, but it was marketed as CartoDB. So they had productized around a software service that more or less made the database not invisible, but the actual brand of the database was irrelevant.\\n\\n\\n##### _[00:17:51] - Ry_\\n\\nDo you see real old versions of PostGIS surface? Like, I\'m sure you probably don\'t see 0.8 anymore, but no. How good are people at staying up on the latest, would you say? I have not as good as you.\\n\\n\\n##### _[00:18:08] - Regina_\\n\\nHaven\'t seen any 1.5 recently. I think there might have been one.\\n\\n\\n##### _[00:18:16] - Paul_\\n\\nYour standards are different from mine, Regina, because I,...I\'d freak out if someone brought me a 1.5. I\'m shocked at how many version two is just still in the wild. Let me account back that first digit is worth about a year. So we\'re at 3.4 now. So 3.0, so that\'s five years. So yeah. So some shows up with a two point something. It\'s a five year old installation.\\n\\n\\n##### _[00:18:35] - Regina_\\n\\nYeah.\\n\\n\\n##### _[00:18:35] - Ry_\\n\\nAnd they\'re probably five versions of Postgres old too, right?\\n\\n\\n##### _[00:18:39] - Paul_\\n\\nYeah, exactly.\\n\\n\\n##### _[00:18:41] - Ry_\\n\\nWhat\'s the biggest jump you\'ll do? Will you take someone from eleven to 15 or Postgres? That is the equivalent?\\n\\n\\n##### _[00:18:48] - Regina_\\n\\nYeah, because the latest wouldn\'t even run on that. Well, in theory, anybody from PostGIS 2...\\n\\n\\n##### _[00:19:00] - Regina_\\n\\nshould be able to go straight to 3.4 without any issue as long as they upgrade their Postgres too. What happens to their development, their applications? Breaking that\'s on them.\\n\\n\\n##### _[00:19:15] - Ry_\\n\\nWell, it must be nice to be relying on Postgres. I think you can criticize, if you would like, various aspects of how Postgres is built, but I think that it\'s really great how stable, I want to say slow that the progress is made because it gives you a very stable and reliable chassis to build this on top of. I\'m sure you guys agree with that.\\n\\n\\n##### _[00:19:44] - Regina_\\n\\nYeah, I think in terms of slowness, they\'re actually much faster than other relational databases.\\n\\n\\n##### _[00:19:49] - Paul_\\n\\nMuch, much faster in terms of how fast things change. Yeah, I guess there\'s always that push particularly thing. New SQL dialect stuff has come in pretty quick.\\n\\n\\n##### _[00:19:58] - Regina_\\n\\nYeah, because I remember when I be talking to my SQL Server friends and they\'re still waiting for the lead function. That happened like five years ago back in the day. But yeah, I think it moves in terms of the SQL standard a lot faster than the others. I think even faster than Oracle, though I don\'t have too many Oracle friends to talk about.\\n\\n\\n##### _[00:20:20] - Paul_\\n\\nYeah. I\'m frequently, frequently surprised by how much internal churn there is because I always feel like, oh, we\'re the supermature extension. We don\'t reach like super deep into the extension, like into the into the core. We\'re not hooking into like executor hooks or planner hooks or stuff like that. And yet there\'s always this there\'s always this medium to long list of things that have to be tweaked when you move up to a new, like in terms of our code or for it to build and still run correctly, that have to be tweaked at each major Postgres release.\\n\\n\\n##### _[00:20:57] - Regina_\\n\\nYeah, because I can\'t think of any release we\'ve done that we didn\'t have to tweak it for the in development major release of Postgres. So they changed it enough that it always affects yeah.\\n\\n\\n##### _[00:21:10] - Paul_\\n\\nYeah.\\n\\n\\n##### _[00:21:10] - Ry_\\n\\nSo even if the outer shell seems pretty stable, the insides are changing and you guys have some stuff poking down at least to the middle, if not the bottom. That\'s great. Yeah, like I said, I think it\'s to me a really perfect pace because we do get that kind of like annual innovation and if there\'s something that\'s really important, it\'ll get taken care of, I think. I\'m curious, are there things happening in Postgres core that you guys are excited about? Maybe for we could talk about 17 or whatever, 18 future? Is there anything in particular that you guys are excited to see?\\n\\n\\n##### _[00:21:53] - Regina_\\n\\nWell, there\'s bi directional seems to be making its way. I don\'t know if it\'s going to make it into 17, but it looks like they\'re putting in the hooks for it anyway, so that\'s kind of exciting.\\n\\n\\n##### _[00:22:05] - Paul_\\n\\nYeah, for me, that\'s what\'s exciting. I\'ve been watching the decade long crawl towards being able to do a replicated OLAP system where you get full push down to all the nodes. So every release, the Postgres FTW folks from Toshiba and other places add a couple more turns of stuff that you can push down, which is exciting to me because we\'re that much closer. We\'re that much closer to be able to do big multi node queries because there\'s OLAP workloads and LTP workloads for spatial databases. Really, the bias is towards OLAP for sure. In terms of what you see, how you see people using the database. I mean, they like transactionality, they like to be able to manipulate their data. But when it comes time to ask, what is this for? It\'s like, oh yeah, we run big analyzes. So the ability to push stuff out to multi node as that gets more mature, as it gets more possible, like, that becomes something that\'s really exciting on the spatial side. So I watch every little tick of the gears towards that endpoint and get very excited. So it\'s been pushed down in the last couple of releases.\\n\\n\\n##### _[00:23:22] - Paul_\\n\\nLast release had good stuff around parallelization in the planner and executor as well for partitions, which is like, that\'s a big deal because the holy grail is you\'ve got your big table partitioned out across the remote nodes. So each of your partition is actually a foreign table. And when you run the query, the planner says, oh, look, I can just ask all the partitions to run it simultaneously, it gets back the result, assembles it and says, Here you go, we\'re getting close to that. Which would be a big deal because there\'s a whole bunch of workloads that you can just turn into sort of like a standard OLAP star schema. One big fact table, a bunch of dimensions and you\'d be able to do lots of really cool spatial stuff. That right now, not so much. I don\'t know when we\'ll ever get to the holy grail, which is to be able be able to do shipping data between nodes in order to allow nodes do things like join across these across two big fact tables. That might never happen. I don\'t see anyone doing that. But that\'s the one that would like having me tear my clothes off and dancing in the street.\\n\\n\\n##### _[00:24:34] - Ry_\\n\\nYeah. So that\'s interesting. You\'re talking about like I haven\'t followed the development of Postgres FDW. I used it recently and it\'s pretty powerful at low scale, but you\'re talking about at high scale, at OLAP scale, just optimization across.\\n\\n\\n##### _[00:24:53] - Regina_\\n\\nAnd Paul has this fantastic foreign data wrapper, a spatial foreign data wrapper, which can read how many formats? 800?\\n\\n\\n##### _[00:25:03] - Paul_\\n\\nYeah, no, there\'s not that many formats in the world, but I don\'t know several different yeah.\\n\\n\\n##### _[00:25:08] - Ry_\\n\\nFormats of geospatial formats. Is that what you said? Or other types of formats.\\n\\n\\n##### _[00:25:14] - Paul_\\n\\nGeospatial formats. But to an extent, geospatial formats is a category which also includes non geospatial things because all you have to do is not have the geospatial column and then what do you call it? Oh, it\'s just a format. So Excel can be a geospatial format. Got it. And it\'s something that you can read. SQL Server.\\n\\n\\n##### _[00:25:32] - Ry_\\n\\nWhat\'s the name of that extension?\\n\\n\\n##### _[00:25:34] - Paul_\\n\\nOr that extension is called OGR. Under more FDW.\\n\\n\\n##### _[00:25:37] - Ry_\\n\\nOGR.\\n\\n\\n##### _[00:25:38] - Paul_\\n\\nIt stands for well, OGR is like it\'s not even worth trying to unpack it. OGR is just the vector side of the GDL library, which also has an acronym which is not worth unpacking anymore because it\'s a 20 year old acronym, but it refers to the library that it\'s binding. This is the OG library for vector formats? Yeah.\\n\\n\\n##### _[00:25:58] - Ry_\\n\\nCool. Yeah, I\'ll check that out. That\'s cool. Kind of wrapping this up on the core. Like if you had a magic wand and you could add any feature to know. Of course, if that was added, then you immediately have work to do in PostGIS as well. But what would your magic wand manifest this weekend in Postgres if you could pick one thing?\\n\\n\\n##### _[00:26:20] - Regina_\\n\\nYeah, I can\'t think of any.\\n\\n\\n##### _[00:26:21] - Paul_\\n\\nI know what Regina\'s is. Oh, come on. I\'ll tell you what yours is then, Regina, if you\'re not going to say.\\n\\n\\n##### _[00:26:30] - Ry_\\n\\nYou guys can answer for each other if you want.\\n\\n\\n##### _[00:26:32] - Paul_\\n\\nIt\'s handling extensions in a slightly different way or sorry, had an extension update in a slightly different way. So extension versioning.\\n\\n\\n##### _[00:26:40] - Regina_\\n\\nOh, yeah. How did you read my mind? I completely forgot about it because I gave up hope on it.\\n\\n\\n##### _[00:26:47] - Ry_\\n\\nYeah, tell me about that.\\n\\n\\n##### _[00:26:48] - Regina_\\n\\nSo Sandro, who\'s also on the PostGIS team, he\'s been working on an update to the extension machinery that would allow us to reduce our upgrade script to one file because right now we ship like 500 files, which are all pretty much just SIM links to the same thing. And so it\'s just a way to have the extension machinery understand that, hey, this script can be used to upgrade this version, this version, this version and this version, instead of having to itemize every single version upgrade.\\n\\n\\n##### _[00:27:30] - Paul_\\n\\nYeah, the extension upgrade machinery is very clever, but it starts with an immediate assumption, which is that as a developer, you will manage your extension upgrades as incrementals between versions and that it will cleverly find the path through the incrementals from the version you have to the version you\'re going to, applying all the little slices on the way.\\n\\n\\n##### _[00:27:55] - Regina_\\n\\nAnd it uses Dijkstra for that.\\n\\n\\n##### _[00:27:58] - Paul_\\n\\nYeah, it\'s super clever. Even like in the best case scenario where you\'d already been doing that, it\'s probably not super ideal for any project where the development path isn\'t linear. So Post just has, I don\'t know, 25 or so minor releases like X, Y, and then within those, there\'s maybe five patch releases across each of those. So we\'ll have a whole bunch of parallel version trees, right? You\'re on 2.3.5, or you\'re going to go to 2.3.6, but then you might want to go to 2.4.8. And that means you have to have all these individual even if you\'re doing things like one tiny step of time, you would have all these individual little hops across the branches. If you have just this one line, it kind of worked. You just sort of chained together this one little line. You don\'t have very many files when you have all these little branches, all of a sudden you need all these extra little hops across the branches. And it\'s made worse because all of our management of the SQL side of it, the SQL side of the extension where you define the functions on SQL land and say, oh, it\'s over here in the dynamic library.\\n\\n\\n##### _[00:29:24] - Paul_\\n\\nWe\'ve been managing that since pre extension world. And our way of managing it was to have a SQL file which can always be cleanly applied against any previous version. So it\'s quite long because the SQL file has every single definition of every single thing in it. So how you handle the incremental thing from 1.2 to 1.3? Well you have one copy for 1.31 copy for the upgrade as well. So every little upgrade has a full copy of that fairly large SQL file. On Unix systems now we just ship sip links instead of syncing the whole file. But you end up with just a huge pile.\\n\\n\\n##### _[00:30:05] - Regina_\\n\\nYeah, actually we changed from that to just a file that has nothing in it. Right. To just on any version which kind.\\n\\n\\n##### _[00:30:13] - Paul_\\n\\nOf the chain eventually arrives at the full one.\\n\\n\\n##### _[00:30:17] - Regina_\\n\\nSo now that\'s the same across. But I think ours is more complicated too because for each version we support multiple versions of Postgres and we also enable new features. If you\'re on like twelve you get something, if you are eleven you don\'t get that something.\\n\\n\\n##### _[00:30:40] - Paul_\\n\\nCertainly something which is not contemplated by the original designers is our file. Our input file actually goes through, we put it through the C preprocessor before we give it to Postgres because we have a whole bunch of if defs against what Postgres version you\'re on. Living inside the SQL file that have to be pre processed before it\'s usable.\\n\\n\\n##### _[00:31:02] - Ry_\\n\\nYeah, I understand. Was maybe you\'re saying like the current design is just naive thinking that you\'re not going to try to support multiple versions of Postgres with one of your versions of the extension and there\'s not home for that information, I guess for what the range is to some degree.\\n\\n\\n##### _[00:31:22] - Paul_\\n\\nYeah, I mean although the extension framework does contemplate the idea of versioned extensions, again, it doesn\'t really contemplate them as anything except for a linear chain. And once you have a more complex situation than that it\'s kind of hard. Like we for a very long time supported being able to run different versions of PostGIS inside the same Postgres cluster. We still do actually support that, but it\'s a feature that it seems like mostly OA developers use. So it\'s optional now and we default just to like one version of Postgres or PostGIS for each Postgres cluster. But that functionality was always there. But the extension facility did not grok that.\\n\\n\\n##### _[00:32:05] - Regina_\\n\\nYeah, and packagers did not grok that either. So they always, always ship one.\\n\\n\\n##### _[00:32:10] - Ry_\\n\\nGreat. I\'m curious, try to wrap up here a little. I realize now I\'ve been kept you here for quite some time, but do either of you listen to podcasts very much?\\n\\n\\n##### _[00:32:20] - Paul_\\n\\nI do all the time. It\'s my gym thing. I go down to the garage gym and that\'s what keeps me from going crazy with me.\\n\\n\\n##### _[00:32:28] - Ry_\\n\\nYour give me some of your favorite podcasts.\\n\\n\\n##### _[00:32:31] - Paul_\\n\\nI tend to go on the current affairs side, so I listen to the Ezra Klein show from New York Times a lot and Odd Lots from Bloomberg, a little bit of financial news.\\n\\n\\n##### _[00:32:40] - Regina_\\n\\nYeah, you can tell he\'s the son of a politician.\\n\\n\\n##### _[00:32:42] - Paul_\\n\\nIt\'s interesting stuff.\\n\\n\\n##### _[00:32:44] - Ry_\\n\\nYeah. How about you, Regina?\\n\\n\\n##### _[00:32:47] - Regina_\\n\\nNo, I\'m not a podcast person. I go swimming. But yeah, I can\'t really hook up a podcast.\\n\\n\\n##### _[00:32:55] - Ry_\\n\\nYeah, underwater is probably possible, but not super comfortable. Right, great. All right, well, so where can listeners find you online? Maybe share your websites or Twitter? Mastodon handles.\\n\\n\\n##### _[00:33:13] - Paul_\\n\\nOn the site formerly known as Twitter. I\'m PWRamsey. I\'m also PWRamsay at Mastodon Social and on the blog world, I\'m at cleverelephantCA.\\n\\n\\n##### _[00:33:25] - Regina_\\n\\nYeah, I have how many blogs do I have? I have BostonGIS.com Postgresonline.com. Twitter is just Reginaobe, I think that\'s and my website, Paragoncorporation.com, I guess those are. Oh, and my book site PostGIS.us.\\n\\n\\n##### _[00:33:47] - Paul_\\n\\nYeah, the book site.\\n\\n\\n##### _[00:33:49] - Ry_\\n\\nWhat\'s that? I always say PostGIS, by the way. I got to learn it\'s PostGIS. But do you have a book for around it, or have you written many books?\\n\\n\\n##### _[00:33:57] - Regina_\\n\\nOh, yeah. So I wrote \\"PostGIS in Action.\\" I\'m working on \\"PG Routing.\\" I\'m also supposedly working on a Postgres book, both of which I\'m very behind on. And I did \\"SQL in a Nutshell.\\" And let\'s see, what else. Is that it? Oh, and \\"Postgres Up and Running.\\" That\'s a pretty popular book. Surprisingly popular.\\n\\n\\n##### _[00:34:25] - Ry_\\n\\nYeah, it\'s sitting right there. I own that one.\\n\\n\\n##### _[00:34:28] - Regina_\\n\\nOh, really?\\n\\n\\n##### _[00:34:29] - Ry_\\n\\nThanks for writing it.\\n\\n\\n##### _[00:34:29] - Regina_\\n\\nOh, yeah.\\n\\n\\n##### _[00:34:31] - Ry_\\n\\nThank you both for joining. Appreciate you, all the work you\'ve done for Postgres and PostGIS, and appreciate having you on the show.\\n\\n\\n##### _[00:34:42] - Paul_\\n\\nThanks for having us, Ron.\\n\\n\\n##### _[00:34:44] - Regina_\\n\\nThanks, Ryan.\\n\\n\\n##### _[00:34:44] - Paul_\\n\\nThanks. Bye."},{"id":"tembo-operator","metadata":{"permalink":"/blog/tembo-operator","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-10-25-tembo-operator/index.md","source":"@site/blog/2023-10-25-tembo-operator/index.md","title":"Tembo Operator: A Rust-based Kubernetes Operator for Postgres","description":"At Tembo, we\u2019ve been developing an open-source Kubernetes Operator for Postgres. We use this operator to power our managed Postgres platform, Tembo Cloud. We\u2019re excited to share our progress, experience, and vision for this project. This post aims to assist anyone interested in utilizing Kubernetes operators for Postgres or writing Kubernetes operators using Rust.","date":"2023-10-25T00:00:00.000Z","formattedDate":"October 25, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"kubernetes","permalink":"/blog/tags/kubernetes"},{"label":"rust","permalink":"/blog/tags/rust"}],"readingTime":7.055,"hasTruncateMarker":false,"authors":[{"name":"Ian Stanton","title":"Founding Engineer","url":"https://github.com/ianstanton","email":"noreply@tembo.io","imageURL":"https://github.com/ianstanton.png","key":"ian"}],"frontMatter":{"slug":"tembo-operator","title":"Tembo Operator: A Rust-based Kubernetes Operator for Postgres","authors":["ian"],"tags":["postgres","kubernetes","rust"],"image":"./k8s-operator.webp"},"prevItem":{"title":"Hacking Postgres, Ep. 6: Regina Obe and Paul Ramsey","permalink":"/blog/hacking-postgres-ep6"},"nextItem":{"title":"Anonymized dump of your Postgres data","permalink":"/blog/anon-dump"}},"content":"At Tembo, we\u2019ve been developing an open-source Kubernetes Operator for Postgres. We use this operator to power our managed Postgres platform, [Tembo Cloud](https://cloud.tembo.io/). We\u2019re excited to share our progress, experience, and vision for this project. This post aims to assist anyone interested in utilizing Kubernetes operators for Postgres or writing Kubernetes operators using Rust.\\n\\n## What is a Kubernetes Operator?\\n\\nKubernetes was designed with automation in mind, and operators allow for users to extend native Kubernetes behavior and principles to manage custom resources and components.\\n\\nWith a Kubernetes operator, users can write code that defines how their application should be deployed and managed on Kubernetes. This code is then packaged into a container image and deployed to Kubernetes. The operator then watches for changes to the custom resource and takes action to reconcile the state of the application\u2019s components with the desired state of the custom resource.\\n\\nIn short, using a Kubernetes operator is the most effective way to run applications on Kubernetes in 2023.\\n\\nYou can read more about Kubernetes operators on this [CNCF blog post](https://www.cncf.io/blog/2022/06/15/kubernetes-operators-what-are-they-some-examples/), where the image below is.\\n\\n![./k8s-operator.webp](./k8s-operator.webp)\\n<p align=\\"center\\">\\n*Image credit: CNCF blog*\\n</p>\\n\\n## Kubernetes Operators and the Rise of Rust\\n\\nBecause Kubernetes itself is written in Go, the majority of Kubernetes operators available today are also written in Go. The [kubebuilder](https://book.kubebuilder.io/) project simplifies the process of building Kubernetes operators in Go and is widely considered the de facto standard for doing so.\\n\\nWith the increasing popularity of Rust, it was only a matter of time before someone developed a framework for building Kubernetes operators in Rust. The [kube-rs](https://github.com/kube-rs/kube) project allows developers to build Rust-based Kubernetes operators in a similar manner to the `kubebuilder` project. This project excited us for a few reasons:\\n\\n1. We were interested in learning Rust.\\n2. We wanted to explore whether Rust could be a viable alternative to Go for writing Kubernetes operators.\\n3. We were inspired by the success of companies like [Stackable](https://github.com/stackabletech), who have developed numerous Kubernetes operators in Rust.\\n\\nThis excitement led us to the decision to write our Kubernetes operator in Rust.\\n\\n## Building the Tembo Operator\\n\\nTembo Cloud distinguishes itself from other managed Postgres offerings in several ways, one of which is the ability to install and enable Postgres extensions on the fly. This experience is in part powered by [Trunk](https://pgt.dev/), a Postgres extension registry and companion CLI that provide a simplified extension management experience.\\n\\nIt also introduces the concept of [Stacks](https://tembo.io/blog/tembo-stacks-intro), which are pre-built use-case-specific Postgres deployments which are optimized and tuned to serve a specific workload.\\n\\n### Roll Your Own\\n\\nIn order to build these unique capabilities, we knew we\u2019d need to harness the power and flexibility of a Kubernetes operator in our own way. Although there are several Kubernetes operators for Postgres available, none of them offer the same unique Postgres extension management experience or the concept of Stacks.\\n\\nInitially, we attempted to build our own operator from scratch. We had successfully built the extension management piece, but soon realized that we were duplicating existing efforts. We had a comprehensive list of baseline features to develop, which included:\\n\\n- Backup\\n- Recovery\\n- Connection Pooling\\n- Failover\\n- Upgrades\\n\\n### CNPG to the Rescue\\n\\nEnter [CloudNativePG](https://cloudnative-pg.io/) (CNPG). CNPG is a Kubernetes operator for Postgres created by the folks at EDB. We found it to be the most compelling of the many Kubernetes operators for Postgres out there. It provided many of the features we needed, including backup, recovery, connection pooling, failover, and upgrades. However, we still needed the ability to install and enable any Postgres extensions on the fly and define Stacks.\\n\\nThis is where the Tembo Operator comes in. We built the Tembo Operator in a way that utilizes CNPG, which enables us to offer a distinctive management experience for Postgres extensions and Stacks while utilizing a reliable and stable Postgres solution.\\n\\n## Using the Tembo Operator\\n\\nLet\u2019s take a look at what a custom resource spec looks like for the Tembo Operator. Here\u2019s an example for our Machine Learning Stack. We can see this sample spec makes use of our Machine Learning Stack and includes a handful of extensions. Keep in mind, these extensions are installed at runtime with Trunk and are not built into the container image.\\n\\n```yaml\\napiVersion: coredb.io/v1alpha1\\nkind: CoreDB\\nmetadata:\\n  name: sample-machine-learning\\nspec:\\n  image: \\"quay.io/tembo/ml-cnpg:15.3.0-1-a3e532d\\"\\n  stop: false\\n  stack:\\n    name: MachineLearning\\n    postgres_config:\\n      - name: pg_stat_statements.track\\n        value: all\\n      - name: cron.host\\n        value: /controller/run\\n      - name: track_io_timing\\n        value: \'on\'\\n      - name: shared_preload_libraries\\n        value: vectorize,pg_stat_statements,pgml,pg_cron,pg_later\\n  trunk_installs:\\n    - name: pgvector\\n      version: 0.5.0\\n    - name: pgml\\n      version: 2.7.1\\n    - name: pg_embedding\\n      version: 0.1.0\\n    - name: pg_cron\\n      version: 1.5.2\\n    - name: pgmq\\n      version: 0.14.2\\n    - name: vectorize\\n      version: 0.0.2\\n    - name: pg_later\\n      version: 0.0.8\\n  extensions:\\n    # trunk project pgvector\\n    - name: vector\\n      locations:\\n        - database: postgres\\n          enabled: true\\n          version: 0.5.0\\n    # trunk project postgresml\\n    - name: pgml\\n      locations:\\n        - database: postgres\\n          enabled: true\\n          version: 2.7.1\\n    # trunk project pg_embedding\\n    - name: embedding\\n      locations:\\n        - database: postgres\\n          enabled: false\\n          version: 0.1.0\\n    - name: pg_cron\\n      description: pg_cron\\n      locations:\\n        - database: postgres\\n          enabled: true\\n          version: 1.5.2\\n    - name: pgmq\\n      description: pgmq\\n      locations:\\n        - database: postgres\\n          enabled: true\\n          version: 0.14.2\\n    - name: vectorize\\n      description: simple vector search\\n      locations:\\n        - database: postgres\\n          enabled: true\\n          version: 0.0.2\\n    - name: pg_later\\n      description: async query execution\\n      locations:\\n        - database: postgres\\n          enabled: true\\n          version: 0.0.8\\n  runtime_config:\\n    - name: shared_buffers\\n      value: \\"1024MB\\"\\n    - name: max_connections\\n      value: \\"431\\"\\n    - name: work_mem\\n      value: \\"5MB\\"\\n    - name: bgwriter_delay\\n      value: \\"200ms\\"\\n    - name: effective_cache_size\\n      value: \\"2867MB\\"\\n    - name: maintenance_work_mem\\n      value: \\"204MB\\"\\n    - name: max_wal_size\\n      value: \\"10GB\\"\\n```\\n\\nTo create our Postgres instance, we run the following command:\\n\\n```bash\\n\u276f kubectl apply -f yaml/sample-machine-learning.yaml\\ncoredb.coredb.io/sample-machine-learning created\\n\u276f kubectl get po\\nNAME                                               READY   STATUS    RESTARTS   AGE\\nsample-machine-learning-1                          1/1     Running   0          19s\\nsample-machine-learning-metrics-5fbcf9b676-hkxtk   1/1     Running   0          31s\\n```\\n\\nOnce we\u2019ve connected to the Postgres instance, we can run `\\\\dx` to confirm the extensions were installed and enabled as expected:\\n\\n```bash\\n\u276f export PGPASSWORD=$(kubectl get secrets/sample-machine-learning-connection --template={{.data.password}} | base64 -d)\\n\u276f psql postgres://postgres:$PGPASSWORD@sample-machine-learning.localhost:5432\\npsql (16.0 (Ubuntu 16.0-1.pgdg22.04+1), server 15.3)\\nSSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, compression: off)\\nType \\"help\\" for help.\\n\\npostgres=# \\\\dx\\n                                            List of installed extensions\\n        Name        | Version |   Schema   |                              Description                               \\n--------------------+---------+------------+------------------------------------------------------------------------\\n pg_cron            | 1.5     | pg_catalog | Job scheduler for PostgreSQL\\n pg_later           | 0.0.8   | pglater    | pg_later:  Run queries now and get results later\\n pg_stat_statements | 1.10    | public     | track planning and execution statistics of all SQL statements executed\\n pgmq               | 0.14.2  | public     | A lightweight message queue. Like AWS SQS and RSMQ but on Postgres.\\n plpgsql            | 1.0     | pg_catalog | PL/pgSQL procedural language\\n vector             | 0.5.0   | public     | vector data type and ivfflat access method\\n vectorize          | 0.0.2   | vectorize  | The simplest way to do vector search on Postgres\\n```\\n\\nLet\u2019s install a new extension by adding the following to our sample spec:\\n\\n```yaml\\n...\\ntrunk_installs:\\n    - name: pg_bm25\\n      version: 0.4.0\\n...\\nextensions:\\n    - name: pg_bm25\\n      locations:\\n        - database: postgres\\n          enabled: true\\n          version: 0.4.0\\n```\\n\\nAfter applying the updated spec and connecting to Postgres, we can see the new extension [pg_bm25](https://pgt.dev/extensions/pg_bm25) is installed and enabled as expected:\\n\\n```bash\\n\u276f psql postgres://postgres:$PGPASSWORD@sample-machine-learning.localhost:5432\\npsql (16.0 (Ubuntu 16.0-1.pgdg22.04+1), server 15.3)\\nSSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, compression: off)\\nType \\"help\\" for help.\\n\\npostgres=# \\\\dx\\n                                            List of installed extensions\\n        Name        | Version |   Schema   |                              Description                               \\n--------------------+---------+------------+------------------------------------------------------------------------\\n pg_bm25            | 0.0.0   | paradedb   | pg_bm25: PostgreSQL-native, full text search using BM25\\n pg_cron            | 1.5     | pg_catalog | Job scheduler for PostgreSQL\\n pg_later           | 0.0.8   | pglater    | pg_later:  Run queries now and get results later\\n pg_stat_statements | 1.10    | public     | track planning and execution statistics of all SQL statements executed\\n pgmq               | 0.14.2  | public     | A lightweight message queue. Like AWS SQS and RSMQ but on Postgres.\\n plpgsql            | 1.0     | pg_catalog | PL/pgSQL procedural language\\n vector             | 0.5.0   | public     | vector data type and ivfflat access method\\n vectorize          | 0.0.2   | vectorize  | The simplest way to do vector search on Postgres\\n```\\n\\n## Up Next\\nWe\u2019re currently working on exciting new features that enable the deployment of custom applications alongside Postgres. These features include a REST API, GraphQL, and more. Stay tuned for future updates!\\n\\nFor more information on running the Tembo Operator, check out our docs at:\\n- https://tembo.io/docs/tembo-stacks/local-tembo-operator\\n\\nIf you\'re interested in contributing to the project, check out our Github repo at:\\n- https://github.com/tembo-io/tembo-stacks/tree/main/tembo-operator\\n\\nAnd if you want to try out the full power of Postgres and fully delegate extension management to us, [try out Tembo Cloud](https://cloud.tembo.io)."},{"id":"anon-dump","metadata":{"permalink":"/blog/anon-dump","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-10-24-anonymous-dump/index.md","source":"@site/blog/2023-10-24-anonymous-dump/index.md","title":"Anonymized dump of your Postgres data","description":"Someone on X wanted to know how to get an anonymous dump of Postgres data, but doesn\'t want to install an extension in their production DB. I want to show how you can start a local database, dump the production data there, then do an anonymized dump from that without too much hassle.","date":"2023-10-24T00:00:00.000Z","formattedDate":"October 24, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"extensions","permalink":"/blog/tags/extensions"},{"label":"postgresql_anonymizer","permalink":"/blog/tags/postgresql-anonymizer"}],"readingTime":2.265,"hasTruncateMarker":false,"authors":[{"name":"Steven Miller","title":"Founding Engineer","url":"https://github.com/sjmiller609","email":"noreply@tembo.io","imageURL":"https://github.com/sjmiller609.png","key":"steven"}],"frontMatter":{"slug":"anon-dump","title":"Anonymized dump of your Postgres data","authors":["steven"],"tags":["postgres","extensions","postgresql_anonymizer"]},"prevItem":{"title":"Tembo Operator: A Rust-based Kubernetes Operator for Postgres","permalink":"/blog/tembo-operator"},"nextItem":{"title":"Hacking Postgres, Ep. 5: Alexander Korotkov","permalink":"/blog/hacking-postgres-ep5"}},"content":"Someone on X wanted to know how to get an anonymous dump of Postgres data, but doesn\'t want to install an extension in their production DB. I want to show how you can start a local database, dump the production data there, then do an anonymized dump from that without too much hassle.\\n\\n## Getting set up\\n\\nDockerfile:\\n\\n```shell\\nFROM quay.io/tembo/tembo-local:latest\\n\\n# Install extensions from Trunk\\nRUN trunk install pgcrypto\\nRUN trunk install postgresql_anonymizer\\n\\n# Setting samples to use for anonymization\\nRUN cd /var/lib/postgresql/data/tembo/extension/anon && \\\\\\n  wget https://gitlab.com/dalibo/postgresql_anonymizer/-/raw/master/data/lorem_ipsum.csv && \\\\\\n  wget https://gitlab.com/dalibo/postgresql_anonymizer/-/raw/master/data/identifiers_category.csv && \\\\\\n  wget https://gitlab.com/dalibo/postgresql_anonymizer/-/raw/master/data/identifier_fr_FR.csv && \\\\\\n  wget https://gitlab.com/dalibo/postgresql_anonymizer/-/raw/master/data/identifier_en_US.csv && \\\\\\n  wget https://gitlab.com/dalibo/postgresql_anonymizer/-/raw/master/data/address.csv && \\\\\\n  wget https://gitlab.com/dalibo/postgresql_anonymizer/-/raw/master/data/city.csv && \\\\\\n  wget https://gitlab.com/dalibo/postgresql_anonymizer/-/raw/master/data/company.csv && \\\\\\n  wget https://gitlab.com/dalibo/postgresql_anonymizer/-/raw/master/data/country.csv && \\\\\\n  wget https://gitlab.com/dalibo/postgresql_anonymizer/-/raw/master/data/email.csv && \\\\\\n  wget https://gitlab.com/dalibo/postgresql_anonymizer/-/raw/master/data/first_name.csv && \\\\\\n  wget https://gitlab.com/dalibo/postgresql_anonymizer/-/raw/master/data/iban.csv && \\\\\\n  wget https://gitlab.com/dalibo/postgresql_anonymizer/-/raw/master/data/last_name.csv && \\\\\\n  wget https://gitlab.com/dalibo/postgresql_anonymizer/-/raw/master/data/postcode.csv && \\\\\\n  wget https://gitlab.com/dalibo/postgresql_anonymizer/-/raw/master/data/siret.csv && \\\\\\n  wget https://gitlab.com/dalibo/postgresql_anonymizer/-/raw/master/data/lorem_ipsum.csv\\n```\\n\\nBuild and run it like this:\\n\\n```bash\\ndocker build -t example-local-image .\\ndocker rm --force local-tembo || true\\ndocker run -it --name local-tembo -p 5432:5432 --rm example-local-image\\n```\\n\\n[This guide](https://tembo.io/docs/tembo-cloud/try-extensions-locally) covers how to quickly try out Postgres extensions locally in more detail.\\n\\n## Dump the data into your local DB\\n\\n```bash\\npg_dump \'your-connection-string-here\' | psql \'postgres://postgres:postgres@localhost:5432\'\\n```\\n\\n## Anonymize the local DB\\n\\nInitialize the extension:\\n\\n```sql\\nSET session_preload_libraries = \'anon\';\\nLOAD \'anon\';\\nCREATE EXTENSION IF NOT EXISTS anon CASCADE;\\nSELECT anon.init();\\n```\\n\\nFor example, I have a table called \\"extension_owners\\", and I would like to anonymize the user_name column:\\n```\\npostgres=# select extension_id,user_name from extension_owners limit 1;\\n extension_id |      user_name\\n--------------+---------------------\\n           26 | coredb-service-user\\n(1 row)\\n```\\n\\nI configured anonymization on that column like this:\\n```sql\\nSECURITY LABEL FOR anon ON COLUMN extension_owners.user_name\\n  IS \'MASKED WITH FUNCTION anon.lorem_ipsum( words := 1 )\';\\n```\\n\\nThere are a lot of other options for anonymizing data, and you can even write your own functions. More information in [these docs](https://gitlab.com/dalibo/postgresql_anonymizer/-/blob/master/docs/masking_functions.md?ref_type=heads).\\n\\n:::caution\\nThis next step replaces data in the local database.\\n:::\\n\\nSince we are working on a local copy of the data, we can just use this function to replace anonymized columns in-place.\\n```\\nSELECT anon.anonymize_database();\\n```\\n\\nWe can see now this column has been anonymized.\\n```\\npostgres=# select user_name from extension_owners limit 10;\\n user_name\\n------------\\n First\\n They\\n Cell\\n Price\\n Them\\n Go\\n Parent\\n Republican\\n With\\n Between\\n(10 rows)\\n```\\n\\nYou can do further modification from here, for example masking and replacing additional columns, formatting columns, etc.\\n\\n## Done!\\n\\nNow you have an anonymized database locally. From here, you can pg_dump to a file, or do something else!\\n\\nIf you think this kind of thing is cool, follow me on X ([@sjmiller609](https://x.com/sjmiller609)) for more content. At Tembo, we are all about Postgres extensions. You can try out extensions on [Tembo Cloud](https://cloud.tembo.io) for free."},{"id":"hacking-postgres-ep5","metadata":{"permalink":"/blog/hacking-postgres-ep5","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-10-23-hacking-postgres-ep5/index.md","source":"@site/blog/2023-10-23-hacking-postgres-ep5/index.md","title":"Hacking Postgres, Ep. 5: Alexander Korotkov","description":"In this episode, Ry and Alexander talk about OrioleDB (and the challenge of fighting bloat), fuzzy and vector search, and the challenges behind database management. Watch below, or listen on Apple/Spotify (or your podcast platform of choice). Special thanks to Alexander for joining us today!","date":"2023-10-23T00:00:00.000Z","formattedDate":"October 23, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"hacking_postgres","permalink":"/blog/tags/hacking-postgres"}],"readingTime":23.415,"hasTruncateMarker":false,"authors":[{"name":"Eric Ankenman","url":"https://github.com/ericankenman","email":"noreply@tembo.io","imageURL":"https://github.com/ericankenman.png","key":"eric"}],"frontMatter":{"slug":"hacking-postgres-ep5","title":"Hacking Postgres, Ep. 5: Alexander Korotkov","authors":["eric"],"tags":["postgres","hacking_postgres"]},"prevItem":{"title":"Anonymized dump of your Postgres data","permalink":"/blog/anon-dump"},"nextItem":{"title":"Hacking Postgres, Ep. 4: Pavlo Golub","permalink":"/blog/hacking-postgres-ep4"}},"content":"In this episode, Ry and Alexander talk about OrioleDB (and the challenge of fighting bloat), fuzzy and vector search, and the challenges behind database management. Watch below, or listen on Apple/Spotify (or your podcast platform of choice). Special thanks to Alexander for joining us today!\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/FrOvwkmAPvg?si=9OgASSFxHfkYSfb2\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n\\nWant to know more about something they mentioned? Here\u2019s a starting point:\\n\\n* OrioleDB - [https://www.orioledata.com/](https://www.orioledata.com/)\\n* fuzzystrmatch - https://www.postgresql.org/docs/current/fuzzystrmatch.html\\n* pg_trgm - https://www.postgresql.org/docs/current/pgtrgm.html\\n* MVCC - [https://www.postgresql.org/docs/7.1/mvcc.html](https://www.postgresql.org/docs/7.1/mvcc.html)\\n* [\u201cTen Things I Hate About Postgres\u201d](https://rbranson.medium.com/10-things-i-hate-about-postgresql-20dbab8c2791) - article by Rick Branson\\n* \u201c[Why Uber Engineering Switched from Postgres to MySQL](https://www.uber.com/blog/postgres-to-mysql-migration/)\u201d - blog from Uber Engineering team\\n\\nDid you enjoy the episode? Have ideas for someone else we should invite? Let us know your thoughts on X at @tembo_io or share them with the team in our [Slack Community](https://join.slack.com/t/tembocommunity/shared_invite/zt-23o25qt91-AnZoC1jhLMLubwia4GeNGw).\\n\\n\\n## Transcript\\n\\n\\n##### _[00:00:12] - Alexander_\\n\\nHi, I\'m Ry Walker, founder of Tembo, a managed Postgres company. And today I have Alexander Korotkov from OrioleDB as my guest. Alexander, welcome to the show.\\n\\n\\n##### _[00:00:24] - Alexander_\\n\\nHello, thank you very much for inviting me. It\'s a pleasure to participate. \\n\\n\\n##### _[00:00:32] - Ry_\\n\\nAwesome. Maybe I\'d like to start by having you give us a quick background, like where did you grow up and what were you doing before you started working on Postgres?\\n\\n\\n##### _[00:00:41] - Alexander_\\n\\nEven before Postgres?\\n\\n\\n##### _[00:00:43] - Ry_\\n\\nYeah, way back. Were you only like ten years old then?\\n\\n\\n##### _[00:00:49] - Alexander_\\n\\nYes, I started actually from web development. Yes. And for web development, Postgres become my favorite database management system. That time there were basically two popular open source database management system, MySQL and Postgres. And Postgres behavior looked way more consistent for me. And this is why Postgres become my favorite. And also thing which attracts my attention was generalized indexing in Postgres that Postgres have even that time had GiST and gene indexes which you could apply to different data types, different search operators. And that was very interesting for me. And I also have studied for PhD in university and I was interested in fuzzy search and features like this. And in Postgres I found fuzzystrmatch complete model. And that model contained Levenshtein function which defines editing distance between two strings, number of editing operations. And I found that it doesn\'t work with multibyte encoding, with multibyte encoding. So it just compares two strings, byte per byte. And my first patch was to just fix this bug. I actually didn\'t know if I could produce the I just downloaded sources. Thankfully that time compiling a Postgres already wasn\'t a problem. So I just had Linux on my desktop and just cloned it.\\n\\n\\n##### _[00:03:08] - Alexander_\\n\\nProbably that time it was CVS repository, I cloned it and it wasn\'t difficult to locate the place in the source code responsible for this function. And I just have to find which functions are responsible for getting lengths of multi byte string in Postgres and stuff. Work was very easy, and I have submitted my first patch to Postgres, but then Robert Haas picked my patch to work on it to review and commit. And that time I get that it\'s not so easy this process community, because we have quite long thread studying possible performance regressions and on, and I rewrote patch this small patch many times. But finally we find a way when this patch not only fixes the problem of multibyte encoding, but also doesn\'t produce noticeable overhead when it\'s single byte encoding or strings, just uses single byte characters. In both of these cases, the overhead was negligible. And then Robert did commit my patch.\\n\\n\\n##### _[00:04:40] - Ry_\\n\\nThat\'s amazing. Yeah, it\'s interesting. How many lines of code was the patch, I wonder?\\n\\n\\n##### _[00:04:46] - Alexander_\\n\\nI don\'t remember exactly. You could find it on the list, but this patch should be in dozens of lights, probably 20 or something like this, really small. But also with these fix, I started to work on improving this function, improving the performance because if you use the Levenshtein function you are typically looking for strings which are close to your string. Right. And the thing is you basically don\'t care about large distances. For instance, you are looking for strings with editing distance three or less. And that means that if it would be four you don\'t care how much bigger is that distance. And if you need only this, then you could easily, not easily, but you could significantly accelerate function Levenshtein calculation in many times and this functionality took even more work for me and Robert. But you could use it, it\'s Levenshtein less equal. Nice function.\\n\\n\\n##### _[00:06:09] - Ry_\\n\\nThat\'s great. Yeah. I also came to Postgres from being a web developer prior and I\'ve realized in recent weeks actually, that the reason why I went to Postgres and not MySQL is primarily I was using Ruby on Rails, which just kind of was Postgres first and then I really didn\'t like PHP. Hopefully you weren\'t a PHP developer, but the lamp stack had MySQL and PHP together and I always just like I don\'t want to go anywhere near anything near PHP. That\'s really not a great reason, but it\'s just a fact.\\n\\n\\n##### _[00:06:48] - Alexander_\\n\\nYes, probably one of the great feature I have discovered in podcast in early days was DDL transactions. So that if you need to do a database schema migration, you can wrap all the changes into transaction and if something go wrong you can roll. Yes, that\'s just amazing. And it\'s interesting that even old and mature database management systems like Oracle or MSS SQL Server lacks of this functionality. I\'m not sure about the present day, but at time indefinitely they all were lacking and this was very useful.\\n\\n\\n##### _[00:07:36] - Ry_\\n\\nYeah, that\'s great. So you built obviously you started with that first patch and worked on other fuzzy search stuff. Have you worked on any Postgres extensions?\\n\\n\\n##### _[00:07:52] - Alexander_\\n\\nYes, I continued to work on Postgres. Then I have found that I get known with Oleg and Theodore who was in Russian contributors of Postgres and I get familiar with their work. Some of their work I already know Gist and Jin. But for others work, it was interesting for me that we could accelerate also search for like patterns. Not just perfect patterns, but imagine you looking something that could be in the middle of the string. And there were that time Pgtrgm module. But at that time it only supports trigram similarity. Search using indexes. But I found that it\'s pretty easy if you decompose string with trigrams it\'s pretty easy to implement like search. So you could just extract trigrams from the like patterns and search for them. And thanks that trigrams are extracted all over the string so you can find this substring anywhere. And that time I would say my feeling was just great. So it\'s just amazing. So with quite small patch you could teach database server with some amazing advanced functionality and I get even more encouragement when I would be sorry if I confusing with names hugh Bert I don\'t know the guy from blogger from Poland who was posting to the planet PostgresQL waiting for and he also posted waiting for this feature like Word for Pgtrm.\\n\\n\\n##### _[00:10:15] - Alexander_\\n\\nThis was one of my first patches. Another thing that during my PhD research I also researched split algorithms for R3 and I have found more advanced algorithm and I have pushed this algorithm to Postgres core and to PostGIS that took time. Yes, because communities are quite conservatives, but it was also good.\\n\\n\\n##### _[00:11:00] - Ry_\\n\\nYeah, great. Well, I want to talk about OrioleDB, but before we do that, I was thinking are you paying attention to the whole vector search since you spent so much time on Postgres search features or I don\'t know, at least sometime. I\'m curious, are you kind of tracking what\'s going on with vector search?\\n\\n\\n##### _[00:11:20] - Alexander_\\n\\nYes. That\'s interesting. That\'s an interesting subject. Yes. While I have researched the split algorithms, I have also experimented with cube concrete model which supported basically multidimensional rectangles of different number of dimensions up to one dimensions. And what I have found is that if you have low dimensionality two, three, four or five, then when you are producing split, your split will be good for one dimension, but almost don\'t differentiate other dimension. So you can imagine this if you have two dimensional space filled with points and you need to split them into two rectangles, there is just two good way to do this split vertically or horizontally. All other ways your rectangles would have a huge overlap. But thing changes when your dimensionality increases. There is a so called Woodman quadratic split algorithm and this algorithm actually do clustering. So it just tries to find to divide the set of points into two clusters. And it doesn\'t work well for low dimensionality, but for high dimensionality of space it becomes better than if you just pick one axis and split in this dimension. And that was interesting for me and I got familiar with cures of dimensions so that if you have low dimensionality space, you can have some guaranteed and quite good search performance.\\n\\n\\n##### _[00:13:36] - Alexander_\\n\\nRight? So imagine the best example is unidimensional space. You can index this just b three and you have some guaranteed or logarithm n time for search for point request. Right? But when you have high dimensionality, that becomes a problem and uniform random data, which could be good, which you could handle very well with low dimensionality, in high dimensionality it becomes almost unindexable. So if you have 1000 of dimension vectors and you need to search for a similarity, then search in uniform data would be almost impossible to accelerate because you can\'t identify which particular dimension could give you a match. You could just eat just cumulative results of cumulative results of all dimensions and it\'s almost impossible to accelerate. I have participated a bit in improvement of PG vector and with some developers of Supabase, we really found that indexing methods, if they applied to uniform data, then they give nothing. So indexing methods, when you have a lot of dimensions, they based on some clustering. The idea is to find, how to say, find the low in the distribution, ununiformity of distribution, and then exploit it. And this is how indexing works. There are different interesting methods for indexing multidimensional space, but I think the favorite is Hnsv method.\\n\\n\\n##### _[00:16:05] - Alexander_\\n\\nAnd I have read about this method in the scientific paper way before this AI Hype ten years ago. And that was interesting how far these methods from all others. So it\'s very self.\\n\\n\\n##### _[00:16:26] - Ry_\\n\\nIt seems like just vector search is just on the exact edge of search. And the LLM AI, like you said, it\'s kind of blending both sides. Well, that\'s cool. So I wanted to obviously chat about OrioleDB, I imagine. Are you spending trying to spend 100% of your time on OrioleDB, or? It\'s probably hard to I mean, it sounds like there\'s lots of great things happening in Postgres to distract you from your own work.\\n\\n\\n##### _[00:16:57] - Alexander_\\n\\nThe thing vector search definitely distract me. And with this AI Hype, it\'s hard to not be yeah, yeah, I know.\\n\\n\\n##### _[00:17:06] - Ry_\\n\\nFor say, I would just love to kind of have the top three or four things you\'re trying to accomplish with Oriole maybe and maybe give us a progress report.\\n\\n\\n##### _[00:17:20] - Alexander_\\n\\nYes. But before this, I\'d like to mention about how I use AI in my work.\\n\\n\\n##### _[00:17:26] - Ry_\\n\\nOh, yeah, sure.\\n\\n\\n##### _[00:17:27] - Alexander_\\n\\nSo I have written the blog post, no more vacuum, no more bloat. You may be aware it was on top of news and interesting that I have mostly generated this with Chat GPT. So I just wrote the short item list and asked Chat GPT to write me a blog post. Then I have corrected a little bit, then I wrote some graph, then add some graph, asked Chet GPT to add another paragraph with them, and then it was done. And I was curious that I expected that I would be revealed and blamed for this. But actually the comments I get was, oh, this blog post is so much well written.\\n\\n\\n##### _[00:18:23] - Ry_\\n\\nI know, it\'s funny. I agree with you. I\'ll take just like, notes. Like, for example, here I\'ve got like a list of 20 questions that I\'m trying to ask you during this interview, possibly ask you. I guarantee you if I sent this list of 20 questions to ChatGPT and say, hey, make these questions better, it would crush it, give me all kinds of much better questions. But anyway, yeah, I agree. People who aren\'t using it are really missing out on a great assistant. All right, so back to Oriole. What are the top three or four.\\n\\n\\n##### _[00:19:02] - Alexander_\\n\\nThings I\'d like to first say about the thing which bring me to the Oriole? When I have studied Postgres, it was just like a magic how MVCC works. So you can run multiple sessions in parallel and each session will have its own snapshot of the data and that was just amazing. And I was very interested what is under the hood, but how this works from user size was perfect, but I always wondered how it implemented internally. Because when you in progress, when you\'re doing an update, then you just have to mark old tuple and insert the new tuple in the heap. And if hot update is not applied, then you also have to insert all the index tuples even if index set values are not updated. And I wondered if we could do this better. And I think I have this background thoughts for years. So I have studied how MVCC implemented in MySQL as and then how it\'s implemented in Oracle. And it was very interesting for me to get how it\'s implemented in Oracle because I heard that Oracle have block level undo. Yes, and then I have thought how could it be on a block level?\\n\\n\\n##### _[00:20:54] - Alexander_\\n\\nBecause if two transaction modifies the same tree page and this page got splitted and then one of transaction could be rolled back, then it\'s not linear list of log records which you could just apply one after another. Because if you need to roll back some change which was before the page split, you need to do some adjustments with it. And then I understood that I need to go deeper and get how it works and then I learn more things, how rider had log working and so on. And I think in 2017 I have started to work on design of my own storage which could work around the most of shortcomings, which I see in Postgres engine. For sure there is no plain wins, in some situation this engine works worse. But I would just like to design some new trade off which could be better in the typical situation. And I can highlight some things in OrioleDB. So the first thing which it is fighting is bloat and in order to eliminate bloat, it has undo log. So if you update a row, then you just have to add a new version of row into the undo chain and you only need to update indexes if their values are updated.\\n\\n\\n##### _[00:23:03] - Alexander_\\n\\nAnd also indexes are versioned trees as well. In OrioleDB I have heard that it is so in Oracle and I found that this is very attractive idea. Thanks to that index only scan becomes very simple because your secondary index already contains all the information and you doesn\'t need to look into any secondary structures. Because I have heard that in Postgres index only scans, index only scans is amazing until you run in some problem because if you have some intensive workloads and so on, you might have significant part of visibility map zero it and your query could get into trouble. Yes, that also could happen. And this is why I found that if secondary index version that\'s attractive idea and OrioleDB have a mixed underlock containing both row level and block level records and block level records allows to handle some changes like eliminating of dead tuples of the page. So for instance, your page contains some tuples which are deleted but still visible for some transaction. And using block level undo lock you can issue a new version of this page and get rid of all dead rows and reclaim their space. But the transactions which need old version can traverse page level underlock and find the tops that they need and another to eliminate bloat is automatic page merging.\\n\\n\\n##### _[00:25:12] - Alexander_\\n\\nAnyway, if even you have undo lock it could happen that your workload is so that you have a lot of sparse pages so you have a lot of data inserted but then the data was deleted and AudioDB supports automatic merge of sparse page to eliminate bloat. Okay, and you had some questions.\\n\\n\\n##### _[00:25:34] - Ry_\\n\\nYeah, I was just remembering that it was a blog post or a presentation where you like here are the ten worst things about Postgres. I don\'t know the exact phrasing you chose but yeah, basically here are ten problems with Postgres.\\n\\n\\n##### _[00:25:52] - Alexander_\\n\\nThat was a blog post of Rick Branson, I hope I spelled the name correctly and yes, I found that was a quite popular blog post. The first popular blog post is definitely Uber blog post about why they moved from Postgres to MySQL. But this blog post from Rick was very interesting for me because the issues he highlighted was very good fit to my vision and to things which I\'m going to improve with OrioleDB.\\n\\n\\n##### _[00:26:36] - Ry_\\n\\nYou\'ll probably never be done with OrioleDB because products are never finished, but how\'s your progress been through your target initial list of things you wanted to accomplish with Oriole are you making good progress through that or give us a progress report.\\n\\n\\n##### _[00:26:53] - Alexander_\\n\\nYes, I\'m making a good progress but currently the main target is not to add new features or handle more things, the current target is stability to get more users. So it doesn\'t work to have amazing products if nobody uses. And I would say that Database World is quite conservative because obviously people don\'t want to lose their data. And this is why, before using some new database technology, you need to ensure that it\'s really stable and mature. And this is the main challenge for new database management system or new storage engine and especially when it\'s OLTP and OrioleDB currently mainly targets OLTP because OLTP is typically the source of your data, right? So where your business operation happened, for instance, OLAP could be not so important. So you pull your data from OLTP system and put to OLAP for analysis and if it will disappear in a OLAP system you can just repeat the procedure but that doesn\'t work for OLTP which is initial source of the data. So this is the main difficulty I think, but we already did very good work with the team, very good work on eliminating the bugs but we definitely need more better testers.\\n\\n\\n##### _[00:28:46] - Ry_\\n\\nYeah, I mean, it\'s a big challenge. It\'s similar for us as well, but you have to create enough differentiation to get people to want to try it. But you can\'t go so far away from like you\'re saying the further the distance is from what they consider stable, the riskier it is. But if you\'re too close to the same thing, there\'s not enough value to do the switch.\\n\\n\\n##### _[00:29:12] - Alexander_\\n\\nRight.\\n\\n\\n##### _[00:29:12] - Ry_\\n\\nSo it\'s a tricky catch 22 that you have to do something a little bit dangerous to get the attention. If it\'s too close, then people will be like, if it\'s 10% faster, who cares? Right?\\n\\n\\n##### _[00:29:28] - Alexander_\\n\\nYes, exactly. This is why I am highlighting the cases where OrioleDB is in times faster or even dozens of times.\\n\\n\\n##### _[00:29:42] - Ry_\\n\\nHave you now released kind of the stable candidate? You said you\'d need more testers. Is that\'s kind of your stage now?\\n\\n\\n##### _[00:29:51] - Alexander_\\n\\nYes. We are currently discussing with our team and advisors when we can make market release candidate on the release. At some point, this is just how to say at some point, this is decisions of the will. So there is no strict procedure you can go through and say, okay, your product now should be better, or your product now should be generally available. You just need to decide for yourself and make this decision weighing all the risks.\\n\\n\\n##### _[00:30:30] - Ry_\\n\\nYeah, it was funny. I\'ll tell you a quick story about that. With my previous company, Astronomer, we were 0.1, 0.2, 0.3. I think we got to like 0.17. And it was like there was nothing big happening in any release that would cause us to say, oh, this should be the 1.0. And then we are also thinking like, this is one of me thinking this, because I was like, let\'s just make the next one 1.0, goddamn it, but let\'s save the 1.0 for a big marketing push. I don\'t know that they ever did it. We waited so long that we never had a 1.0 of that platform. By the time we got to it was like, oh, let\'s rewrite it. I think it\'s a tricky thing to ship, but I\'m a big fan of shipping early and often. And just mark your thing as an RC candidate. It doesn\'t matter. It will attract more attention with that RC dot. That zero dot.\\n\\n\\n##### _[00:31:35] - Alexander_\\n\\nYes. Actually, the number in the version is marketing you can find in the shop. The price for the good could be $20. But you go away, you come back, and now it\'s $30 crossed $25.01 of advices. I heard about version numbering was never number. If you have new products, never number your version 1.0.0, nobody will trust it\'s stable. So you should number something like 1.3 point eleven.\\n\\n\\n##### _[00:32:21] - Ry_\\n\\nYeah, well, I\'m kind of a big fan of CalVer, where the version of your software should be like, how many releases? It\'s 23 dot whatever.\\n\\n\\n##### _[00:32:35] - Alexander_\\n\\n2023.\\n\\n\\n##### _[00:32:35] - Ry_\\n\\nAnd here\'s our 6th release in 2023. And that kind of gets rid of all of the traditional semver stuff. But again, that\'s kind of hard to do as well. Well, I had a bunch of extra little questions I want to ask you just to get to know you a little better. But if you had a magic wand that you could add any feature to Postgres and tomorrow morning we wake up and it\'s there, what would it be? What\'s the most important thing? Or would you be most excited to see?\\n\\n\\n##### _[00:33:06] - Alexander_\\n\\nMy question...it could sound probably selfish, but I have a patch set to improve Postgres table access method API. This patch set definitely needs some work and if I magic went I would like this work to be just done and everything is perfect and in the Postgres core perfected. And that would make OrioleDB become a pure extension. But not only that, I think it could clear the path for more table access method implementations.\\n\\n\\n##### _[00:33:48] - Ry_\\n\\nGot it. That\'s good, that\'s a great one.\\n\\n\\n##### _[00:33:50] - Alexander_\\n\\nAnd one thing if you have a few minutes, the Postgres Extensibility is probably the idea which comes from Postgres early design and remains Postgres differentiation feature till now. Because all our ideas from Postgres design today it sounds weird and it\'s not.\\n\\n\\n##### _[00:34:19] - Ry_\\n\\nThere, but Extensibility is it\'s it\'s honestly, I always just thought MySQL and Postgres were just two different. Yeah, I didn\'t realize how much more Extensible Postgres at least tries to be and has been and how many extensions exist for it. Once you start looking at that list, it\'s pretty amazing, all the work that people have done to extend it. Is there anything, would you say about Postgres that almost nobody agrees with you about? Do you have an opinion that\'s controversial or just it\'s hard for people to agree with you about?\\n\\n\\n##### _[00:35:03] - Alexander_\\n\\nProbably I don\'t know this opinion. So there are opinions where we have disagreement in community, but I can\'t remember the opinion which leaves me alone with that. So probably not. Yeah, there are opinions where I appear in a minority, but not more.\\n\\n\\n##### _[00:35:28] - Ry_\\n\\nYeah, a lot of times that\'s just because you don\'t understand everything yet, right? People come in with a hot take and then the mailing list will educate them on other factors.\\n\\n\\n##### _[00:35:40] - Alexander_\\n\\nYes, there are disagreements because we are writing patches to Postgres and we understand these patches as improvements. But actually there is never a pure win. If you achieve something, you always lose something. That might be not obvious, but I don\'t know. So if you are adding too many SQL comments and your parser get complicated and parcel state machine cannot fit to processor cache and get slower and your source code also becomes bigger and harder to understand and so on. And finally we are just a group of people which needs to make some decision of will, of where to go. No way is a clear win. We are trying our best to don\'t discourage existing users, but even the best offers of them, even with best efforts of them. There are some disprovements for sure. At the end of the day, we need to negotiate and make a decision.\\n\\n\\n##### _[00:36:54] - Ry_\\n\\nWhere can people find you online? And in particular, what\'s the best way for them to get involved with testing OrioleDB?\\n\\n\\n##### _[00:37:04] - Alexander_\\n\\nYes, any person can reach me via email and testing Oriole, you just go to GitHub, download and compile sources or even easier, go to Docker Hub and get the docker image and just go experiment with your data and your workload and raise an issue or discussion and share your experience and.\\n\\n\\n##### _[00:37:32] - Ry_\\n\\nRefresh us one last time. Where are the use cases where Oriole is going to outperform vanilla Postgres most dramatically I guess yes.\\n\\n\\n##### _[00:37:43] - Alexander_\\n\\nSo the situations are when you have a lot of updates, very update intensive workload, then OrioleDB could outperform things to undo log. Another case is huge multi core machines and OrioleDB eliminates a lot of bottlenecks and can perform in times faster and also OrioleDB implements role level write ahead log and if you have a huge write ahead log stream and you need geographical replication, OrioleDB can also help a lot. Awesome.\\n\\n\\n##### _[00:38:31] - Ry_\\n\\nOkay, great. Thank you. Yeah. If there\'s anything else you want to share, feel free, but otherwise it was great chatting with you.\\n\\n\\n##### _[00:38:40] - Alexander_\\n\\nNo more from me. Thank you very much for inviting. It\'s always very lovely and friendly talk with."},{"id":"hacking-postgres-ep4","metadata":{"permalink":"/blog/hacking-postgres-ep4","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-10-21-hacking-postgres-ep4/index.md","source":"@site/blog/2023-10-21-hacking-postgres-ep4/index.md","title":"Hacking Postgres, Ep. 4: Pavlo Golub","description":"In this episode, Ry and Pavlo talk about pg_timetable, about the value and place of risk in the ecosystem, and about building for the Postgres core. If you haven\u2019t seen or listened to it yet, you can watch below, or listen on Apple/Spotify (or your podcast platform of choice). Special thanks to Pavlo for joining us today!","date":"2023-10-21T00:00:00.000Z","formattedDate":"October 21, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"hacking_postgres","permalink":"/blog/tags/hacking-postgres"}],"readingTime":16.81,"hasTruncateMarker":false,"authors":[{"name":"Eric Ankenman","url":"https://github.com/ericankenman","email":"noreply@tembo.io","imageURL":"https://github.com/ericankenman.png","key":"eric"}],"frontMatter":{"slug":"hacking-postgres-ep4","title":"Hacking Postgres, Ep. 4: Pavlo Golub","authors":["eric"],"tags":["postgres","hacking_postgres"]},"prevItem":{"title":"Hacking Postgres, Ep. 5: Alexander Korotkov","permalink":"/blog/hacking-postgres-ep5"},"nextItem":{"title":"Hacking Postgres, Ep. 3: Eric Ridge","permalink":"/blog/hacking-postgres-ep3"}},"content":"In this episode, Ry and Pavlo talk about pg_timetable, about the value and place of risk in the ecosystem, and about building for the Postgres core. If you haven\u2019t seen or listened to it yet, you can watch below, or listen on Apple/Spotify (or your podcast platform of choice). Special thanks to Pavlo for joining us today!\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/uDfUxtZK8_Q?si=wrmh4_2l5-LpNOlY\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n\\nWant to know more about something they mentioned? Here\u2019s a starting point:\\n\\n\\n* Cybertec - https://www.cybertec-postgresql.com/en/\\n* pg_timetable - https://github.com/cybertec-postgresql/pg_timetable\\n* pgwatch - https://github.com/cybertec-postgresql/pgwatch2\\n* pg_cron - https://github.com/citusdata/pg_cron\\n* baseql - https://www.baseql.com/\\n* pg_dump - https://www.postgresql.org/docs/current/app-pgdump.html\\n* vscode - https://github.com/microsoft/vscode\\n* GoLand - https://www.jetbrains.com/go/\\n\\nDid you enjoy the episode? Have ideas for someone else we should invite? Let us know your thoughts on X at [@tembo_io](https://twitter.com/tembo_io) or share them with the team in our [Slack Community](https://join.slack.com/t/tembocommunity/shared_invite/zt-23o25qt91-AnZoC1jhLMLubwia4GeNGw).\\n\\n\\n## Transcript\\n\\n\\n##### _[00:00:12] - Ry_\\n\\nHello, welcome to Hacking Postgres. I\'m Ry Walker, founder of Tembo, a managed Postgres company. And today I have Pavlo Golub. Did I say that right? Pavlo? Yeah, you can fix it from Cybertec as my guest. So Pavlo, welcome to the show.\\n\\n\\n##### _[00:00:30] - Pavlo_\\n\\nYes, thank you for having me. Hi there.\\n\\n\\n##### _[00:00:32] - Ry_\\n\\nHi there. Yeah, so maybe you could start by giving us a quick background, like where you grew up and what were you doing before you started working on Postgres.\\n\\n\\n##### _[00:00:41] - Pavlo_\\n\\nOkay, so I was born in Soviet Union so I\'m a little bit old, but yeah, I lived in Ukraine. And the first time I saw Postgres, when I was in my high school, it was like fourth year of high school and at that time it wasn\'t very popular. Everybody we\'re talking about MySQL like the only database with open source and yeah, at that time I saw the PostgresQL, I tried to install it and run it. It was hard for me to do that because at that time the version was like 7.4 something and at that time there was no installer for Windows and we were mostly like Windows guys at that times. So, yeah, I need to install Linux and then install Postgres. Yeah, I managed to do that, but I wasn\'t happy about it. The whole process is like I feel like cumbersome or something like that. So I left it for like a couple of years and then my first job in IT I was a support guy, so it was directly connected with the Postgres. So I started my IT career with Postgres and I\'m happy that I had this chance.\\n\\n\\n##### _[00:02:36] - Ry_\\n\\nYeah, that\'s crazy. I don\'t personally remember when I first started using Postgres. I think it could have just been when I started using Ruby on Rails, I don\'t know. But it\'s funny, I don\'t remember when I first used Linux or a lot of things, but it\'s great that you have that distinct memory. You started in support, you\'ve built some stuff. Now on top of it, tell me about the body of work you\'ve done on Postgres or with Postgres. I\'m curious to know what you\'ve built.\\n\\n\\n##### _[00:03:13] - Pavlo_\\n\\nSo right now at Cybertec I\'m a consultant, so my position called consultant, they called me consultant, but I prefer to think about myself as a developer. I\'m not an Admin, I\'m not a DBA, I\'m a developer. So I\'m mostly doing developing things for Postgres. I had a couple of small patches to these source, but I don\'t consider them as major things. So I did some client libraries for Delphi if somebody still knows what it is, and later I switched to C, Go, Python, that kind of stuff. And right now I\'m mostly develop with the Go language. And at the moment we have a couple of utilities made in Cybertec, so I\'m in charge of them. So one of them is like pg_timetable, which is a scheduler for PostgresQL, and another is pg_watch which is a monitoring solution for Postgres.\\n\\n\\n##### _[00:04:30] - Ry_\\n\\nYeah. Yeah. What caught my attention was pg_timetable because I come from a company that my previous company was Apache Airflow. So the idea of doing dags jobs that are chained off of other jobs, to me, there\'s no other way. Trying to schedule jobs and hope that they finish by the time of the next one, that\'s dependent is something that people used to do and they should never, ever do anymore. But I\'m sure it still happens. Right. So certainly if people are using pg_cron, no way to chain them, as far as I can see.\\n\\n\\n##### _[00:05:11] - Pavlo_\\n\\nWe had a client and we have a process, like, lasting, like 14, 15 hours consisting of many steps which are dependent in some complicated ways. So we used a make file to split them into the jobs and to specify that this stage must be executed after that. And that and that worked fine. It\'s cool to have a special utility to run that kind of jobs or chains, whatever.\\n\\n\\n##### _[00:05:56] - Ry_\\n\\nYeah. Well, one of the things I think about a lot is how Postgres, the Postgres server is doing lots of jobs, right. I don\'t know how many processes it\'s dealing with, someone with better internal knowledges, but I know it\'s certainly vacuuming. It\'s certainly processing queries. It\'s doing a lot of things right. And then question is I\'m a fan of SRP single responsibility principle and programming in general, but I also like monoliths too. We all have conflicting ideas in our heads. And having a queue managed inside the Postgres, you\'re kind of like straddling, right? Because you have a container next to it. So some of the work is happening over there, but some of it is inside of Postgres too. Why not have it all be inside? What are you thinking about for future versions of pg_timetable? Why not just do all the work inside of the running Postgres cluster?\\n\\n\\n##### _[00:07:00] - Pavlo_\\n\\nSo, yeah, thank you for the question because that was like, the second thought when we released the pg_timetable. So the idea is that PostgreSQL uses a separate process for everything. If you have a new connection, you need a new process. So if you have a background worker for cron or for another scheduler, you will need a background worker. A separate process. Yeah. It\'s possible. We can create an extensions with Go. It\'s possible, yes. But right now, I\'m not sure what benefits we will have.\\n\\n\\n##### _[00:07:58] - Ry_\\n\\nIt also feels kind of gross, right, to think of throwing it all together. But I have this conversation all the time with my team. If you have a system that\'s dependent on the Postgres, let\'s say there\'s an API, right? And the API, if the Postgres is only invoked when the API happens you could theoretically put all that compute into one bucket and just let the API talk to I\'m thinking, like, say, take it\'s a Rust API. Should you serve that Rust API as a separate container and have the separation of concerns? Or should you just create a custom Postgres extension and let the Rust run inside the Postgres cluster and there\'s zero latency to the data in that regard, but it\'s a little bit of a Frankenapp at that point. It\'s a tricky problem.\\n\\n\\n##### _[00:08:52] - Pavlo_\\n\\nI will tell you that, that I want to implement this scheduler as an extension for Postgres, not because I think that we will have some benefits, but because this is very interesting task for me as a developer, first of all.\\n\\n\\n##### _[00:09:08] - Ry_\\n\\nYeah. Is pg_timetable supported by any cloud providers right now?\\n\\n\\n##### _[00:09:16] - Pavlo_\\n\\nYeah, we can run pg_timetable against any cloud provider. If you want, you can run it in Docker and Kubernetes as a standalone binary, or even you can try to grab the source code and put it into this AWS fancy one time run and forget I don\'t remember how they called it.\\n\\n\\n##### _[00:09:46] - Ry_\\n\\nYeah, but because it has the sidecar, it probably can\'t be like a standard extension in RDS, for example.\\n\\n\\n##### _[00:09:54] - Pavlo_\\n\\nRight, it might be, but I think to make this happen, we need to somehow communicate with AWS and make that happen. It\'s more like politics, not yeah, well.\\n\\n\\n##### _[00:10:10] - Ry_\\n\\nI think it\'s also just like I don\'t know if any of the other extensions that they offer have a sidecar requirement. Right. So that\'s one of the things we\'re thinking about at Tembo is like, let\'s allow sidecars if you allow sidecars to extensions in our managed service yes. It\'s like, again, it\'s a lot more dangerous in the sense that there\'s more things that could break, but it\'s also more powerful. And if we had the rule that dangerous things are bad and you wouldn\'t have a combustible engine inside of a car where little explosions are happening all the time to power it.\\n\\n\\n##### _[00:10:50] - Pavlo_\\n\\nOn the other hand, you can build your own image, right, and somehow to limit the binary in it to do some dangerous things, et cetera.\\n\\n\\n##### _[00:11:02] - Ry_\\n\\nYeah. I joke sometimes and say we\'re building the dangerous Postgres just because it\'s not really we support 150 extensions now, so it\'s like, yeah, our support footprint is increased and scarier than if you had 75, but I still say it\'s worth it to the developer to have the capability. So I\'d love to get pg_timetable as a standard option. I want to use it, basically. What\'s funny is I\'m building a data warehouse right now for our company, and I started with pg_cron, and I have these four jobs that all start at the same time and it\'s very early, so it\'s not a problem. But I would love to have them be chained. Like I said, I could do that fake with some spacing things out, but I really feel like I\'ve just stepped back 20 years into the world of early data processing. So I appreciate what you built and use it soon.\\n\\n\\n##### _[00:12:08] - Pavlo_\\n\\nOkay. Let me know if you\'re happy with it.\\n\\n\\n##### _[00:12:11] - Ry_\\n\\nYeah, I think it\'s a good idea and it could also help developers avoid having to go pick up some other tool to take the next stage of their data processing, which to me is a big win. And we\'re trying to make the Meme \\"Postgres for everything.\\" I know you are too, in a sense, because you built the extension. But yeah, I think there\'s so much it can do, and it can do a lot more than just writing and reading data.\\n\\n\\n##### _[00:12:54] - Pavlo_\\n\\nOne more thing about having the pg_timetable as an external application is that you can use your own binaries inside jobs like baseql or pg_dump or something, like to grab something, to convert something, et cetera. So we want to have a possibility for developers to create their own docker images with the binaries or tools they only need and pg_timetable. So in this way it sounds like a Swiss knife, right? So I need a pg_timetable and a couple of utilities for grabbing something to manipulate with files, et cetera. And then we pack it and that\'s all. We don\'t need to think how we should allow the application to install it or to have it on the system version, et cetera, et cetera.\\n\\n\\n##### _[00:13:58] - Ry_\\n\\nYeah, basically what you\'re saying is it gets even more dangerous quickly, right? Like the base use case is one thing, but then when people start doing some more wild things, including their own, it could be their own packages, right? Their own, of course, completely custom. Nobody knows what\'s inside of it. It could be a bitcoin miner, it could be anything, right?\\n\\n\\n##### _[00:14:23] - Pavlo_\\n\\nYeah.\\n\\n\\n##### _[00:14:26] - Ry_\\n\\nAre you working on the next version of it or is it pretty stable right now and doesn\'t require a lot?\\n\\n\\n##### _[00:14:32] - Pavlo_\\n\\nSo at the moment we are stable. So we are open for ideas and then requests, et cetera. But at the moment we are fine, I believe. So it\'s like in maintenance mode, so we update versions, packages, et cetera. So no new box. I\'m happy with it.\\n\\n\\n##### _[00:14:56] - Ry_\\n\\nDid you build it? Was it sort of a small team there or did you kind of build it solo?\\n\\n\\n##### _[00:15:02] - Pavlo_\\n\\nSo the initial idea was by Hans, our CEO, and yeah, that was my first project in Go. So I was trying to learn a new language to see how can I do this? And we are now here. That\'s cool, I believe.\\n\\n\\n##### _[00:15:36] - Ry_\\n\\nWhat were the biggest challenges would you say you faced while working, while building it? Was it hard to, for example, understand internals or was that part easy? I\'m kind of curious what was toughest.\\n\\n\\n##### _[00:15:49] - Pavlo_\\n\\nFor me, I would say that the new tools that you need to use, like if you are familiar with your favorite IDE and then for Go language, you need to switch to something like Vs code or Go land or whatever. This is completely new tool for you. And you have no this muscular memory for shortcuts, et cetera, et cetera. It\'s kind of difficult. Like six months I was trying to get used to new tooling, but after that I\'m pretty happy.\\n\\n\\n##### _[00:16:26] - Ry_\\n\\nYeah, that\'s so when was it that you started working on timetable? pg_timetable?\\n\\n\\n##### _[00:16:35] - Pavlo_\\n\\nMaybe three years ago, something like that.\\n\\n\\n##### _[00:16:39] - Ry_\\n\\nYeah. Okay, well, that\'s great. So have you guys implemented it many times now for customers or how has it been? I know it\'s not a commercial product, but I assume it should be helping the consulting business, right? Yeah.\\n\\n\\n##### _[00:16:58] - Pavlo_\\n\\nSo it usually goes in a package with something. Usually if we have a new client, we are talking about high availability and we are talking about monitoring, and we are talking about that and that, and then we say, okay, we have a scheduler if you want it, and usually people want it. I don\'t remember we have like a standalone cell for a pg_timetable. But yeah, it\'s usually like packaged into something bigger, usually with a high availability and monitoring.\\n\\n\\n##### _[00:17:33] - Ry_\\n\\nGot it. It\u2019s like a sweetener, right? Cool. Well, maybe I\'ll shift into some more general topics. I\'d love to know what are some of the developments in Postgres ecosystem that you\'re excited about?\\n\\n\\n##### _[00:17:50] - Pavlo_\\n\\nSo the Postgres itself, the core, I always want to constantly want to do something for the core, but.\\n\\n\\n##### _[00:18:03] - Ry_\\n\\nIt\'s like you\'re racking your brain around there.\\n\\n\\n##### _[00:18:07] - Pavlo_\\n\\nThe process itself is not like user friendly. First of all, you need to communicate through the email lists. No GitHub, no issues, no pull requests. And you need constantly update your patches because the development is very active, and you need to defend your patch against many people in the community because why you did it, what purpose, why you do it that way, not another. So that might take some time for.\\n\\n\\n##### _[00:18:46] - Ry_\\n\\nWho are your favorite people in the Postgres ecosystem? You hate to play favorites, but people I should interview get to know anybody come to mind?\\n\\n\\n##### _[00:18:56] - Pavlo_\\n\\nYeah, a lot of them, frankly speaking. Well, I think that the community is the reason, number one, why I am still doing Postgres things. I was working like, for five years without knowing nobody from the community. And then in 2011, I was first time on the PG conference in Europe, in Amsterdam, and that changed my mind. Absolutely. I saw what the community is and what are the main things and why are those working that way and not another. So if you want names, so Bruce Momjian, for sure. Magnus Hagander, Ilya Kosmodemiansky, Vik Fearing, Andreas Scherbaum, a lot of them. Like L\xe6titia Avrot . A lot of them.\\n\\n\\n##### _[00:19:59] - Ry_\\n\\nWell, I\'ll reach out to those people. It\'s obviously a very big community too, certainly, considering the extent, anyone who\'s touching it is very excited. First, I\'m going to the know, the New York City conference here in a couple of months. Yeah. So I\'m excited to meet a bunch of people there. And we went to an event in San Jose last spring, which was fun. All right, so here\'s another one. If you had a magic wand and you could add any feature to Postgres? What would you change about it or what would you add?\\n\\n\\n##### _[00:20:39] - Pavlo_\\n\\nI don\'t think that I can change something in a way that it will be a brick and change but I would like to see if we can implement Postgres using threads, not the processes. I would like to see if that might be better solution that we have now. I\'m not sure.\\n\\n\\n##### _[00:21:04] - Ry_\\n\\nIt\'s being debated, right?\\n\\n\\n##### _[00:21:07] - Pavlo_\\n\\nYeah, absolutely.\\n\\n\\n##### _[00:21:08] - Ry_\\n\\nYeah, we\'ll see.\\n\\n\\n##### _[00:21:09] - Pavlo_\\n\\nIt\'s a hot topic.\\n\\n\\n##### _[00:21:10] - Ry_\\n\\nHot topic, yeah. I did join the mailing list for a short period of time. I\'m starting a company too and it just was too much. I was like, oh my gosh, this is a fire hose and someday I want to be able to have the time to consume it. But today wasn\'t that day. Is there anything about Postgres that no one agrees with you about? Do you have any contrarian views about any part of it that you can think of?\\n\\n\\n##### _[00:21:38] - Pavlo_\\n\\nYeah, probably my views about the Windows because I was a Windows guy and I\'m still a Windows guy and I\'m always saying that we need to support Windows as well as we support Linux. So that means installers for Windows and extensions and tools, et cetera, et cetera. And people usually say, no, Linux is fine and that\'s enough is enough.\\n\\n\\n##### _[00:22:08] - Ry_\\n\\nYeah, it\'s interesting, it\'s fun. I\'m interested in a lot of different areas of programming. I was looking at, say, Unity, you know, game development and it\'s C# and I\'m like \\"Ugh\\" and it seems like very Windows centric and I\'m like because I\'m a Mac user and whatever. For me it\'d be like Mac, Linux, Windows in terms of my priority but I\'m like certainly not C#, right? But anyway, yeah, the religious wars will never end there, I guess, in terms of platforms.\\n\\n\\n##### _[00:22:47] - Pavlo_\\n\\nThe bad thing about our approach right now that I think that we don\'t have enough good GUI programs for PostgreSQL every time we are using psql to show something to demo. And I think that for newbies it would be much easier to follow a demonstration if you use some fancy GUI application.\\n\\n\\n##### _[00:23:15] - Ry_\\n\\nWell, it was great chatting. Where can listeners find you online? Are you on social media anywhere?\\n\\n\\n##### _[00:23:23] - Pavlo_\\n\\nYeah, I\'m like on every kind of social media. Like GitHub, LinkedIn, Twitter, Instagram, Facebook, Blue Sky, Mastodon.\\n\\n\\n##### _[00:23:36] - Ry_\\n\\nGreat. All right, well, like I said, someday soon I\'m going to try pg_timetable and I\'ll give you some feedback and maybe some ideas in the GitHub and we\'ll see you there, I suppose.\\n\\n\\n##### _[00:23:47] - Pavlo_\\n\\nSure. Thank you.\\n\\n\\n##### _[00:23:48] - Ry_\\n\\nGreat, great chatting with you. Thanks for joining.\\n\\n\\n##### _[00:23:51] - Pavlo_\\n\\nThank you. Thank you."},{"id":"hacking-postgres-ep3","metadata":{"permalink":"/blog/hacking-postgres-ep3","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-10-20-hacking-postgres-ep3/index.md","source":"@site/blog/2023-10-20-hacking-postgres-ep3/index.md","title":"Hacking Postgres, Ep. 3: Eric Ridge","description":"In this episode, Ry and Eric talk about ZomboDB, the complicated road that led to the development of pgrx, and what it means for the future of extensions within Postgres. You can watch it below or listen to it on Apple/Spotify (or your podcast platform of choice). Special thanks to Eric for joining us today!","date":"2023-10-20T00:00:00.000Z","formattedDate":"October 20, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"hacking_postgres","permalink":"/blog/tags/hacking-postgres"}],"readingTime":18.005,"hasTruncateMarker":false,"authors":[{"name":"Eric Ankenman","url":"https://github.com/ericankenman","email":"noreply@tembo.io","imageURL":"https://github.com/ericankenman.png","key":"eric"}],"frontMatter":{"slug":"hacking-postgres-ep3","title":"Hacking Postgres, Ep. 3: Eric Ridge","authors":["eric"],"tags":["postgres","hacking_postgres"]},"prevItem":{"title":"Hacking Postgres, Ep. 4: Pavlo Golub","permalink":"/blog/hacking-postgres-ep4"},"nextItem":{"title":"Hacking Postgres, Ep. 2: Adam Hendel","permalink":"/blog/hacking-postgres-ep2"}},"content":"In this episode, Ry and Eric talk about ZomboDB, the complicated road that led to the development of pgrx, and what it means for the future of extensions within Postgres. You can watch it below or listen to it on Apple/Spotify (or your podcast platform of choice). Special thanks to Eric for joining us today!\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/tD9e6KXnB20?si=sjZJEGyxhqE7x9U-\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n\\nWant to know more about something they mentioned? Here\u2019s a starting point:\\n\\n* ZomboDB - https://github.com/zombodb/zombodb\\n* pgrx - https://github.com/pgcentralfoundation/pgrx\\n* plrust - https://github.com/tcdi/plrust\\n* Elasticsearch - https://www.elastic.co/elasticsearch/\\n* Supabase wrappers - [https://github.com/supabase/wrappers](https://github.com/supabase/wrappers)\\n* TCDI - https://www.tcdi.com/\\n* PostGIS - https://postgis.net/\\n* postgres_fdw -https://www.postgresql.org/docs/current/postgres-fdw.html\\n* Citus - https://www.citusdata.com/\\n* Valgrind - https://valgrind.org/\\n\\nDid you enjoy the episode? Have ideas for someone else we should invite? Let us know your thoughts on X at [@tembo_io](https://twitter.com/tembo_io) or share them with the team in our [Slack Community](https://join.slack.com/t/tembocommunity/shared_invite/zt-23o25qt91-AnZoC1jhLMLubwia4GeNGw).\\n\\n\\n## Transcript\\n\\n\\n##### _[00:00:12] - Ry_\\n\\nHey, Eric. What\'s going on, man?\\n\\n\\n##### _[00:00:14] - Eric_\\n\\nHi, Ry. How are you?\\n\\n\\n##### _[00:00:15] - Ry_\\n\\nLong time no talk.\\n\\n\\n##### _[00:00:16] - Eric_\\n\\nYeah, it has been a while.\\n\\n\\n##### _[00:00:18] - Ry_\\n\\nHow are things going?\\n\\n\\n##### _[00:00:19] - Eric_\\n\\nWell, I think.\\n\\n\\n##### _[00:00:21] - Ry_\\n\\nWell, maybe let\'s give our listeners a quick intro on what you\'ve done. I mean, Eric Ridge is pretty much a household name in mean, in case there\'s someone listening who doesn\'t know what you\'ve been working on. You want to give us a quick background on your involvement in postgres over the years?\\n\\n\\n##### _[00:00:36] - Eric_\\n\\nI think a household name is absolutely not true. First of all, I\'m Eric. I am the reason that this postgres extension name ZomboDB exists. And over the years of developing that, something that\'s now called Pgrx kind of fell out of all that work that lets us develop extensions for postgres using the Rust programming language. And these things have been where my interest has been as it relates to postgres. I guess past seven, eight years now. Things ZomboDB has been out since 2015, so we\'re right at eight years.\\n\\n\\n##### _[00:01:14] - Ry_\\n\\nDo you know how many it\'s at? An uncountable number of Rust extensions now, probably you probably at one point knew how many there were, but now you\'ve lost track.\\n\\n\\n##### _[00:01:24] - Eric_\\n\\nYeah, that\'s a good question. It\'s hard to know. There\'s no real clearinghouse around extensions.\\n\\n\\n##### _[00:01:30] - Ry_\\n\\nIf only there was, that would be a great thing for someone to build.\\n\\n\\n##### _[00:01:33] - Eric_\\n\\nYeah, it would. I wish somebody would get on that. Right. Yeah, that\'s a good question. There\'s some bigger names using Pgrx, and the Timescale is one. Supabase is one. We recently released plrust, and that\'s found its way out on RDS. So that\'s exciting for us. What we see popping up are small extensions that are written in Pgrx, and we see a lot of people writing I don\'t want to call them one off extensions, but one off extensions that are just, like, specific to their business use case and solve some interesting data domain problem that only they would have. But now they have the ability to build that business logic or data manipulation or what have you into their database and into the database itself.\\n\\n\\n##### _[00:02:27] - Ry_\\n\\nWithout having to go find a C programmer that\'s willing to do such work. Right?\\n\\n\\n##### _[00:02:31] - Eric_\\n\\nYeah. And that was one of the motivators for developing Pgrx, was I guess I had been working on postgres extensions in some form or fashion since postgres 8.0, and I think that was even before the extension packaging and create extension command and all that came into existence not long before. But I think that that was before and yeah, I mean, just over the years, I got tired of working in C. When you\'re developing a postgres extension in C, you\'re not only putting on your C programmer hat, but you\'re trying to put on the hat of a postgres hacker. Right. You need to understand postgres sources just as much as you do the language itself. So Rust was becoming a popular a number of years ago. So I was like, you know what, I\'m going to try to do this.\\n\\n\\n##### _[00:03:22] - Ry_\\n\\nWell, it\'s interesting. It\'s like how a lot of games, if they have mods, the language for the mods has to be easier than the language for the core game. Right. So a game might be written in something like Unity, but then they give you, like, Lua as a tool or JavaScript or something simple to build. But in Postgres, that wasn\'t true. Now, is Rust as simple as Lua or JavaScript? Maybe not, but it\'s still like very beloved people who try to learn it. In my experience so far, it\'s 100% like anyone who tries to learn Rust learns it and loves it. I haven\'t found someone yet and I have a small sample size, but, yeah, seems like it\'s a pretty good hit rate in terms of the perfect mix of control and speed with the ease of onboarding.\\n\\n\\n##### _[00:04:10] - Eric_\\n\\nIf you\'re not a C programmer, then writing the postgres extension in C is going to be difficult again, because C and then also because you\'re really programming Postgres\'s version of C, which is great, but it\'s not JavaScript, it\'s not Ruby, it\'s not Java. If you\'re not a Rust programmer, first you\'ve got to learn Rust, and Rust is different enough from all the other languages to make that a point. But what we\'re trying to do with Pgrx, and it\'s going to be a multi year effort to do this, it may never get done, but we\'re trying to present the postgres internals in the most idiomatic way possible to Rust. Right. So that if you are a Rust programmer or you are beginning to learn Rust, it\'ll be pretty obvious what you need to do in order to make your postgres extension useful.\\n\\n\\n##### _[00:05:10] - Ry_\\n\\nSo you said you were working on extensions as early as Postgres Eight. Were you a Postgres user or our developer even earlier than that, or was that sort of when you got involved?\\n\\n\\n##### _[00:05:21] - Eric_\\n\\nYeah, Postgres user, I think we started using Postgres at work in the year 2000 and that might have been I mean, is that 7.1, 7.2? It\'s quite a long time ago and we\'ve been using it every day since. And, yeah, somewhere along the way we needed to do some work inside the database. So here we are today.\\n\\n\\n##### _[00:05:46] - Ry_\\n\\nYeah. For me, when I think about when did I start using Postgres, it\'s like thinking about when did I first start using a laptop, it\'s like I don\'t remember exactly when, but definitely transitioned to having a laptop at some point. But, yeah, it didn\'t seem monumental to start using Postgres back then, but obviously it\'s come a long way and it\'s a pretty exciting ecosystem right now. So, yeah. What do you think the most powerful thing about extensions are? Do you have any thoughts on what are the most powerful things you can do with extensions, out of curiosity?\\n\\n\\n##### _[00:06:19] - Eric_\\n\\nIt\'s an interesting question.\\n\\n\\n##### _[00:06:21] - Ry_\\n\\nWhat\'s the coolest part of, I guess what API? Or maybe another good question is what hasn\'t been exposed yet that should be or needs to be in Pgrx? That would be powerful.\\n\\n\\n##### _[00:06:32] - Eric_\\n\\nYeah, I\'ll start there and then work backwards. There\'s a lot of internal APIs within Postgres, and there\'s a number of interesting ones. There\'s the index access method API, which is really easy to implement. We haven\'t added safe wrappers around that to Pgrx, but it\'s real easy to implement. It\'s really just a handful of functions. There\'s the Foreign Data Wrapper API, which Supabase released a project that they call Wrappers that\'ll let you create foreign data wrappers using Rust backed up by Pgrx. So kind of the next big API around that genre of internals. There would be the Table Access Method APIs, and we haven\'t exposed these at all yet. They\'re actually difficult to get at even through our bindings just because of the way they\'re set up in the postgres sources. So when we get around to doing that, and I hope that we will, there\'s going to be some development effort on our part to safely expose these or safely expose the Table Access Method APIs. But that\'s the API that would let you write your own heap storage engineer system in Rust.\\n\\n\\n##### _[00:07:49] - Ry_\\n\\nTell me about I mean, probably not everybody knows what ZomboDB is. Maybe quickly describe what it does and what it doesn\'t do and what\'s coming next with it.\\n\\n\\n##### _[00:07:59] - Eric_\\n\\nYeah, it\'s the worst named software product.\\n\\n\\n##### _[00:08:03] - Ry_\\n\\nFirst off, why\'d you call it Zombo, everyone wants to know.\\n\\n\\n##### _[00:08:06] - Eric_\\n\\nI think, Ry, you\'re old enough to remember Zombo.com. If you don\'t remember it, then that just means you weren\'t playing on the internet. You\'re probably doing real work. It still exists so listeners can go and check it out, but the name just came from that. So ZomboDB is a postgres extension, and it\'s been through numerous iterations over the past eight years, but it has always been postgres extension that lets you create an index on a table that\'s backed by Elasticsearch. Typically the usage pattern is you create the index indexing all of the columns in the table and you point it to an Elasticsearch endpoint over Http or Https, and the ZomboDB keeps that index up to date. It keeps the index MVCC correct. All of your search results are you only see results that would otherwise be visible within the executing transaction has its own query language for doing full text queries, so users don\'t have to learn the Elasticsearch query DSL, which is pretty intense.\\n\\n\\n##### _[00:09:14] - Ry_\\n\\nCan you use the Elasticsearch DSL with it, though?\\n\\n\\n##### _[00:09:17] - Eric_\\n\\nYou can, and there\'s a number of ways to do that. The query language itself allows you to just insert as part of an expression. You can insert a JSON block to represent a query DSL node. There\'s a full set of SQL builder functions, so you can build query DSL through SQL statements. You can mix and match these things together. It\'s pretty sophisticated on that front, and it also exposes nearly all of Elasticsearch\'s aggregate endpoints to enable you to do really interesting aggregations with your full text. And since all the fields of a table are also stored in the Elasticsearch index, you can do some really interesting things that involve full text queries and aggregating on various metadata fields and do all this with a large distributed elastic search cluster behind the scenes and do it in near real time. It\'s really slick. The downside to ZomboDB is that you now have an Elastic search cluster running alongside your postgres cluster. And ES is a living breathing system. It requires significant attention and maintenance in production. It gives you a lot of capabilities to scale out horizontally very easily, but it is its own living breathing system there.\\n\\n\\n##### _[00:10:42] - Ry_\\n\\nYeah, it\'d be great if you can build a version of this without requiring elastic.\\n\\n\\n##### _[00:10:46] - Eric_\\n\\nYeah, I know a guy that\'s working on that. I know a company that\'s working.\\n\\n\\n##### _[00:10:50] - Ry_\\n\\nYeah, someday that\'d be possible.\\n\\n\\n##### _[00:10:53] - Eric_\\n\\nYeah, well, you had asked where the name came from and I said all that to say that the name came from. Once we kind of got ZomboDB working, we were like, whoa, it\'s really amazing what you can do with this and how easy it is to use. And one of the mantras from Zombo.com is the only limit is yourself. And ZomboDB really kind of makes it feel that way because it makes querying your data, especially real dense text data, just super simple.\\n\\n\\n##### _[00:11:26] - Ry_\\n\\nAnd so you use Zombo pretty aggressively within TCDI, which is where you work, right?\\n\\n\\n##### _[00:11:33] - Eric_\\n\\nYeah, TCDI is my employer. Yeah. We\'ve had Zombo in production since 2013 and we didn\'t open source until 2015. Yeah, I don\'t want to give away numbers, but we have more ZomboDB installations and Elasticsearch instances than we probably want.\\n\\n\\n##### _[00:11:53] - Ry_\\n\\nIs it sort of a competitive advantage for the business? The power that I know, you guys, it\'s like legal documents. I might get it wrong, but like searching legal documents to some degree, right? Is it a competitive advantage for the business? The fact that Zombo is inside, would you say?\\n\\n\\n##### _[00:12:08] - Eric_\\n\\nYeah, absolutely. Our industry is ediscovery litigation support and among many other areas within that domain, searching large volumes of text is a key thing. And we\'re the only player in our industry that has text search capabilities to the degree that we do. And I\'m not going to go name or other competitors or anything, but their abilities to do complicated text search that involves fuzzy queries plus regular expressions, plus boolean expressions, plus long tail proximity queries hit highlighting it\'s our abilities to do that just far exceed our competitors because of ZomboDB, because of postgres, and because of Elasticsearch.\\n\\n\\n##### _[00:12:55] - Ry_\\n\\nDo you use the extension in both transactional use cases as well as analytical? Or is it mostly just powering the search bar inside the product or is there some back end? I guess I call it like analytical type use cases for it as well.\\n\\n\\n##### _[00:13:12] - Eric_\\n\\nYeah. So with postgres. Right. Everything is transactional.\\n\\n\\n##### _[00:13:15] - Ry_\\n\\nYeah.\\n\\n\\n##### _[00:13:16] - Eric_\\n\\nWe expose search in a very complex manner. I guess, to our users. Users, it\'s much more than just a search box. They can search by particular fields and design proximity queries, and they can do term lookups. They can create real time dashboards and whatnot through our software. That\'s all backed by ZomboDB. One interesting use case is in the world of litigation. The plaintiffs will provide the defendants with a list of keywords that need to be searched in order to find documents that are responsive to particular to these topics. And these keywords are sometimes just literally a keyword one word. Sometimes they\'re phrases, sometimes they\'re wild card terms. Sometimes they\'re phrases with embedded wildcards. Sometimes they\'re patterns that need to be recognized. And so, like an analytics side of what we use ZomboDB for is taking these ginormous lists of things and kicking it off and getting back a list of the 6 million documents out of the 200 million that might match this set of keywords that the plaintiffs have requested production for.\\n\\n\\n##### _[00:14:25] - Ry_\\n\\nYeah, that\'s intense. Could see the power of that. What are your other favorite extensions? Probably all the other ones you\'ve made, but give me your top five postgres extensions. zombos, one. What are a couple of others that you like?\\n\\n\\n##### _[00:14:39] - Eric_\\n\\nI don\'t use PostGIS or PostGIS, but I am super impressed with it. I think of it as kind of like the gold standard of what a postgres extension ought to look like. ZomboDB does integrate with it, but the data world that I live in doesn\'t involve the need to do geographical type queries. But I\'ve always been really impressed with it. I think the postgres foreign data wrapper as an extension is really great, and that comes from the core community, but pretty impressed with that. Citus has always been interesting to me. I guess they\'ve long since been bought out by Microsoft and are still going strong. I think that Citus really shows off the power of what a postgres extension can do, and it also shows off the power of having committers on the hackers and hackers to be able to help move the extension system in postgres forward so that you can then implement something like Citus on top of it.\\n\\n\\n##### _[00:15:36] - Ry_\\n\\nWhen they were building that, did they have to add things to extensions in order to build that? I don\'t know the whole history of Citus, but was that kind of like.\\n\\n\\n##### _[00:15:46] - Eric_\\n\\nYeah, they\'ve had a number of changes in postgres, I think, especially in the planner and executor to be able to support the ability to extend it better. And I think some of that came from the original Citus company people. That\'s great, right?\\n\\n\\n##### _[00:16:03] - Ry_\\n\\nYeah, that\'s awesome.\\n\\n\\n##### _[00:16:05] - Eric_\\n\\nWhat I like to see are people using Pgrx to just solve a problem that they have with their application. I see things like somebody just needed to talk to an S3 bucket, and so they put together a little Pgrx extension to do that. Little things like that are what get me excited about Postgres extensions.\\n\\n\\n##### _[00:16:26] - Ry_\\n\\nYeah, it\'s pretty timely. I have a co-op that started literally today. Shout out to Jay and I told him to build a clerk FDW using the Supabase Wrappers SDK. And he\'s like, what the hell do you say to me? Just try read the docs, we\'ll see where it goes. But I\'ll report back how that goes. But yeah, Clerk, we\'re using Clerk for Auth for our company, and I just need a simple read access to three collections. So I figure he should be able to get it done by the end of the day, but I might be pushing too hard.\\n\\n\\n##### _[00:17:00] - Eric_\\n\\nWell, it\'s possible. And that\'s a great example of where having a simple framework for making an extension is really powerful. Right. I mean, once you figure out the documentation and how to tie the two things together, it probably is just an afternoon\'s worth of work. And now you\'ve got something that solves a problem and you can move on.\\n\\n\\n##### _[00:17:21] - Ry_\\n\\nYeah, I figure his second one maybe will take an afternoon, first one might take a few days, but we\'ll see.\\n\\n\\n##### _[00:17:26] - Eric_\\n\\nWell, sure, yeah.\\n\\n\\n##### _[00:17:28] - Ry_\\n\\nWell, that\'s great. Yeah, we\'re super excited, obviously, by all the possibilities that Pgrx brings to Postgres, and the great work you guys have done historically to get us to this point is much appreciated.\\n\\n\\n##### _[00:17:42] - Eric_\\n\\nWell, thanks. That\'s one of the reasons why we, TCDI donated Pgrx to the PG Central Foundation is to provide it a long term home for the Postgres community. TCDI is a litigation support company. We\'re not a database company, so to speak. So we wanted to get this stuff in the hands of a foundation that can help shepherd it forward and make sure that it can survive.\\n\\n\\n##### _[00:18:09] - Ry_\\n\\nI imagine you have some open issues there that you could use help with. They\'re probably pretty hard to do, right? Like building a framework is a lot harder to building a thing with a framework. But are you looking for additional help from people to build out Pgrx?\\n\\n\\n##### _[00:18:23] - Eric_\\n\\nYeah, for sure. I can\'t even tell you off the top of my head how many contributions we\'ve had over the past four years that Pgrx has been alive. And it seems like every couple of weeks somebody new comes by with a new contribution. But yeah, one of the things that we\'re focused on right now is I guess it\'s two things kind of concurrently. One is we\'re improving Pgrx, is testing fundamentals, and it\'s both tying in more interesting things in CI like Valgrind, to different approaches to unit testing, which includes we\'re doing a big push on property testing right now within Pgrx. I think there\'s going to be some pull request landing in the next week or so with that. And we\'re also focused on improving some of the memory safety and soundness impedance mismatches between Postgres internals and Rust\'s compiler. So it\'s a big concerted effort to take a look at these issues.\\n\\n\\n##### _[00:19:25] - Ry_\\n\\nWell, it seems like contributing to Pgrx could be like a gateway drug towards getting closer to helping with postgres itself. Right. Because you need to understand the internals to do anything in know, at least unless you\'re working on tests or yeah, I mean in general, adding new features to Pgrx means understanding postgres well enough to create that interface, which is big challenge.\\n\\n\\n##### _[00:19:48] - Eric_\\n\\nIt does. And we myself and the others that on the decor development team there, we spend probably as much time in the postgres sources as we do actually writing code for it in.\\n\\n\\n##### _[00:20:03] - Ry_\\n\\nYou guys. If you come up with a big new release of Pgrx or any other extensions that you can think of, I\'d love to have you back. Eric, it was great catching up with you.\\n\\n\\n##### _[00:20:12] - Eric_\\n\\nYeah, sure. Likewise. Thank you so much for having me."},{"id":"hacking-postgres-ep2","metadata":{"permalink":"/blog/hacking-postgres-ep2","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-10-19-hacking-postgres-ep2/index.md","source":"@site/blog/2023-10-19-hacking-postgres-ep2/index.md","title":"Hacking Postgres, Ep. 2: Adam Hendel","description":"In this episode, Ry and Adam talk about developing extensions for Postgres, being in the trenches of the modern data stack sprawl, and the future of what\u2019s possible with Tembo Stacks. If you haven\u2019t seen or listened to it yet, you can do so below, or on Apple/Spotify (or your podcast platform of choice). Special thanks to Adam for joining us today!","date":"2023-10-19T00:00:00.000Z","formattedDate":"October 19, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"hacking_postgres","permalink":"/blog/tags/hacking-postgres"}],"readingTime":18.66,"hasTruncateMarker":false,"authors":[{"name":"Eric Ankenman","url":"https://github.com/ericankenman","email":"noreply@tembo.io","imageURL":"https://github.com/ericankenman.png","key":"eric"}],"frontMatter":{"slug":"hacking-postgres-ep2","title":"Hacking Postgres, Ep. 2: Adam Hendel","authors":["eric"],"tags":["postgres","hacking_postgres"]},"prevItem":{"title":"Hacking Postgres, Ep. 3: Eric Ridge","permalink":"/blog/hacking-postgres-ep3"},"nextItem":{"title":"Unleashing the power of vector embeddings with PostgreSQL","permalink":"/blog/pgvector-and-embedding-solutions-with-postgres"}},"content":"In this episode, Ry and Adam talk about developing extensions for Postgres, being in the trenches of the modern data stack sprawl, and the future of what\u2019s possible with Tembo Stacks. If you haven\u2019t seen or listened to it yet, you can do so below, or on Apple/Spotify (or your podcast platform of choice). Special thanks to Adam for joining us today!\\n\\nWant to know more about something they mentioned? Here\u2019s a starting point:\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/8vkhlaY7AqM?si=ttqL-LBuwgXzEvFG\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n\\n\\n* Pgmq - [https://github.com/tembo-io/pgmq](https://github.com/tembo-io/pgmq); also see Adam\u2019s blog post about pgmq - [https://tembo.io/blog/introducing-pgmq](https://tembo.io/blog/introducing-pgmq)\\n* pg_later - [https://github.com/tembo-io/pg_later](https://github.com/tembo-io/pg_later); Adam wrote about this one too - https://tembo.io/blog/introducing-pg-later\\n* clerk_fdw -[https://github.com/tembo-io/clerk_fdw](https://github.com/tembo-io/clerk_fdw); for more information, have a look at our blog post about it - https://tembo.io/blog/clerk-fdw/\\n* Supabase wrappers - [https://github.com/supabase/wrappers](https://github.com/supabase/wrappers)\\n* pgrx - https://github.com/pgcentralfoundation/pgrx\\n* pg_cron - https://github.com/citusdata/pg_cron\\n* Kafka - https://kafka.apache.org/\\n* Tembo Stacks - [https://github.com/tembo-io/tembo-stacks/tree/main/tembo-operator/src/stacks/templates](https://github.com/tembo-io/tembo-stacks/tree/main/tembo-operator/src/stacks/templates); also check out our blog about them - [https://tembo.io/blog/tembo-stacks-intro](https://tembo.io/blog/tembo-stacks-intro)\\n\\nDid you enjoy the episode? Have ideas for someone else we should invite? Let us know your thoughts on X at @tembo_io or share them with the team in our [Slack Community](https://join.slack.com/t/tembocommunity/shared_invite/zt-23o25qt91-AnZoC1jhLMLubwia4GeNGw).\\n\\n\\n## Transcript\\n\\n\\n##### _[00:00:13] - Ry_\\n\\nWelcome to the show, Adam. Excited to chat with you today. How many postgres extensions have you authored in 2023?\\n\\n\\n##### _[00:00:23] - Adam_\\n\\nOh, it at least two.\\n\\n\\n##### _[00:00:28] - Ry_\\n\\nAt least two?\\n\\n\\n##### _[00:00:29] - Adam_\\n\\nWell, in early 2023, I was just like, getting started and trying to figure out how to write one. And I know I wrote like a handful of them that didn\'t really do anything. I think the first one was just put like a rest server inside an extension and see what happens. Kind of didn\'t work very well.\\n\\n\\n##### _[00:00:57] - Ry_\\n\\nYeah, I remember that.\\n\\n\\n##### _[00:00:59] - Adam_\\n\\nI think I made like, a prometheus exporter, and then recently there were two ones that are kind of more useful.\\n\\n\\n##### _[00:01:05] - Ry_\\n\\nYeah, that\'s great. Well, so obviously we know each other. We both worked together at Tembo. I mean, I could ask you questions as if I don\'t already know the answers, but since I do, I\'m not going to do that. I was going to say getting into postgres, though, joining Tembo wasn\'t your first touch of postgres. Do you remember starting with, like I can\'t really quite remember when I started. But can you remember when you did?\\n\\n\\n##### _[00:01:33] - Adam_\\n\\nWell, the first time I heard of postgres I\'m pretty sure was like in undergrad, like database management systems class. And it was like, there\'s Oracle, there\'s these companies around it, and then there\'s open source stuff, postgresQL, you know? And I remember thinking, like, oh, that\'s And I probably Googled it and saw the elephant. And then I know I started using it in undergrad. I was doing some scientific computing stuff and I just needed somewhere to I didn\'t want to just keep writing to CSVs. I wanted to put it somewhere that was easier to search. And I just used postgres for it. Didn\'t really know what I was doing. It was just like, dump it into this database.\\n\\n\\n##### _[00:02:21] - Ry_\\n\\nNice, nice. And then as far as hacking postgres, that just started recently, or have you messed with the internals of postgres prior to this year?\\n\\n\\n##### _[00:02:35] - Adam_\\n\\nDefinitely not prior to this year. It\'s mostly focused on the application layer, building stuff on top of postgres. This year still getting deeper into the internals of postgres. I had installed some extensions before this year. Definitely didn\'t even think about how do you write build an extension, but it\'s.\\n\\n\\n##### _[00:03:00] - Ry_\\n\\nPretty easy to build them?\\n\\n\\n##### _[00:03:01] - Adam_\\n\\nYeah, that\'s very recent for me.\\n\\n\\n##### _[00:03:02] - Ry_\\n\\nPretty easy to build them. Right. Would you say?\\n\\n\\n##### _[00:03:04] - Adam_\\n\\nYeah. A shallow one is easy if you can learn rust. Yeah, it\'s a challenge. I think it\'s hard.\\n\\n\\n##### _[00:03:17] - Ry_\\n\\nYeah, I was kind of kidding.\\n\\n\\n##### _[00:03:19] - Adam_\\n\\nAnd then there\'s like a learning curve, and then once you get over a curve, you can kind of get moving with it.\\n\\n\\n##### _[00:03:27] - Ry_\\n\\nMaybe. Let\'s talk about the extensions that you\'re, I guess, most proud of or have the most traction so far. What are those?\\n\\n\\n##### _[00:03:39] - Adam_\\n\\nPgmq, I think, has the most traction message queue extension. It\'s a lot like SQS Amazon\'s simple Queue service or Redisimple Message queue except on postgres. Well, we wrote it for Tembo to help run our cloud. We needed a message queue between Control Plane and Data Plane, so we wrote it for that. And then just in the last couple of months, we started kind of talking about it in the community. And after we wrote a blog about it, we\'ve had a few people from the community that didn\'t know before who are now consistently contributing to the project. So Pgmq is definitely the one that I think has most traction.\\n\\n\\n##### _[00:04:36] - Ry_\\n\\nAnd I think you\'re also working on well, you kind of have a stack of them, right? Because your other extension relies on Pgmq. True?\\n\\n\\n##### _[00:04:48] - Adam_\\n\\nPG later.\\n\\n\\n##### _[00:04:50] - Ry_\\n\\nIs that true?\\n\\n\\n##### _[00:04:51] - Adam_\\n\\nYeah. Okay. Yeah, PG later, that\'s another extension that lets you submit a query to Postgres and then forget about it and come back later and see the results from that query. And that\'s built on top of Pgmq. So it\'s like a stack of extensions.\\n\\n\\n##### _[00:05:20] - Ry_\\n\\nNice. I guess you\'re in the position now to be the extension mentor for others in the company who may be building extensions.\\n\\n\\n##### _[00:05:29] - Adam_\\n\\nI know.\\n\\n\\n##### _[00:05:29] - Ry_\\n\\nI just hired a co op who\'s working on one, and I imagine you\'re mentoring him to some degree.\\n\\n\\n##### _[00:05:37] - Adam_\\n\\nYeah, Jay talking about? Yeah, Jay\'s. Great. Hey, Jay. Jay, if you\'re listening yeah. Jay\'s been working on the another extension written in Rust, Clerk FDW. So it\'s a foreign data wrapper around Clerk, which is like our identity provider. We just want to be able to build this data warehouse that has our users and their organizations and have that data persisted in our data warehouse. So we built well, Jay is building that foreign data wrapper around Clerk, which is pretty cool.\\n\\n\\n##### _[00:06:17] - Ry_\\n\\nYeah, we probably should give a shout out to Supabase for the Wrappers project that we\'re using to build that with. Sounds like it\'s been a pretty nice experience. Jay\'s a co op that just joined, like, last week and so already being very productive. So I\'m excited. I think that there\'s the possibility of an explosion of new extensions for Postgres now that Pgrx allows us to use Rust. You agree with that?\\n\\n\\n##### _[00:06:51] - Adam_\\n\\nYeah, it\'s pretty powerful because take this Clerk FDW, that extension that Jay\'s building. We can bootstrap the software together the same way as you might do outside of Postgres, but we don\'t have to spin up another resource somewhere, manage a pod or another VM, can package it up into an extension, use Supabase Wrapper. So that\'s a software that\'s already built. It\'s tested. We don\'t have to reinvent the wheel on that. We write the extension in Rust. So we pull in Rust libraries that are already built, already tested. Then we write our own tests on top of these things and then package it into our extension and then deploy it as part of the database. And then we can kind of just monitor the data there and not have to manage another service that could crash.\\n\\n\\n##### _[00:07:54] - Ry_\\n\\nYeah, it\'s pretty nice. So I was going to change a little bit to...you\'ve also been using a lot of other people\'s extensions. Again, probably a lot more since joining Tembo than before, but what are some of your, I don\'t know, favorite extensions that you\'ve been exposed to?\\n\\n\\n##### _[00:08:15] - Adam_\\n\\nYeah, you said PG Cron a few different places. There\'s another extension that I\'m working on. It\'s, like, really early, but it\'s kind of wrapping up kind of some of the stuff that Langchain does in Python, but wrapping it up into an extension, pulling in PG vector into that, pulling in pg_cron, pulls in Pgmq and PostgresML as well. So it\'s like a bootstrapped extension. So all of those are pretty good PostgresML, pg_cron, pgvector, and they all have pretty good docs, so you can get up and running with them quickly and kind of figure out what you need to do.\\n\\n\\n##### _[00:09:09] - Ry_\\n\\nYeah. So have you found that you\'ve had to study the internals of postgres much as part of the process of building these extensions?\\n\\n\\n##### _[00:09:23] - Adam_\\n\\nNot really too much. Only when I\'m having trouble getting it up and running for the first time do I need to really look at postgres. But a lot of these extensions don\'t. They\'re not like touching with hooks into and replacing core functionality of postgres. They\'re extending things. So a lot of what PostgresML does is just give you functions that go off and make rest calls or download data from elsewhere in the Internet and put it in your database for you, give you access to some pre trained machine learning models. And it\'s not changing fundamentally how postgres runs. It\'s still like the normal postgres. It\'s kind of like a layer on top of yeah. Yeah.\\n\\n\\n##### _[00:10:19] - Ry_\\n\\nI think some extensions can do surgery and some are, know, laying on top of and not messing with the core functionality.\\n\\n\\n##### _[00:10:28] - Adam_\\n\\nRight? Yeah, it\'s an area I\'m kind of new with that, so I haven\'t gotten super deep into replacing core functionality of postgres quite yet. But soon we\'ll probably start working in that space a little bit more.\\n\\n\\n##### _[00:10:47] - Ry_\\n\\nWhat\'s, like the testing story? If you\'re a dev that likes to write TDD, test driven development, are you able to do that with extensions in particular, I guess, with Pgrx. What\'s the test driven story there look like?\\n\\n\\n##### _[00:11:03] - Adam_\\n\\nPgrx, I think, is pretty good. They have a way for you to say unit test or integration test for the extension that you\'re writing, so you can say, execute some queries and then make assertions on the outputs of those queries. So kind of like, have normal assertions that you would do in whatever language that you\'re testing. Pgrx has some tooling around like, oh, I want to spin up postgres 15.3, spin up a new environment for that, install your extension into it, and then run your test suite on it. Yeah, I guess it can get a little bit trickier, I think could be because depending on how complex the extension is, you could have system dependencies, um, like, oh, I need a specific version of GCC or something, or OpenSSL version something has to be installed. I haven\'t really quite found a good way to test all of those things to make it super portable to say, like, yep, all these test pass. And that means my extension is just good for everybody.\\n\\n\\n##### _[00:12:28] - Ry_\\n\\nFor the test running on your local machine. I know they\'re running on your local machine, but in a docker container or just natively.\\n\\n\\n##### _[00:12:41] - Adam_\\n\\nYou could do both, I guess. Kind of like my workflow for it. I guess I would do, like, run the test locally and then have the extension in a GitHub repo and have a CI pipeline that runs the same tests, but in Docker or within a GitHub workflow to be like, hey, before you merge this, you need to pass all these tests, kind of gate yourself a little bit.\\n\\n\\n##### _[00:13:10] - Ry_\\n\\nThat\'s great. Well, cool. So I was going to ask, as far as your background prior to Tembo, you were doing little ML engineering too. Did you use postgres much in that role? I guess the question is, did you use any of the special parts of Postgres or was it really just like a standard transactional store with little knowledge of the extension ecosystem?\\n\\n\\n##### _[00:13:39] - Adam_\\n\\nNothing too crazy. I\'ve always tried to do a queue on postgres, so be like, build up this giant queue of things that a machine learning model needs to make predictions on, or make a bunch of different predictions, dump them into a queue, and then have another process. Look at all these predictions and pick the best one before giving it back to the user. But a lot of OLTP like high transaction workloads in kind of machine learning space. Yeah.\\n\\n\\n##### _[00:14:18] - Ry_\\n\\nSo you said you\'ve been trying to build queues historically, a lot of times in bigger companies, there are less tools for various tasks. For example, use redis if you want to build a queue, or use snowflake if you want to build a data warehouse. I don\'t even know that these things were officially declared by any sort of powers that be. It\'s more like the sales reps at those companies decided that this company is going to be a salesforce company or a snowflake company. Right. It\'s kind of a slow boil that you suddenly realize, oh, I guess we have kafka now. I guess we\'re a kafka shop and starts with a POC and then ends up you have all these tools. We call it the modern data stack. But yeah, I\'m curious how your take on that in particular? Around start with the queue since you\'ve just implemented it in postgres and probably had to use other tools in the past.\\n\\n\\n##### _[00:15:16] - Adam_\\n\\nYeah, a couple of places that I\'ve worked at, it\'s like, hey, we need to pass messages between two services. And any engineer will be like, all right, well, we\'re going to evaluate what we have here. We want to try to do things like, oh, let\'s keep things simple, not make things too complex. And then depending on the size of the organization, it\'ll be like, well, we use Kafka for messaging, so use Kafka. And so it\'s like, okay, so make progress. A lot of times it\'s better to not fight that and just be like, we want to make progress, we just need our application to work. It\'s not like Kafka wouldn\'t work for this, but it\'s definitely overkill for a lot of situations. So then it\'s like, okay, we\'re going to build this in Kafka and maybe the rest of the application is in postgres, but this message piece of it, we\'re going to put it in Kafka. And from the application perspective, it\'s kind of no difference. Like I said, it\'s an overkill tool for a lot of use cases. But then when things start to go wrong and need to troubleshoot it, it\'s like, okay, now we have to bring in the Kafka expert.\\n\\n\\n##### _[00:16:35] - Adam_\\n\\nAnd if it\'s a big company, it\'s probably a handful of people who are on this Kafka team and they got a lot of other stuff going on. So it\'s like, what do you have to do? You have to learn Kafka. Yeah.\\n\\n\\n##### _[00:16:47] - Ry_\\n\\nAnd you\'re a small use case, right? You\'re like a small unimportant use case. And so, yeah, you can\'t get their attention. You kind of accepted the fact that, okay, I can learn. It\'s kind of fun to learn new technologies and try new things out, right? That\'s the day. Zero joy of learning something new. But then now all of a sudden you\'ve got to support it, right?\\n\\n\\n##### _[00:17:09] - Adam_\\n\\nYeah.\\n\\n\\n##### _[00:17:10] - Ry_\\n\\nYou support what you make a lot of. Yeah, you got judoed into that.\\n\\n\\n##### _[00:17:16] - Adam_\\n\\nIn my career, it was fun to learn Kafka and there\'s some things I really like about it, but then at the same time, it\'s a very complex tool and it does take a team to run and manage it. Same with RabbitMQ. If you\'re going to do those things, you need some people dedicated to making sure that they\'re functioning the way you expect it to.\\n\\n\\n##### _[00:17:45] - Ry_\\n\\nYeah, well, I think kind of leads into one of our core missions at the company, which you\'re leading at Tembo, which is our stacks. I guess it probably would make sense for you to say a few words on what we\'re trying to accomplish with stacks at the company.\\n\\n\\n##### _[00:18:05] - Adam_\\n\\nYeah. So stacks are workload optimized postgres clusters. So the message queue stack is one that we have and our goal with that is if you need to run a message queue, we want this to be the most optimized way to do a message queue on postgres. Of course, there\'ll be a point in time when it\'s like, hey, your use case has grown so big that maybe that stack\'s not going to fit you, but that stack will be to the very edge of what postgres can do for that workload. We\'re taking that same approach with our OLAP stack that we\'re building right now. We have an OLTP stack, there\'s a machine learning stack. So each one of these stacks is do it on postgres and we\'re going to make it be the best possible squeeze every last piece of juice out of postgres that we possibly can for that workload.\\n\\n\\n##### _[00:19:08] - Ry_\\n\\nYeah. And you\'re curating extensions for each stack. What else are you doing besides that.\\n\\n\\n##### _[00:19:13] - Adam_\\n\\nYeah, extensions are a big piece. A lot of that has to do with there are certain types of developer experience that we think people want around workloads. And then there\'s the postgres configuration itself. So like, what should shared buffers be or how many parallel worker processes and how should the auto vacuum vacuum error be tuned for that workload? That\'s a whole class of things that are unique to every stack. Of course, there\'s like a user interface component of every stack as well. So if you want to come up and to look and see and observe the stack, there\'ll be user interface metrics are kind of really tightly related to the UI, so there\'s different metrics for every stack as well. Some of them are going to be the same across stacks. But for example, current number of messages in a queue, that\'s like a metric that you can monitor, you can write alerts around that metric and it\'s mostly unique to the message queue workload.\\n\\n\\n##### _[00:20:32] - Ry_\\n\\nYeah. And if the message queue stack is competing against a commercial queue product, they probably have some sort of visualization of the state of each queue.\\n\\n\\n##### _[00:20:45] - Adam_\\n\\nRight?\\n\\n\\n##### _[00:20:46] - Ry_\\n\\nAnd so a postgres backed queue ought to have that same UI, that same monitoring tailored monitoring system. It makes it, I don\'t know how many times harder versus just configuration and curating some extensions, but I think it\'s all worth it to the user to be able to really defend their choice to use postgres for this use case against one of the modern data stack alternatives. \\n\\n\\n##### _[00:21:20] - Adam_\\n\\nRight? Yeah. I think something really useful that I like about having stacks and having postgres aligned to specific workloads, the complexity of an overall application can really come down a lot by running everything on postgres. You could still have separate postgres clusters for workloads, like a certain set of CPU and memory dedicated to mission critical process A and a separate one for this other mission critical process. But still when it comes to troubleshooting these things, you\'re troubleshooting postgres and it\'s not like, hey, I have to switch and be like, okay, now I\'m troubleshooting Kafka, or jumping to Redis or jumping to Mongo or Snowflake. It\'s still like the context switching I think, for the developers is big time minimized when it\'s still the same underlying data store between all the different workloads, same technology. Yeah.\\n\\n\\n##### _[00:22:27] - Ry_\\n\\nAnd we have this vision of potentially having some built in stack connectivity, right, where these databases, if they\'re all kind of sitting back to back to back so you have five different stacks they could and should be able to communicate really well with each other. And you should be able to write queries across them in the same way that Citus allows you to write queries across an array of postgres clusters very efficiently. You should be able to do the same thing here and pull data from multiple stacks with a very nice user experience, again, without having to move data around. So that\'s one of the exciting things for me as a former airflow company founder. All these data pipelines are very painful between modern data stack companies. And one of the things I\'m excited about is the possibility that we can give developers the option to not have to create all those pipelines.\\n\\n\\n##### _[00:23:36] - Adam_\\n\\nYeah, I\'m really excited to work on that problem when we start doing that, but that\'ll be I think, a really big differentiator is to say I have ten different machines running postgres and I have a single pane of view across all of them. I think it\'s definitely doable. It\'ll be challenging.\\n\\n\\n##### _[00:24:02] - Ry_\\n\\nYeah, it\'s a dream we\'re chasing. Yeah. Good. Well, I think it was great chatting with you, Adam. I\'m sure we\'ll have you on the show again. I know that, again, you\'ve got a lot more extensions coming and appreciate the work you\'ve done for the community so far and yeah, looking forward to seeing your future work and talking about it.\\n\\n\\n##### _[00:24:28] - Adam_\\n\\nSounds good. Thanks a lot, Ry."},{"id":"pgvector-and-embedding-solutions-with-postgres","metadata":{"permalink":"/blog/pgvector-and-embedding-solutions-with-postgres","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-10-18-pgvector-and-embeddings/index.md","source":"@site/blog/2023-10-18-pgvector-and-embeddings/index.md","title":"Unleashing the power of vector embeddings with PostgreSQL","description":"Language models are like the wizards of the digital world, conjuring up text that sounds eerily human. These marvels of artificial intelligence, such as GPT-3.5, are sophisticated algorithms that have been trained on vast swathes of text from the internet. They can understand context, generate coherent paragraphs, translate languages, and even assist in tasks like writing, chatbots, and more. Think of them as your trusty digital scribe, ready to assist with their textual sorcery whenever you summon them.","date":"2023-10-18T00:00:00.000Z","formattedDate":"October 18, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"extensions","permalink":"/blog/tags/extensions"},{"label":"embedding","permalink":"/blog/tags/embedding"},{"label":"vector","permalink":"/blog/tags/vector"},{"label":"pgvector","permalink":"/blog/tags/pgvector"}],"readingTime":7.955,"hasTruncateMarker":false,"authors":[{"name":"Binidxaba","title":"Community contributor","url":"https://github.com/binidxaba","email":"noreply@tembo.io","imageURL":"https://github.com/binidxaba.png","key":"rjzv"}],"frontMatter":{"slug":"pgvector-and-embedding-solutions-with-postgres","title":"Unleashing the power of vector embeddings with PostgreSQL","authors":["rjzv"],"tags":["postgres","extensions","embedding","vector","pgvector"],"image":"./RAG.png"},"prevItem":{"title":"Hacking Postgres, Ep. 2: Adam Hendel","permalink":"/blog/hacking-postgres-ep2"},"nextItem":{"title":"Hacking Postgres Ep. 1: Marco Slot","permalink":"/blog/hacking-postgres-ep1"}},"content":"*Language models are like the wizards of the digital world, conjuring up text that sounds eerily human. These marvels of artificial intelligence, such as GPT-3.5, are sophisticated algorithms that have been trained on vast swathes of text from the internet. They can understand context, generate coherent paragraphs, translate languages, and even assist in tasks like writing, chatbots, and more. Think of them as your trusty digital scribe, ready to assist with their textual sorcery whenever you summon them.*\\n\\nIf you have used ChatGPT in the past, you probably were able to suspect that the previous paragraph was generated using it. And that\'s true. See the prompt [here](https://chat.openai.com/share/9fab8ac9-6e34-481d-a281-db2f00b0f7f5).\\n\\nFrom the example above, you can witness the eloquence LLMs are capable of. Some people have been shocked so much that they became convinced that these [models were sentient](https://www.scientificamerican.com/article/google-engineer-claims-ai-chatbot-is-sentient-why-that-matters/). However, in the end, they are nothing but a large, complex series of [matrix and vector operations](https://www.youtube.com/watch?v=bCz4OMemCcA). These matrices and vectors have been trained to represent the semantic meaning of words.\\n\\nIn today\'s post, we will explore these meaning vectors and how they are related to Postgres. In particular, we are going to play with sentence transformers, vectors, and similarity search. All of that with the help of the pgvector Postgres extension.\\n\\nLet\u2019s go!\\n\\n\\n## From words to vectors\\n\\nLike we said, a vector can represent many things, for example, the position of a character in a 3D video game, the position of a pixel in your screen, the force applied to an object, a color in the RGB space, or even the meaning of a word\u2026\\n\\nWord embedding refers to the technique by which words can be represented as vectors. These days, the embeddings offered by [OpenAI](https://openai.com/blog/new-and-improved-embedding-model) are very popular. However, other alternatives exist, like [word2vect](https://arxiv.org/abs/1301.3781), [Glove](https://aclanthology.org/D14-1162.pdf), [FastText](https://fasttext.cc/docs/en/support.html), and [ELMo](https://arxiv.org/abs/1802.05365v2).\\n\\nSimilarly, entire sentences can be represented as vectors using [OpenAI embeddings](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings) or [SentenceTransformers](https://sbert.net/), for example.\\n\\nThese models can be accessed through libraries for different languages. The following Python snippet shows how to obtain the vector embeddings of three sentences using SentenceTransformer:\\n\\n```python\\nfrom sentence_transformers import SentenceTransformer\\n\\nmodel = SentenceTransformer(\'all-MiniLM-L6-v2\')\\nsentences = [\'SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings.\',\\n             \'Pgvector is postgres extension for vector similarity search.\',\\n             \'Tembo will help you say goodby to database sprawl, and hello to Postgres.\']\\n\\nsentence_embeddings = model.encode(sentences)\\n\\nfor sentence, embedding in zip(sentences, sentence_embeddings):\\n    print(\\"Sentence:\\", sentence)\\n    print(\\"Embedding:\\", embedding)\\n    print(\\"\\")\\n```\\n\\n:::note \\nThe code used in this blog post can be found in [this gist](https://gist.github.com/binidxaba/2eb3bff573c6be700e4391d650a302db).\\n:::\\n\\nThe mind-blowing part is that words and sentences with a similar meaning will have similar vectors. This characteristic is the basis of a search technique called similarity search, where we simply find the nearest embedding vectors to find texts that are similar to our query.\\n\\n\\n## Postgres meets Language Models\\n\\nModels are great at generating content that seems credible, as shown earlier. However, you may have experienced cases where ChatGPT hallucinates answers or delivers out-of-date information. That\'s because LLMs are **pre-trained** using **general data**. And, because of that, creating a chatbot based only on the pre-trained data wouldn\'t be helpful for your customers, for instance.\\n\\nThe concept of [RAG (Retrieval-Augmented Generation)](https://arxiv.org/abs/2005.11401) acknowledges this limitation. \\n\\nOne way of overcoming these problems is to store your company\'s knowledge base in a database.... preferably in a vector database. You could then query related content and feed that content to the LLM of your preference.\\n\\n![RAG with pgvector](./RAG.png)\\n\\n\\nSpecialized vector databases include [Milvus](https://milvus.io/), [Qdrant](https://qdrant.tech/), [Weaviate](https://weaviate.io/), and [Pinecone](https://www.pinecone.io/). However, you probably want to [stick to your Postgres database](https://www.amazingcto.com/postgres-for-everything/). \\n\\nPostgres is not in itself a vector database, but extensions can come to the rescue one more time... This time with [**pgvector**](https://github.com/pgvector/pgvector).\\n\\nLet\'s use it and explore how we would query related content from a Postgres database.\\n\\n\\n## pgvector: Postgres as a vector database\\n\\n[pgvector](https://github.com/pgvector/pgvector) is a Postgres extension that helps work with vectors and stores them in your postgres database. It offers functions for calculating the distance between vectors and for similarity search.\\n\\nFor the following demo, I converted all of Tembo\u2019s blogs into document vectors using the following Python script that uses the [langchain framework](https://python.langchain.com).\\n\\n```python\\nfrom langchain.document_loaders import TextLoader\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.vectorstores.pgvector import PGVector\\n\\nimport os\\n\\n\\nCONNECTION_STRING = \\"postgresql+psycopg2://postgres:password@localhost:5432/vector_db\\"\\nCOLLECTION_NAME = \'my_collection\'\\n\\nembeddings = HuggingFaceEmbeddings(model_name=\'all-MiniLM-L6-v2\')\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 20)\\n\\nfiles = os.listdir(\'./corpus\')\\n\\nfor file in files:\\n    file_path = f\\"./corpus/{file}\\"\\n    print(f\\"Loading: {file_path}\\")\\n    loader = TextLoader(file_path)\\n    document = loader.load()\\n    texts = text_splitter.split_documents(document)\\n    sentence_embeddings = embeddings.embed_documents([t.page_content for t in texts[:5]])\\n\\n    db = PGVector.from_documents(\\n            embedding=embeddings,\\n            documents=texts,\\n            collection_name=COLLECTION_NAME,\\n            connection_string=CONNECTION_STRING)\\n```\\n\\nIt basically loads each document and then inserts them into Postgres using the [`PGVector` class](https://python.langchain.com/docs/integrations/vectorstores/pgvector). As a result, in my Postgres database called `vector_db`, I got two tables:\\n\\n![Show tables](./table-list.png)\\n\\n- `langchain_pg_collection`: contains information about all collections.\\n- `langchain_pg_embedding`: contains all the resulting vectors.\\n\\nThe following picture shows part of the contents of (2):\\n\\n![Show vectors](./select-vectors.png)\\n\\nThe resulting vectors have [384 dimensions](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).  \\n\\n\\n## Are these sentences similar?\\n\\nLet\u2019s now play with these vectors.\\n\\nUsing pgvector we can search content that is similar to a query. For example, we can find content related to `postgres 16`.\\n\\nFirst, we can obtain a vector that represents a query:\\n\\n```python\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\nembeddings = HuggingFaceEmbeddings(model_name=\'all-MiniLM-L6-v2\')\\nprint embeddings.embed_query(\u201cWhat is new in postgres 16\\")\\n```\\n\\nThen we can search vectors stored in the database that are similar to the query vector. The tool for that is [`cosine distance`](https://en.wikipedia.org/wiki/Cosine_similarity), which in pgvector is represented with the `<=>` operator:\\n\\n```sql\\nSELECT document, 1-(embedding <=> \'[<your_vector_here>]\') as cosine_similarity\\nFROM langchain_pg_embedding\\nORDER BY cosine_similarity DESC\\nLIMIT 2;\\n```\\n\\nThe above query retrieves vectors/chunks of text ordered by how close they are (in terms of `cosine distance`) to the query vector. In my case, the most similar chunk of text was:\\n\\n> In case you missed it, Postgres 16 came out last week - and this year it\\n> arrived earlier than the last few years. There are many features that\\n> I\u2019ve been looking forward to for the last few months and I\u2019m excited to\\n> see them get into the hands of users. Before we dive into the specific\\n> features of this release, let\u2019s discuss what a Postgres major release\\n> actually means.\\n> \\n> [postgres-16]\\n> \\n> Postgres Releases\\n> \\n> The PostgreSQL Global Development Group releases a new major version\\n> every year with new features.\\n> \\n> In addition, Postgres releases minor versions of each major release\\n> every 3 months or so with bug fixes and security fixes. No new features\\n> are released in minor versions, and that\u2019s what makes major version\\n> releases so exciting as it\u2019s the culmination of about a year\u2019s worth of\\n> development work on the project.\\n\\nWhich is an excerpt from [Postgres 16: The exciting and the unnoticed](https://tembo.io/blog/postgres-16).\\n\\nLet us look at what Postgres is doing behind the scenes, using `explain analyze`:\\n\\n```console\\nLimit  (cost=28.07..28.08 rows=2 width=641) (actual time=1.069..1.071 rows=2 loops=1)\\n   ->  Sort  (cost=28.07..28.53 rows=181 width=641) (actual time=1.067..1.068 rows=2 loops=1)\\n         Sort Key: ((embedding <=> \'[<your_vector>]\'::vector))\\n         Sort Method: top-N heapsort  Memory: 28kB\\n         ->  Seq Scan on langchain_pg_embedding  (cost=0.00..26.26 rows=181 width=641) (actual time=0.036..0.953 rows=181 loops=1)\\n Planning Time: 0.079 ms\\n Execution Time: 1.093 ms\\n(7 rows)\\n\\n\\n```\\n\\nWe can observe that Postgres is sequentially scanning all rows. Then it computes the `cosine distance` for all those rows and sorts them. Finally,  it takes the first two rows.\\n\\nThe `sequential scan` could be avoided if we had an index. Indeed, we can create one thanks to pgvector, for example:\\n\\n```sql\\nalter table langchain_pg_embedding alter column embedding type vector(384);\\n\\nCREATE INDEX ON langchain_pg_embedding  USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\\n```\\n\\n```console\\n Limit  (cost=5.01..5.11 rows=2 width=641) (actual time=0.175..0.179 rows=1 loops=1)\\n   ->  Index Scan using langchain_pg_embedding_embedding_idx2 on langchain_pg_embedding  (cost=5.01..13.49 rows=181 width=641) (actual time=0.172..0.175 rows=1 loops=1)\\n         Order By: (embedding <=> \'[<your_vector>]\'::vector)\\n Planning Time: 0.154 ms\\n Execution Time: 0.224 ms\\n(5 rows)\\n```\\n\\nOne thing to keep in mind is that these indexes are used for `approximate nearest neighbor search`. We\u2019ll explore what that means in a future blog post. [Let us know](https://twitter.com/tembo_io) if that would be interesting for you.\\n\\n\\n## Pgvector(ize)? \\n\\nOk, at this point you should now have a sense of what pgvector is, and how to use it together with Python. However, wouldn\'t it be great if the vectorizing step could happen all within Postgres?\\n\\n[**Pg_vectorize**](https://github.com/tembo-io/pg_vectorize) is an extension being developed by **Tembo** that intends to streamline the process of generating vectors from the data in your Postgres tables. It uses a background worker to generate and update the embeddings in batches every *N* seconds. Also, if you need to find similar vectors, the extension can do that. All within Postgres. Isn\'t that a cool idea? \\n\\nI invite you to check out the repository and stay tuned.\\n\\n\\n## To wrap up...\\n\\nIn this post, we briefly discussed the concept of `embeddings`, why they are important, and how they can be generated using one of the multiple available libraries. We also explored how to store and query the resulting vectors using Postgres and the pgvector extension.\\n\\nThese concepts are relevant to leveraging a knowledge base in conjunction with LLMs in an emerging technique called RAG. Of course, when implementing a real-life solution, [more factors need to be considered](ttps://medium.com/@neum_ai/retrieval-augmented-generation-at-scale-building-a-distributed-system-for-synchronizing-and-eaa29162521), and this post was just an introduction.\\n\\nI invite everyone to try out pgvector (e.g. using the scripts in this post), and the different operations that it offers. Also, can you think of other uses of pgvector? Let us know your thoughts in [@tembo_io](https://twitter.com/tembo_io).\\n\\n## Disclaimer\\n\\n*The first paragraph in this blog post was generated using ChatGPT. [Here\u2019s the prompt](https://chat.openai.com/share/9fab8ac9-6e34-481d-a281-db2f00b0f7f5)*"},{"id":"hacking-postgres-ep1","metadata":{"permalink":"/blog/hacking-postgres-ep1","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-10-16-hacking-postgres-ep1/index.md","source":"@site/blog/2023-10-16-hacking-postgres-ep1/index.md","title":"Hacking Postgres Ep. 1: Marco Slot","description":"Episode Notes","date":"2023-10-16T00:00:00.000Z","formattedDate":"October 16, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"hacking_postgres","permalink":"/blog/tags/hacking-postgres"}],"readingTime":25.03,"hasTruncateMarker":false,"authors":[{"name":"Eric Ankenman","url":"https://github.com/ericankenman","email":"noreply@tembo.io","imageURL":"https://github.com/ericankenman.png","key":"eric"}],"frontMatter":{"slug":"hacking-postgres-ep1","title":"Hacking Postgres Ep. 1: Marco Slot","authors":["eric"],"tags":["postgres","hacking_postgres"]},"prevItem":{"title":"Unleashing the power of vector embeddings with PostgreSQL","permalink":"/blog/pgvector-and-embedding-solutions-with-postgres"},"nextItem":{"title":"Introducing Terraform Provider for Tembo","permalink":"/blog/introducing-terraform-provider-for-tembo"}},"content":"## Episode Notes\\n\\nIn this episode, Ry and Marco talk about the early days of Citus and its development, creating pg_cron (on a plane!), and the new possibilities on the horizon for extensions in the Postgres landscape. If you haven\u2019t seen or listened to it yet, you can play the video below, or listen on Apple/Spotify (or your podcast platform of choice). Special thanks to Marco for joining us today!\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/UxUrn6bKDfU?si=JMm_cMMPToh1K2KK\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n\\nWant to know more about something they mentioned? Here\u2019s a starting point:\\n\\n\\n\\n* CloudFront - https://aws.amazon.com/cloudfront/\\n* Citus - https://www.citusdata.com/\\n* pg_cron - https://github.com/citusdata/pg_cron\\n* pg_timetable - https://github.com/cybertec-postgresql/pg_timetable\\n* pgrx - https://github.com/pgcentralfoundation/pgrx\\n* PostGIS - https://postgis.net/\\n* PostgresML - https://postgresml.org/\\n* pgvector - https://github.com/pgvector/pgvector\\n* pg_embedding - [https://github.com/neondatabase/pg_embedding](https://github.com/neondatabase/pg_embedding)\\n\\nDid you enjoy the episode? Have ideas for someone else we should invite? Let us know your thoughts on X at [@tembo_io](https://twitter.com/tembo_io) or share them with the team in our [Slack Community](https://join.slack.com/t/tembocommunity/shared_invite/zt-23o25qt91-AnZoC1jhLMLubwia4GeNGw).\\n\\n\\n## Transcript\\n\\n\\n##### _[00:00:12] - Ry_\\n\\nWelcome to Hacking Postgres. Today we have Marco Slot joining us. I\'m meeting Marco for the first time today. I\'m super excited to be meeting you, Marco. Welcome to the show.\\n\\n\\n##### _[00:00:25] - Marco_\\n\\nHey Ry. Yeah, very nice to meet you. Yeah. So I\'m Marco, I work at Microsoft and I\'ve been working on Postgres extensions for about nine years now, starting with Citus, pgCron, and a bunch of other ones. Yeah.\\n\\n\\n##### _[00:00:44] - Ry_\\n\\nThat\'s awesome.\\n\\n\\n##### _[00:00:45] - Marco_\\n\\nYeah.\\n\\n\\n##### _[00:00:45] - Ry_\\n\\nSo this is not really supposed to be an interview. It\'s more just us geeking out about Postgres, extensions, what they mean to users and opinions about anything and everything related to our work is fair game. Yeah, sounds good. So what\'d you do before you got into Postgres stuff, out of curiosity?\\n\\n\\n##### _[00:01:09] - Marco_\\n\\nYeah. My background is more in distributed systems, so I spent a few years at AWS. It was quite early days in AWS when I think I joined. There were three Web Services. S3, SQS and EC2.\\n\\n\\n##### _[00:01:24] - Ry_\\n\\nWow.\\n\\n\\n##### _[00:01:25] - Marco_\\n\\nAnd then we built number Four, which was CloudFront, which gives people started using S3 for websites, which it wasn\'t really built for. But then we were asked to solve this and we built the CDN, and then we also built Route 53 for the DNS.\\n\\n\\n##### _[00:01:40] - Ry_\\n\\nWow.\\n\\n\\n##### _[00:01:41] - Marco_\\n\\nAnd then yeah, I did a PhD in sort of self driving cars, but specifically like, cooperation between self driving cars, like, what if they can communicate with each other and sort of coordinate? So it\'s sort of advanced distributed systems in a way as well. Yeah. So that led to me joining Citus, which is sort of founded by former Amazon folks.\\n\\n\\n##### _[00:02:05] - Ry_\\n\\nGot it. So building Citus was probably easy compared to all other stuff, actually.\\n\\n\\n##### _[00:02:12] - Marco_\\n\\nYeah. If you do self driving cooperation between self driving cars, which is this kind of like life critical system distribute where your nodes kind of just move around and they move away from each other, it does make things relatively easy. But then yeah, building databases is never actually very easy.\\n\\n\\n##### _[00:02:31] - Ry_\\n\\nNo, I know. How good do you want to make it right? Is how hard it becomes.\\n\\n\\n##### _[00:02:38] - Marco_\\n\\nYeah. I guess the challenge is always everything sort of in the word relational. Right. Like everything relates to everything else. There\'s not like a feature you can implement without considering all the other aspects of the database and all the things that can happen concurrently with the operation that you\'re working on and all the possible ways in which things can fail. Especially if you\'re in a kind of distributed setup.\\n\\n\\n##### _[00:03:01] - Ry_\\n\\nYeah, I think that\'s really interesting perspective. You think of all of the configuration options, too. Adding a new configuration option adds complexity, adds flexibility, adds potential complex, and it\'s tough.\\n\\n\\n##### _[00:03:20] - Marco_\\n\\nYeah, definitely. I think one thing we learned is there\'s a sort of certain danger in creating modes where every setting you add, which can be on/off now you have two modes in your system. You add another setting that can be on/off. Now you have four modes in your system. It\'s kind of this exponential explosion of possible ways in which your database might be running. And so that\'s hard learned lesson that\'s, like, don\'t introduce major modes. Sometimes you have to because of backwards compatibility reasons, maybe, but you want to get rid of them over time.\\n\\n\\n##### _[00:03:57] - Ry_\\n\\nWere you one of the first team members on Citus? Actually, I don\'t really know the full history. Did it start as a fork and made its way as an extension, or was it always an extension?\\n\\n\\n##### _[00:04:10] - Marco_\\n\\nYeah, I was one of the first, though I think Samay was already there. I think he\'s joined Tembo. So but yeah, we started out as a fork and then we called it CitusDB, and then we made it an extension and called it Citus. Though I think the name CitusDB kind of stuck around for...I still hear it sometimes. But yeah, it was very pretty early days for extensions. Like 2015 PostGIS obviously existed. It was one of the first major ones. But yeah, we were lucky at the time. We had Andres Freund working for us. Well, he\'s also now working for Microsoft, but he\'s like one of the top Postgres committers, and we put him in a room for a while, know, can you turn this into an extension, please? And he came up with some terrible, terrible hacks. But over time, those also drove some of the extension APIs in Postgres itself, where things that used to require really terrible hacks are now kind of more structured and you can introduce your own sort of data structures into Postgres without doing lots of weird stuff.\\n\\n\\n##### _[00:05:22] - Ry_\\n\\nWell, I was thinking probably not everybody knows what Citus is. Maybe if you could give a quick I\'d love to hear a quick overview of what it is and then maybe a couple of things that maybe you can think of that are unusual, that people might not know about it, that are part of it. I don\'t know if that\'s a stupid question, but yeah, if you could try no.\\n\\n\\n##### _[00:05:41] - Marco_\\n\\nYeah, so it comes from basically this denotion that Postgres is ultimately limited to a single machine. I mean, you can read replicas, but you definitely cannot scale the writes. Usually if you have read replicas, they\'ll kind of end up having the same stuff in memory. So you cannot really scale the memory either. And so Citus is a solution to basically adds sharding to Postgres in a very transparent way, where you can have tables that are sort of transparently sharded across many nodes, you still connect to a Postgres server. There\'s just a table, looks, walks, and talks like a table. But if you insert into it, it actually gets rooted to a particular node. And if you insert another value, it might get rooted into another node that uses this hash distribution. And that way you can scale across infinitely many machines and have some users have petabytes of data in their Citus cluster part of the advantage is also that queries can get paralyzed on these tables because we can use all the nodes at the same time and multiple cores per node. But more and more it\'s also actually being used for LTP workloads.\\n\\n\\n##### _[00:06:55] - Marco_\\n\\nAnd particularly the most popular use case is actually multitenancy, where you have this kind of B2B app where you have lots of relatively independent users and you kind of want to just transparently spread those across different machines without having to actually manage that yourself and figure out how to move data around. Because that was all very kind of automated. You can just ask it to rebalance your data and it does that using logical replication. And so then you have the advantage of just as much memory as you need to make your application fast and as much disk IOPS as you need by just adding more machines with more disks and so that helps you scale. And then we have a sort of managed service in Azure around that. But yeah, we use every hook that\'s available in Postgres to achieve things like starting with the planner hook. That\'s one of the interesting things about Postgres. It\'s like you just replace the whole planner with something else. You get a parsed query tree and then you can do whatever you want with it. You can just do some like okay, look at this query and log something interesting and then just go to the regular planner.\\n\\n\\n##### _[00:08:12] - Marco_\\n\\nOr you could just do something completely different. And so we use all these hooks to kind of create this facade around Postgres tables where it\'s like Postgres table is still there, you just cannot touch it. We just intercept everything that\'s going on. The most recent and most terrible hack we did it was like the give it as this hack in Postgres so we can figure out so we have shards. Like the writes go into shards but then if you do logical decoding you see writes on shards, you don\'t see writes on these what we call distributed tables. But the user doesn\'t even know about shards probably. So how can we fix that? And there wasn\'t really any kind of hook we could use to change the way that the logical decoding process works except that we realized that the client specifies a decoder name usually just hard coded like Pgoutput or Wal2json or test_decoding and that actually refers to the name of a .so,  like a shared library. And we realized we could create a .so with the same name and put it in a different directory and then change the dynamic library path to be prefixed with that directory so it would load our library instead.\\n\\n\\n##### _[00:09:34] - Marco_\\n\\nThis is kind of the worst thing we did. But it works nicely actually. I mean, that new .so file calls the original .so file and it kind of makes some changes to hide the sharding from the coding. But yeah, it is a hack. It\'s very much a hack. But it\'s nice that Postgres ultimately lets you do these kind of things.\\n\\n\\n##### _[00:09:59] - Ry_\\n\\nI think, like Citus says, there\'s like a requirement that Citus loads first as an extension. Is that true? I\'m trying to remember if I\'m making that up.\\n\\n\\n##### _[00:10:08] - Marco_\\n\\nIt is true. I mean, maybe it\'s a bit of laziness on our part, but it\'s also.\\n\\n\\n##### _[00:10:13] - Ry_\\n\\nBecause...\\n\\n\\n##### _[00:10:16] - Marco_\\n\\nWe generate very weird plans that wouldn\'t make any sense to other extensions. So if you have multiple layers of planner hooks, you kind of want Citus to be but by putting it first, it actually becomes the last because everyone else overwrites Citus. And then hopefully when you go through the chain of planner hook, Citus is the last one remaining and it can produce some quirky distributed query plan that doesn\'t make sense to anyone else and pick that up.\\n\\n\\n##### _[00:10:48] - Ry_\\n\\nAre there known extensions that aren\'t compatible with Citus that you\'re aware of or as far as you know? It kind of works with most everything.\\n\\n\\n##### _[00:10:56] - Marco_\\n\\nYeah, I mean, Citus is a little bit of a paradigm shift because now you have many servers and some extensions just don\'t make sense if you\'re on many servers because they keep their state on one of them and then they\'re not aware of others. But most things just work. But I kind of feel like there\'s this notion of a deep extension, if you will, like timescaleDB or Citus. They kind of go really deep into planner hooks and change behavioral characteristics. Whereas something like PostGIS and a lot of extensions that just introduce new types and functions. They\'re sitting on top of this much more clean interface and usually the interoperability of those things is pretty good. But Timescale has this notion of a hyper table inside as a distributed table and you cannot really make a distributed hyper table. But yeah, compatibility, it comes down to sometimes also the individual features. It\'s not like if you install one, then the other doesn\'t work anymore. I mean, that happens for some extensions, but for Citus, most other things just kind of work with it. Yeah.\\n\\n\\n##### _[00:12:08] - Ry_\\n\\nSo let\'s talk about pg_Cron. How early were you involved in that project?\\n\\n\\n##### _[00:12:13] - Marco_\\n\\nWell, I created it. I used to have this thing where I am much more productive on flights and I flew to San Francisco a lot for a while and then I had a few of these projects that I was just working on on flights and pg_Cron was one of them. And so at the time we had customers who did kind of this real time analytics scenarios on Postgres on Citus. And those involve a lot of materialization. So a bunch of raw data keeps coming in like time series data. And at some point you want to take all the new data and kind of pre aggregate it inside of the database and that needs to happen periodically. I think the background worker concept was pretty new at the time. I don\'t know. This was many years ago. And so I realized you could do something like Cron. It maybe also comes from the Amazon background because Amazon, at least at the time, was all like glued together with Cron, Perl and R Sync. There were Cron jobs that would R sync metadata files to other servers and it was all pretty hacky. But anyway, I figured it would be pretty useful to have this kind of thing in Postgres also for just vacuuming or calling certain stored procedures that, I don\'t know, delete your old data.\\n\\n\\n##### _[00:13:42] - Marco_\\n\\nLike those kind of maintenance tasks. It was just much easier if that\'s like you run a command once and then it works forever, rather than I need to maintain this separate infrastructure that periodically connects to the database and does that. So that became quite popular. I think pretty much all major managed services have pg_Cron now. Yeah, I kind of built it as a very I mean, it was definitely my side project. So I also built it as a very low maintenance thing that I wouldn\'t have to constantly fix bugs in. Also a little bit selective about adding new features. For a long time I resisted adding people want like, can I run a job every few seconds? And normally Cron is like at the minute, granularity. But recently I caved and I added doing it every few seconds because I realized it\'s kind of feature if there\'s some kind of issue, some kind of memory leak or whatever, if you run it every few seconds, it\'s going to be way worse than if you run it once a minute. So you need to be more careful in that case. But you can now also do like every second, for example, which is kind of useful.\\n\\n\\n##### _[00:14:56] - Ry_\\n\\nThat\'s awesome. Yeah, I was looking at we\'re just building a new data warehouse, trying to use Postgres to do that. I\'m exploring all the columnar extension as well, which sort of is part of Citus. I don\'t know if you did any work on that, but yeah, looking at pg_cron versus pg_timetable, have you had the question of what about have you looked at timetable and its features and yeah, I\'m kind of curious to get your assessment of the two.\\n\\n\\n##### _[00:15:28] - Marco_\\n\\nYeah, I\'ve looked at it a mean for a long time. It wasn\'t so much an extension. I think nowadays it is more or less an autonomous extension, I think. Yeah, I mean, it\'s somewhat more complex in a way. I don\'t feel like competitive of like you should use pg_cron or use pg_timetable. I think pg_cron is just intentionally simple such just this very simple thing that just works and does what you\'d expect it to mean almost unless your time zone is in a different unless you\'re not in GMT, then it sort of doesn\'t quite do what you\'d expect it to anyway. So yeah, I think Cron is just simpler. But if you need some more specific, I guess it comes down to do you need the extra features that pg_timetable adds? But I think for most people, pg_cron is good enough because you can also just I\'ve also seen people do their own kind of job queues on top of pg_cron, where you just have this thing that runs. Every few seconds or every minute, and then looks are there some jobs to do? And it actually executes the stuff that\'s in the job state.\\n\\n\\n##### _[00:16:39] - Marco_\\n\\nSo it\'s kind of composable as well. You can build your own things on top if you want. Yeah.\\n\\n\\n##### _[00:16:45] - Ry_\\n\\nAnd especially if you could trigger it every second to check for work. Yeah, that\'s just fine. Yeah, I\'m coming from my previous company was doing Apache Airflow and so it\'s like kind of getting back to some of the stuff. I mean, obviously that\'s a very complicated system with a lot of capabilities and dag like chaining of tasks and fanning out and all that kind of stuff. But yeah, I think it\'s interesting to try to do some of that stuff inside of Postgres without requiring we\'re trying to go with the mantra of like just use Postgres for everything.\\n\\n\\n##### _[00:17:29] - Marco_\\n\\nIt\'s not a bad mantra. You\'ve got, I guess, different levels of commitment to that idea of how much of our backend can we shove into Postgres? And there\'s these GraphQL extensions where you pretty much put your entire backend in Postgres. Yeah.\\n\\n\\n##### _[00:17:51] - Ry_\\n\\nYour API, is there?\\n\\n\\n##### _[00:17:52] - Marco_\\n\\nYeah, I guess Supabase is going pretty long way in that direction, but at some point you want to debug stuff or you want to have certain rollout procedures. And sometimes Postgres is not as mature a tool as just shipping Python files somewhere. There\'s just better CI tools for most normal programming languages than for Postgres. So you have to find the right balance, I think.\\n\\n\\n##### _[00:18:24] - Ry_\\n\\nYeah, we\'re building and maybe this shouldn\'t have happened, but apparently yesterday we were working on a new FDW and I think it hit an exception that wasn\'t handled and it appeared to shut down the Postgres server. This is kind of fun. Should that have happened? Should that be possible? Should an extension error take it down? But we\'re still investigating that. But yeah, I think there\'s always risk with trying to do a lot inside, but I think with a mature extension that\'s well tested and battle tested, it should be. I mean, if you think about everything that\'s happening, there\'s no SRP in Postgres, right? It\'s not just doing one thing, it\'s got a vacuum. There\'s like lots of processes running and so then the question is, is it a sin to add one more thing? Especially if it has the capability to have a background worker? It\'s just like staring you in the face. Like use me.\\n\\n\\n##### _[00:19:29] - Marco_\\n\\nLike the extension APIs are slowly evolving from just a complete hacky approach for people to try out their Postgres batches before merging them into core. To some, I guess Extensibility was there from the start. But that\'s more too for custom types and custom functions than it is for planner hooks and background workers and those kind of things. Those are a little bit more what if we add this function pointer to see maybe extensions can do something interesting with it. And it\'s not very extremely well structured, but I mean the whole rust like pgrx and building extensions in rust, it does create an opportunity to have a little bit more of a well defined method of developing extensions.\\n\\n\\n##### _[00:20:25] - Ry_\\n\\nYeah, I think what\'s interesting to me about Postgres is like the history of forks and it sucks to be on the true fork, as I\'m sure you were aware. Creating the extension framework helps people out of the land of forks, but it does create the possibility for even more risk extensions in terms of all that. So it\'s like a double edged sword. I think it\'s great though, that to me is the reason why Postgres is doing so well now, is all that freedom that the core team has given the community in terms of how to use it. It\'s unprecedented, almost dangerous, but I also think empowering to developers who want to have some fun with their database.\\n\\n\\n##### _[00:21:21] - Marco_\\n\\nYeah, definitely. What we also often find is just an extension versus an external tool. At least we\'re sort of in the business of running a Postgres managed service. But even if you have just your own Postgres servers, it has a fairly straightforward deployment model, right? Like you don\'t have to create a whole separate cluster with its own configuration that connects to your Postgres database and does things with it. You just shove it into your existing Postgres server and it\'ll restart the background workers and you don\'t have to worry about all those things. It does have a lot of merit, actually, to develop a software as a Postgres extension. I mean, it comes with its own protocol. I think the most interesting thing is just like you get all this synergy between all these different extensions, that the sum of the parts is greater than the or what is it? The whole is greater than the sum of the parts. But also things like PostGIS and Citus, they kind of layer like you can distribute it, geospatial joins, whatever. But then also maybe you have some interesting function and you can run it periodically using pg_cron.\\n\\n\\n##### _[00:22:39] - Marco_\\n\\nAnd it\'s like all these things kind of give you this little platform where you can just by running SQL queries, do increasingly interesting things. Maybe you have an FTW that does an Http call and you run it using pg_cron, for example. And now your Postgres server is not just pg_cron, is not just Postgres cron, it\'s actually anything cron. Like you can reach out to other systems. So that plugability.\\n\\n\\n##### _[00:23:10] - Ry_\\n\\nI literally have a co-op that started last week and his first task was, I said create an FDW for this API and use pg_cron and ingest that data. And he\'s like, well, how do I do it? Just brand new to data warehousing in general, but he\'s got the FDW built and now he\'s working on if he has any problems, I\'ll send him your way. Kidding. I won\'t. Kidding, kidding. I won\'t do that.\\n\\n\\n##### _[00:23:41] - Marco_\\n\\nIf there\'s bugs open issues like I\'d like to know, of course.\\n\\n\\n##### _[00:23:44] - Ry_\\n\\nBut there\'s no bugs. No. So do you have any long flights coming up where you have some new extensions coming?\\n\\n\\n##### _[00:23:51] - Marco_\\n\\nNo, we had a baby last year. It\'s like my flight budget is very low at the moment. That\'s true.\\n\\n\\n##### _[00:23:58] - Ry_\\n\\nYour latest extension is human.\\n\\n\\n##### _[00:24:01] - Marco_\\n\\nYeah, but yeah, I\'d like to play more with Rust because that the problem with extensions in the past. Like developing them in C has always been like c doesn\'t have any good sort of dependency framework. There\'s these old established library like, I don\'t know, LibZ and LibXML or something, like something that was developed 20 years ago and now, okay, every system has it, you can use it, but for anything newer than ten years, it\'s extremely annoying and hard to use a C library. And then even if you can figure out your configure and your make files, the memory allocation is probably going to not play nicely with Postgres. So that\'s where Rust is kind of interesting. Now there\'s this whole ecosystem of open source libraries that can potentially become extensions and we\'re still sort of at the very start of that, what\'s going to happen?\\n\\n\\n##### _[00:25:05] - Ry_\\n\\nIt\'s kind of scary because it could be very much of a Cambrian explosion. What happens if there are 300 new extensions that are worth adding? It\'s sort of a pain for the managed service providers.\\n\\n\\n##### _[00:25:19] - Marco_\\n\\nYeah, definitely. Yeah. And they can have funny incompatibilities. It helps that then Rust is a little bit more safe and a little bit more not managed, but the memory allocation is a bit more sort of safe as well. But then, yeah, you can have a lot of weird bugs if you combine all these unknown bits of code.\\n\\n\\n##### _[00:25:46] - Ry_\\n\\nThat\'s great. Are there any extensions that you\'ve run across either recently or in the past that you love, that you would mention as exciting? I mean, you mentioned PostGIS and anything else that you can think of. It\'s no big deal if not.\\n\\n\\n##### _[00:26:04] - Marco_\\n\\nYeah, there\'s many. I\'m sort of intrigued by Postgres_ML. It\'s like this just machine learning extension that they seem to be doing a lot of interesting things. I don\'t have a good use case for it myself yet, but I really like what you\'re doing. Also in Rust, I think MIT licensed, so it can be used in a lot of places. There\'s of course a lot of buzz around pg_vector and more recently pg_embedding, which is sort of an interesting kind of development because now I think they both added this HNSW indexes which are faster than the IFV flat indexes that pgvector provided. But then it also means they are now incompatible with the latest versions. You can create one or the other, not both, which is a little awkward. But I think that is where, of course, one of the most fastest moving areas and I think some of these things will be figured out.\\n\\n\\n##### _[00:27:06] - Ry_\\n\\nHave you gotten those extensions added to the Citus managed service?\\n\\n\\n##### _[00:27:11] - Marco_\\n\\nYeah, we definitely have pg_vector. Yeah, I mean, I think that took the landscape by storm. I think they\'re pretty much on every managed service now.\\n\\n\\n##### _[00:27:19] - Ry_\\n\\nIt\'s interesting, we\'re working on this thing called Trunk where we\'re trying to maybe I don\'t know if we can get you guys to participate, but the idea would be to send some metadata to some sort of central repository that we\'d know, like which extensions are trending, waning, completely dead or not. I just think it\'s interesting as a new person coming into the Postgres ecosystem, there\'s just this library of potential extensions that it\'s pretty expansive and kind of trails off in a know because you can find them loosely here or there on GitHub. But I think having a better directory of them would be good for the community.\\n\\n\\n##### _[00:28:04] - Marco_\\n\\nYeah, I guess. One thing that\'s also tricky about extensions, it\'s like you\'d want it in an ideal world, you write it once and then it works forever. In the real world, every Postgres version potentially breaks your extension, so someone has to actually go and fix it. For new Postgres versions, sometimes it\'s okay. I think with pg_cron, I had like once or twice. It\'s like I think Postgres 16 didn\'t actually no, it did require some fixes. I think there was one Postgres version which didn\'t require any fixes. Maybe PG 15, I was already happy. But usually C is a bit of this wild west of programming languages. But yeah, sometimes just some function header changes, there\'s an extra argument and then none of the extensions that use that function compile you have to have some level of maintenance behind each extension. Well, not every extension does well. Great.\\n\\n\\n##### _[00:29:03] - Ry_\\n\\nI mean, it was great to chat with you. Happy to have you back on again. If you have a big new release of Citus, I don\'t know, do you guys have any big plans for it or has it reached a certain point of stability where it is what it is? Yeah, I\'m kind of curious.\\n\\n\\n##### _[00:29:19] - Marco_\\n\\nWell, our most recent major release added this kind of notion of Schema based Sharding, where so far it\'s always been like you have distributed tables and you have a distribution column and you need to pick which columns you use. But the Schema based Sharding is just like every Schema becomes its own group, own Shard, essentially, so it might be on a different node. So for apps that use Schema per tenant, that\'s a very nice model. And so we\'re investing more in that at the moment.\\n\\n\\n##### _[00:29:49] - Ry_\\n\\nWell, great. Good to meet you. Looking forward to continuing to get to know you. And thanks for joining us on the show.\\n\\n\\n##### _[00:29:56] - Marco_\\n\\nYeah, it was great. Thanks for having me."},{"id":"introducing-terraform-provider-for-tembo","metadata":{"permalink":"/blog/introducing-terraform-provider-for-tembo","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-10-10-introducing-terraform-provider-for-tembo/index.mdx","source":"@site/blog/2023-10-10-introducing-terraform-provider-for-tembo/index.mdx","title":"Introducing Terraform Provider for Tembo","description":"tembo-terraform-provider","date":"2023-10-10T00:00:00.000Z","formattedDate":"October 10, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"announcement","permalink":"/blog/tags/announcement"},{"label":"terraform","permalink":"/blog/tags/terraform"},{"label":"infrastructure_as_code","permalink":"/blog/tags/infrastructure-as-code"}],"readingTime":2.95,"hasTruncateMarker":false,"authors":[{"name":"Adarsh Shah","url":"https://github.com/shahadarsh","email":"noreply@tembo.io","imageURL":"https://github.com/shahadarsh.png","key":"adarsh"}],"frontMatter":{"slug":"introducing-terraform-provider-for-tembo","title":"Introducing Terraform Provider for Tembo","authors":["adarsh"],"tags":["postgres","announcement","terraform","infrastructure_as_code"],"image":"./images/tembo-terraform-provider.png"},"prevItem":{"title":"Hacking Postgres Ep. 1: Marco Slot","permalink":"/blog/hacking-postgres-ep1"},"nextItem":{"title":"Unlocking value from your Clerk User Management platform with Postgres","permalink":"/blog/clerk-fdw"}},"content":"import ReactPlayer from \\"react-player\\"\\nimport DemoVideoUrl from \\"./videos/tembo-terraform-provider.mov\\"\\n\\n![tembo-terraform-provider](./images/tembo-terraform-provider.png)\\n\\nAt Tembo, we want to empower developers to build fast. That\u2019s why we built the [Terraform Provider for Tembo Cloud](https://registry.terraform.io/providers/tembo-io/tembo), allowing developers to manage Postgres resources using Infrastructure-as-Code (IaC). In this blog, we\'ll explore the Terraform Provider for Tembo and how it can help you streamline database management while keeping teams agile, accountable and enforcing organizational policies.\\n\\nSimpler, faster, safer, and easier? That\u2019s Tembo - but let\u2019s back up and start at the beginning.\\n\\n## What is Tembo Cloud?\\n\\n[Tembo Cloud](https://cloud.tembo.io/) is a developer-first, fully-extensible, fully-managed, secure, and scalable Postgres service. It not only simplifies Postgres management but also provides specialized Postgres deployments through [Tembo Stacks](https://tembo.io/blog/tembo-stacks-intro), catering to various use cases.\\n\\n## Database as Code using Terraform\\n\\nDatabases as code provides a safe, consistent, and repeatable way of managing databases and is a game-changer for developers. This approach allows you to version-control any changes, ensuring auditability, and facilitates efficient workflows such as Pull Request flows and [GitOps](https://about.gitlab.com/topics/gitops/).\\n\\nTerraform is the most popular Infrastructure as Code tool today. It provides a way to define the desired state of your infrastructure using Hashicorp Configuration Language(HCL). Terraform handles all the complexity of making sure that when changes are applied, the actual state matches the desired state. Therefore, it is an efficient way of managing databases as code.\\n\\n## Using Terraform Provider for Tembo\\n\\nWith the Terraform Provider for Tembo, you can manage Postgres databases on Tembo Cloud with all the benefits mentioned in the previous section. You can find the Terraform Provider for Tembo on the [Terraform Registry](https://registry.terraform.io/providers/tembo-io/tembo) along with [the documentation](https://registry.terraform.io/providers/tembo-io/tembo/latest/docs/resources/), and the Github repo is [here](https://github.com/tembo-io/terraform-provider-tembo). A new Postgres instance can be provisioned on Tembo Cloud in about 1 minute and destroying an instance takes just 10 seconds.\\n\\n### Example Terraform file\\n\\nYou can create a new Tembo instance on Tembo Cloud using the `tembo_instance` resource available with the provider. Below is an example configuration.\\n\\n**Note:** Tembo Terraform Provider needs an `access_token` to authenticate with the API. Generate a long-lived API token by following steps [here](https://tembo.io/docs/tembo-cloud/security-and-authentication/api-authentication#create-a-long-lived-api-token).\\n\\n```terraform\\nterraform {\\n  required_providers {\\n    tembo = {\\n      source = \\"tembo-io/tembo\\"\\n      version = \\">= 0.1.0\\"\\n    }\\n  }\\n}\\n\\nprovider \\"tembo\\" {\\n  access_token = var.access_token\\n}\\n\\nvariable \\"access_token\\" {\\n  type = string\\n}\\n\\nresource \\"tembo_instance\\" \\"test_instance\\" {\\n  instance_name = \\"test-instance\\"\\n  org_id        = \\"org_test\\" # Replace this with your Tembo organization id\\n  cpu           = \\"1\\"\\n  stack_type    = \\"Standard\\"\\n  environment   = \\"dev\\"\\n  memory        = \\"4Gi\\"\\n  storage       = \\"10Gi\\"\\n  replicas      = 1\\n  extensions = [{\\n    name        = \\"plperl\\"\\n    description = \\"PL/Perl procedural language\\"\\n    locations = [{\\n      database = \\"app\\"\\n      schema   = \\"public\\"\\n      version  = \\"1.0\\"\\n      enabled  = false\\n      },\\n      {\\n        database = \\"postgres\\"\\n        schema   = \\"public\\"\\n        version  = \\"1.0\\"\\n        enabled  = true\\n    }]\\n  }]\\n}\\n\\noutput \\"instance\\" {\\n  value = tembo_instance.test_instance\\n}\\n```\\n\\nApplying the above Terraform will provision a Postgres Instance on Tembo Cloud. For a demo explaining the steps watch the video below.\\n\\n## Demo\\n\\nThe demo in the video explains how to use the Terraform Provider for Tembo. The Terraform code used in the video can be found [here](https://github.com/tembo-io/terraform-provider-tembo/tree/main/examples/resource-creation).\\n\\n<ReactPlayer controls url={DemoVideoUrl} />\\n\\n## What\u2019s next for the Provider\\n\\nWe are actively working on an Import feature that will allow you to import existing Tembo instances to Terraform to easily start managing existing instances with Terraform. Stay updated by starring our [GitHub repository](https://github.com/tembo-io/terraform-provider-tembo) and monitoring changes on the [Terraform Registry](https://registry.terraform.io/providers/tembo-io/tembo). Most importantly, please try it out, and [file any issues and feature requests in our repository](https://github.com/tembo-io/terraform-provider-tembo/issues). You can also access the documentation for the Tembo provider [here](https://registry.terraform.io/providers/tembo-io/tembo/latest/docs)."},{"id":"clerk-fdw","metadata":{"permalink":"/blog/clerk-fdw","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-10-03-clerk-fdw/index.md","source":"@site/blog/2023-10-03-clerk-fdw/index.md","title":"Unlocking value from your Clerk User Management platform with Postgres","description":"The maxim of building a product is to know your user. However, any company big or small will often have user data spread around in various systems. A data platform team will often deploy various pipelines that can sync data from various sources into a data warehouse. As an alternative, Postgres supports the concept of a Foreign Data Wrapper. Let\u2019s dive into what this is and how it can help us.","date":"2023-10-03T00:00:00.000Z","formattedDate":"October 3, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"extensions","permalink":"/blog/tags/extensions"}],"readingTime":4.47,"hasTruncateMarker":false,"authors":[{"name":"Jay Kothari","title":"Software Engineering Intern","url":"https://github.com/Jayko001","email":"noreply@tembo.io","imageURL":"https://github.com/Jayko001.png","key":"jay"}],"frontMatter":{"slug":"clerk-fdw","title":"Unlocking value from your Clerk User Management platform with Postgres","authors":["jay"],"tags":["postgres","extensions"],"image":"./clerk-flowchart.png"},"prevItem":{"title":"Introducing Terraform Provider for Tembo","permalink":"/blog/introducing-terraform-provider-for-tembo"},"nextItem":{"title":"Version History and Lifecycle Policies for Postgres Tables","permalink":"/blog/table-version-history"}},"content":"The maxim of building a product is to know your user. However, any company big or small will often have user data spread around in various systems. A data platform team will often deploy various pipelines that can sync data from various sources into a data warehouse. As an alternative, Postgres supports the concept of a Foreign Data Wrapper. Let\u2019s dive into what this is and how it can help us.\\n\\nIn this blog, we\'ll look at [clerk_fdw](https://github.com/tembo-io/clerk_fdw)\u2014a tool that bridges the gap between [Clerk](https://clerk.com/), a leading user management solution, and your very own Postgres Database. By the end, you\'ll discover how this integration can empower you to make data-driven decisions, optimize your pricing strategy, and refine your market approach. Let\'s get started!\\n\\n## What\u2019s a Foreign Data Wrapper?\\n\\nA foreign data wrapper is an extension available in PostgreSQL that allows you to bring \u2018foreign data\u2019 (i.e. data in a different Postgres DB, a different database like DB2, or even a different kind of data source, like an API) and query it the same way you would query a normal Postgres table. They are particularly useful when your data may be segregated into different databases, but are still related in ways that you could gather some useful information from them. In building a foreign data wrapper for Clerk.com, we have used [Supabase Wrappers](https://supabase.github.io/wrappers/) that make it easier to build Foreign Data Wrappers and interact with third-party data using SQL.\\n\\nIf you should take something away from this blog, is that Postgres\u2019 Foreign Data Wrappers are a great tool to build an analytics platform based on Postgres. See examples of other [FDWs in Trunk](https://pgt.dev/?cat=connectors)\\n\\n## What\u2019s Clerk?\\n\\n[Clerk](https://clerk.com/) is a user management tool. With Clerk, users experience a seamless sign-up and sign-in flow, whether they prefer using email, SMS, or even their favorite social media accounts. Its versatility and developer-friendly APIs make it an excellent choice for us at Tembo for both efficiency and a superior user experience.\\n\\n## The Power of Integration\\n\\nBeing able to access data from a User Management Tool like Clerk as part of your data platform is especially useful because it enables you to have a 360-degree view of the user experience on your product, without having to set up any complex data export pipelines from Clerk into other systems.\\n\\nIn fact, we built `clerk_fdw` at Tembo to address needs in our internal analytics pipeline . Here are some of the ways we are using it:\\n- Run advanced analytics that combine internal data with user data from Clerk.\\n- Understand user interaction patterns with our product.\\n- Identify and engage with top users.\\n\\n![clerk](clerk-flowchart.png \\"clerk_fdw structure\\")\\n\\n## Setting up `clerk_fdw`\\n\\nThe first step would be installing the `clerk_fdw` extension. You can [install this extension using trunk](https://tembo-io.github.io/trunk/#trunk-install).\\n```bash\\ntrunk install clerk_fdw\\n```\\nThe next step would be to enable the extension in your postgres instance. You can do so using the following command:\\n```sql\\ncreate extension if not exists clerk_fdw;\\n```\\n\\n### Create the foreign data wrapper for clerk\\n```sql\\ncreate foreign data wrapper clerk_wrapper\\n  handler clerk_fdw_handler\\n  validator clerk_fdw_validator;\\n```\\n\\n### Connect to Clerk using your credentials\\n```sql\\ncreate server my_clerk_server\\n  foreign data wrapper clerk_wrapper\\n  options (\\n    api_key \'<clerk secret Key>\');\\n```\\n\\n### Create Foreign Table:\\n#### User table\\nThis table will store information about the users.\\n> Note: The current limit is 500 users. We are working to increase this limitation in future releases.\\n```sql\\ncreate foreign table clerk_users (\\n  user_id text,\\n  first_name text,\\n  last_name text,\\n  email text,\\n  gender text,\\n  created_at bigint,\\n  updated_at bigint,\\n  last_sign_in_at bigint,\\n  phone_numbers bigint,\\n  username text\\n  )\\n  server my_clerk_server\\n  options (\\n      object \'users\'\\n  );\\n```\\n\\n#### Organization Table\\nThis table will store information about the organizations.\\n> Note: The current limit is 500 organizations. We are working to increase this limitation in future releases.\\n```sql\\ncreate foreign table clerk_organizations (\\n  organization_id text,\\n  name text,\\n  slug text,\\n  created_at bigint,\\n  updated_at bigint,\\n  created_by text\\n)\\nserver my_clerk_server\\noptions (\\n  object \'organizations\'\\n);\\n```\\n\\n#### Junction Table\\nThis table connects the `clerk_users` and `clerk_orgs`. It lists out all users and their roles in each organization.\\n```sql\\ncreate foreign table clerk_organization_memberships (\\n  user_id text,\\n  organization_id text,\\n  role text\\n)\\nserver my_clerk_server\\noptions (\\n  object \'organization_memberships\'\\n);\\n```\\n\\n## Dive into the Data\\n\\nNow you can query through your database and get useful information like:\\n- How many organizations have been created each week in the past\\n- How many users have signed up in the past 30 days\\n- What organizations is a user part of\\n- All users and their roles in an organization\\n- And more\u2026.\\n\\nHere are some of the charts we were able to make from the clerk foreign data wrapper using some synthetic data.\\n\\n![chart1](chart1.png \\"Daily New Signups\\")\\n![chart2](chart2.png \\"Total Organizations\\")\\n\\n## Conclusion\\n\\nIn conclusion, we believe that Postgres\u2019 concept of Foreign Data Wrappers is more than just a technical integration\u2014it\'s a game-changer that allows Postgres users to build data warehouse platforms that reach across all data sources in the business. It paves the way for businesses to harness critical insights directly from their operational databases, making informed decisions easier than ever before. See examples of other [FDWs in Trunk](https://pgt.dev/?cat=connectors)\\n\\nGive us a star and try out [clerk_fdw](https://github.com/tembo-io/clerk_fdw) by running the example in the README. If you hit any snags, please create an issue. We would greatly welcome contributions to the project as well."},{"id":"table-version-history","metadata":{"permalink":"/blog/table-version-history","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-09-29-table-version-history/index.md","source":"@site/blog/2023-09-29-table-version-history/index.md","title":"Version History and Lifecycle Policies for Postgres Tables","description":"back-in-time","date":"2023-09-29T00:00:00.000Z","formattedDate":"September 29, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"extensions","permalink":"/blog/tags/extensions"},{"label":"temporal_tables","permalink":"/blog/tags/temporal-tables"},{"label":"pg_partman","permalink":"/blog/tags/pg-partman"},{"label":"trunk","permalink":"/blog/tags/trunk"}],"readingTime":17.85,"hasTruncateMarker":false,"authors":[{"name":"Steven Miller","title":"Founding Engineer","url":"https://github.com/sjmiller609","email":"noreply@tembo.io","imageURL":"https://github.com/sjmiller609.png","key":"steven"}],"frontMatter":{"slug":"table-version-history","title":"Version History and Lifecycle Policies for Postgres Tables","authors":["steven"],"tags":["postgres","extensions","temporal_tables","pg_partman","trunk"],"image":"./images/back-in-time.jpeg"},"prevItem":{"title":"Unlocking value from your Clerk User Management platform with Postgres","permalink":"/blog/clerk-fdw"},"nextItem":{"title":"Anatomy of a Postgres extension written in Rust: pgmq","permalink":"/blog/postgres-extension-in-rust-pgmq"}},"content":"![back-in-time](./images/back-in-time.jpeg)\\n\\nA nice feature of AWS S3 is version history and lifecycle policies. When objects are updated or deleted, the old object version remains in the bucket, but it\u2019s hidden. Old versions are deleted eventually by the lifecycle policy.\\n\\nI would like something like that for my Postgres table data. **We can use the temporal_tables extension for version history, and combine it with pg_partman to partition by time, automatically expiring old versions.**\\n\\n## Data model\\n\\nLet\'s say we have a table **employees**, and it looks like this:\\n```\\n       name       |  salary\\n------------------+----------\\n Bernard Marx     | 10000.00\\n Lenina Crowne    |  7000.00\\n Helmholtz Watson | 18500.00\\n```\\n\\nWe will add one more column to this table, `sys_period`, which is a time range. This time range represents \\"since when\\" is this row the current version. This range is unbounded on the right side, because all the rows in the **employees** table are the present version.\\n\\n```\\n       name       |  salary  |             sys_period\\n------------------+----------+------------------------------------\\n Helmholtz Watson | 18500.00 | [\\"2023-09-28 13:30:19.24318+00\\",)\\n Bernard Marx     | 11600.00 | [\\"2023-09-28 13:33:58.735932+00\\",)\\n Lenina Crowne    | 11601.00 | [\\"2023-09-28 13:33:58.738827+00\\",)\\n```\\n\\nWe will make a new table **employees_history** to store previous versions. This will have the same columns as the **employees** table, but all the rows in `sys_period` are bounded on the the right and the left sides. These ranges represent when this row was the current version. We will configure **temporal_tables** to automatically create these rows when anything changes in the **employees** table.\\n\\n```\\n     name      |  salary  |                            sys_period\\n---------------+----------+-------------------------------------------------------------------\\n Bernard Marx  | 10000.00 | [\\"2023-09-28 13:30:19.18544+00\\",\\"2023-09-28 13:33:58.683279+00\\")\\n Bernard Marx  | 11200.00 | [\\"2023-09-28 13:33:58.683279+00\\",\\"2023-09-28 13:33:58.731332+00\\")\\n Bernard Marx  | 11400.00 | [\\"2023-09-28 13:33:58.731332+00\\",\\"2023-09-28 13:33:58.735932+00\\")\\n Lenina Crowne |  7000.00 | [\\"2023-09-28 13:30:19.239152+00\\",\\"2023-09-28 13:33:58.738827+00\\")\\n```\\n\\nTo automatically delete old versions, we\'ll add one more column to the **employees_table**, `created_at`. We will use this information to expire old versions after they are older than our retenion configuration, with the help of **pg_partman**.\\n\\n## Getting set up\\n\\n[This guide](https://tembo.io/docs/tembo-cloud/try-extensions-locally) covers how to quickly try out Postgres extensions locally. I\'ve followed that guide to set up my environment with **temporal_tables** and **pg_partman**.\\n\\nI have a Dockefile, two SQL scripts, and a file with Postgres configurations.\\n\\n```\\n.\\n\u251c\u2500\u2500 Dockerfile\\n\u251c\u2500\u2500 0_startup.sql\\n\u251c\u2500\u2500 1_create_versioned_table.sql\\n\u2514\u2500\u2500 custom.conf\\n```\\n\\n**Dockerfile:** We use [Trunk](https://pgt.dev) to install pg_partman and temporal_tables. Then, we copy the three other files into the image.\\n\\n```Dockerfile\\nFROM quay.io/tembo/tembo-local:latest\\n\\nRUN trunk install pg_partman\\nRUN trunk install temporal_tables\\n\\nCOPY 0_startup.sql $PGDATA/startup-scripts\\n\\nCOPY 1_create_versioned_table.sql $PGDATA/startup-scripts\\n\\nCOPY custom.conf $PGDATA/extra-configs\\n```\\n\\n**0_startup.sql:** Enables temporal_tables and pg_partman when Postgres starts.\\n\\n```sql\\nCREATE EXTENSION IF NOT EXISTS temporal_tables;\\nCREATE EXTENSION IF NOT EXISTS pg_partman;\\n```\\n\\n**1_create_versioned_table.sql:** Creates a sample table, then enables version history on it.\\n```sql\\n-- Sample: an existing table we want to enable versioning on\\nCREATE TABLE employees\\n(\\n  name text NOT NULL PRIMARY KEY,\\n  department text,\\n  salary numeric(20, 2)\\n);\\n\\n/*\\nAdding version history to the table,\\nfirst we need to add a time range to the existing table.\\nThis represents \\"since when\\" has this row been current.\\n*/\\nALTER TABLE employees ADD COLUMN sys_period tstzrange NOT NULL;\\n\\n/*\\nCreating a time-partitioned version table\\neach row has the range the data was valid for,\\nand also the time this version was created.\\n*/\\nCREATE TABLE employees_history (\\n    LIKE employees INCLUDING DEFAULTS EXCLUDING INDEXES EXCLUDING CONSTRAINTS,\\n    created_at timestamptz NOT NULL DEFAULT now())\\n    PARTITION BY RANGE (created_at);\\n\\n-- Allow efficient querying of partition key and name\\nCREATE INDEX ON employees_history (created_at);\\n\\n/*\\nEnable automatic partitioning with pg_partman, partitioning every 1 minute.\\n\\nIt\'s more realistic to partition daily or greater.\\n*/\\nSELECT create_parent(\'public.employees_history\', \'created_at\', \'native\', \'1 minute\');\\n\\n-- This connects employees table to employees_history\\nCREATE TRIGGER versioning_trigger\\n    BEFORE INSERT OR UPDATE OR DELETE ON employees\\n    FOR EACH ROW EXECUTE PROCEDURE versioning(\'sys_period\',\\n                                              \'employees_history\',\\n                                              true);\\n\\n/*\\nConfigure retention policy for employee history to keep old versions for 10 minutes.\\n\\nIt\'s more realistic to configure retention for 1 year.\\n*/\\nUPDATE part_config\\n    SET retention = \'10 minutes\',\\n        retention_keep_table = false,\\n        retention_keep_index = false,\\n        infinite_time_partitions = true\\n    WHERE parent_table = \'public.employees_history\';\\n\\n```\\n\\n**custom.conf:** our additions to the Postgres configuration.\\n\\n```\\n# Enable pg_partman background worker\\nshared_preload_libraries = \'pg_partman_bgw\'\\n\\n# How many seconds between pg_partman background worker runs\\n# It\'s more realistic to run every 3600 seconds, or longer\\npg_partman_bgw.interval = 10\\n\\n# Which database pg_partman should target\\npg_partman_bgw.dbname = \'postgres\'\\n\\n# It\'s best practice to use limited permissions for the background worker\\n# pg_partman_bgw.role = \'limitedrole\'\\n\\n# This was helpful when I was working on getting the settings working\\n# log_min_messages = \'DEBUG1\'\\n```\\n\\nWith those four files in place, we can run Postgres like this:\\n\\n```bash\\ndocker build -t example-local-image .\\ndocker run -it -d --name local-tembo -p 5432:5432 --rm example-local-image\\n```\\n\\nIn a separate shell, I connect into the Postgres container.\\n\\n```bash\\npsql postgres://postgres:postgres@localhost:5432\\n```\\n\\n## Basic demo of saving old versions\\n\\nAfter we are set up, we have version history and retention policy configured on the **employees** table, but both the **employees** table and the **employees_history** table are empty.\\n\\n```sql\\nSELECT * FROM employees;\\n```\\n```\\n name | department | salary | sys_period\\n------+------------+--------+------------\\n(0 rows)\\n```\\n```sql\\nSELECT * FROM employees_history;\\n```\\n```\\n name | department | salary | sys_period | created_at\\n------+------------+--------+------------+------------\\n(0 rows)\\n```\\n\\nAdding data:\\n\\n```sql\\nINSERT INTO employees (name, department, salary)\\nVALUES (\'Bernard Marx\', \'Hatchery and Conditioning Centre\', 10000);\\n\\nINSERT INTO employees (name, department, salary)\\nVALUES (\'Lenina Crowne\', \'Hatchery and Conditioning Centre\', 7000);\\n\\nINSERT INTO employees (name, department, salary)\\nVALUES (\'Helmholtz Watson\', \'College of Emotional Engineering\', 18500);\\n```\\n\\nNow, the **employees** has some data, and **employees_history** is still empty.\\n```sql\\nSELECT name, salary, sys_period FROM employees;\\n```\\n```\\n       name       |   salary  |             sys_period\\n------------------+-----------+------------------------------------\\n Bernard Marx     |  10000.00 | [\\"2023-09-28 20:23:14.840624+00\\",)\\n Lenina Crowne    |   7000.00 | [\\"2023-09-28 20:23:14.911528+00\\",)\\n Helmholtz Watson |  18500.00 | [\\"2023-09-28 20:23:14.913555+00\\",)\\n(3 rows)\\n```\\n```sql\\nSELECT * FROM employees_history;\\n```\\n```\\n name | department | salary | sys_period | created_at\\n------+------------+--------+------------+------------\\n(0 rows)\\n```\\n\\nModifying data:\\n\\n```sql\\nUPDATE employees SET salary = 11200 WHERE name = \'Bernard Marx\';\\nUPDATE employees SET salary = 11400 WHERE name = \'Bernard Marx\';\\nUPDATE employees SET salary = 11600 WHERE name = \'Bernard Marx\';\\nUPDATE employees SET salary = 11601 WHERE name = \'Lenina Crowne\';\\n```\\n\\nNow, the **employees_history** table has past versions.\\n\\n```sql\\nSELECT name, salary, sys_period FROM employees;\\n```\\n```\\n       name       |  salary  |             sys_period\\n------------------+----------+------------------------------------\\n Helmholtz Watson | 18500.00 | [\\"2023-09-28 20:23:14.913555+00\\",)\\n Bernard Marx     | 11600.00 | [\\"2023-09-28 20:23:50.731597+00\\",)\\n Lenina Crowne    | 11601.00 | [\\"2023-09-28 20:23:50.734214+00\\",)\\n(3 rows)\\n```\\n```sql\\nSELECT name, salary, sys_period FROM employees_history;\\n```\\n```\\n     name      |  salary  |                            sys_period\\n---------------+----------+-------------------------------------------------------------------\\n Bernard Marx  | 10000.00 | [\\"2023-09-28 20:23:14.840624+00\\",\\"2023-09-28 20:23:50.684293+00\\")\\n Bernard Marx  | 11200.00 | [\\"2023-09-28 20:23:50.684293+00\\",\\"2023-09-28 20:23:50.727283+00\\")\\n Bernard Marx  | 11400.00 | [\\"2023-09-28 20:23:50.727283+00\\",\\"2023-09-28 20:23:50.731597+00\\")\\n Lenina Crowne |  7000.00 | [\\"2023-09-28 20:23:14.911528+00\\",\\"2023-09-28 20:23:50.734214+00\\")\\n(4 rows)\\n```\\n\\n## Looking up past versions\\n\\nLet\'s say we want to look up Bernard\'s salary at a previous date. We can check the **employees_history** table to find the row where the time range matches our provided timestamp. However, this wouldn\'t find the correct salary if we provide a timestamp that is after the most recent update to Bernard\'s salary, since that row is in the **employees** table.\\n\\nWe can first create a [view](https://www.postgresql.org/docs/current/tutorial-views.html) for this purpose. We only need to do this once, then we can query this view like a table going forward.\\n\\n```sql\\nCREATE VIEW employee_history_view AS\\n\\nSELECT name, department, salary, sys_period\\nFROM employees\\n\\nUNION ALL\\n\\nSELECT name, department, salary, sys_period\\nFROM employees_history;\\n```\\n\\nThen, we can use this query to find Bernard\'s salary at any given date.\\n\\n```sql\\nSELECT salary\\nFROM employee_history_view\\nWHERE name = \'Bernard Marx\'\\nAND sys_period @> TIMESTAMP WITH TIME ZONE \'2023-09-28 20:23:30+00\'\\nLIMIT 1;\\n```\\n\\n`@>` Is a *containment operator* and you might recognize it if you have used [JSONB](https://tembo.io/docs/postgres_guides/postgres-basics/jsonb).\\n\\nComparing to the **employees_history** table shown above, it is returning the correct value.\\n\\n```\\n  salary\\n----------\\n 10000.00\\n(1 row)\\n```\\n\\nIt also works to look up the current salary:\\n```sql\\nSELECT salary\\nFROM employee_history_view\\nWHERE name = \'Bernard Marx\'\\nAND sys_period @> now()::TIMESTAMP WITH TIME ZONE\\nLIMIT 1;\\n```\\n```\\n  salary\\n----------\\n 11600.00\\n(1 row)\\n```\\n```sql\\nSELECT salary FROM employees WHERE name = \'Bernard Marx\';\\n```\\n```\\n  salary\\n----------\\n 11600.00\\n(1 row)\\n```\\n\\nIf I try to query a salary from the future, it will return the current salary. If I try to query a salary from before Bernard is known in the **employees_history** table, then I get an empty result.\\n\\n## Partitioning\\n\\n**What is partitioning?** [Postgres documentation](https://www.postgresql.org/docs/current/ddl-partitioning.html) has detailed information on partitioning but just to summarize, partitioning is about splitting what is logically one large table into smaller tables. Typically, this is done for query performance. In our case, **we are partitioning to expire old versions.**\\n\\nPartitioning tables is something I\u2019m familiar with from Tembo\u2019s work in [PGMQ](https://github.com/tembo-io/pgmq), which is a queueing extension for Postgres.\\n\\n## Performance\\n\\n### Writes\\n\\nWe should expect write performance to be slower, since we are writing to two tables for every update.\\n\\nI created a new table that does not have versioning enabled to compare write performance.\\n```sql\\n-- Create a table like employees\\nCREATE TABLE employees_write_test\\nAS TABLE employees\\nWITH NO DATA;\\n\\n-- ...and insert one row\\nINSERT INTO employees_write_test (name, department, salary, sys_period)\\nVALUES (\'Bernard Marx\', \'Hatchery and Conditioning Centre\', 11600.00, tstzrange(now(), null));\\n```\\n\\nThen, I used `EXPLAIN ANALYZE` to compare the write performance. I ran the query a few times for each.\\n\\n**Without versioning:**\\n```sql\\nEXPLAIN ANALYZE\\nUPDATE employees_write_test\\nSET salary = 11608 WHERE name = \'Bernard Marx\';\\n```\\nThree samples:\\n```\\n Planning Time: 1.654 ms\\n Execution Time: 1.540 ms\\n\\n Planning Time: 0.760 ms\\n Execution Time: 0.707 ms\\n\\n Planning Time: 1.707 ms\\n Execution Time: 2.079 ms\\n```\\n\\n**With versioning:**\\n```sql\\nEXPLAIN ANALYZE\\nUPDATE employees\\nSET salary = 11610 WHERE name = \'Bernard Marx\';\\n```\\nThree samples:\\n```\\n Planning Time: 2.423 ms\\n Trigger versioning_trigger: time=2.430 calls=1\\n Execution Time: 4.783 ms\\n\\n Planning Time: 2.311 ms\\n Trigger versioning_trigger: time=1.091 calls=1\\n Execution Time: 2.979 ms\\n\\n Planning Time: 2.825 ms\\n Trigger versioning_trigger: time=1.711 calls=1\\n Execution Time: 5.686 ms\\n```\\n\\nIt\'s more than twice as slow on a single update. That\'s because we have to write to two rows instead of one, there is more data to write (the time ranges), and because there is some additional processing, for instance determining which range to put on each row. In the next section, I also compare how much time it takes to write 100,000 rows in each of these tables.\\n\\n### Reads\\n\\nWe created a view which is a union between **employees** and **employees_history**, then we query the view to find an employee\'s salary at a given time.\\n\\nTo generate some data, let\'s make a procedure to update a salary 100,000 times in a row. The below example uses [PL/pgSQL](https://www.postgresql.org/docs/current/plpgsql.html). By default, PL/pgSQL functions run as a single transaction, so it would only result in a single update to the **employees_history** table. For this reason, I am using a procedure with `COMMIT` so that each increment will be a separate transaction, this way we also get 100,000 updates to the **employees_history** table. I had to explain that nuance to chatGPT in order for this procedure to be produced properly.\\n\\n```sql\\n-- Table name and employee name as inputs\\nCREATE OR REPLACE PROCEDURE increment_salary(p_name text, p_table_name text)\\nLANGUAGE plpgsql AS $$\\nDECLARE\\n    v_salary numeric(20,2);\\n    i integer;\\n    v_sql text;\\nBEGIN\\n    -- Dynamically construct the SQL to get the current salary\\n    v_sql := format(\'SELECT salary FROM %I WHERE name = $1\', p_table_name);\\n    EXECUTE v_sql INTO v_salary USING p_name;\\n\\n    -- Loop 100 thousand times\\n    FOR i IN 1..100000\\n    LOOP\\n        -- Increment the salary\\n        v_salary := v_salary + 1;\\n\\n        -- Dynamically construct the SQL to update the salary\\n        v_sql := format(\'UPDATE %I SET salary = $2 WHERE name = $1\', p_table_name);\\n        EXECUTE v_sql USING p_name, v_salary;\\n\\n        COMMIT;  -- Commit the transaction, triggering the versioning procedure\\n    END LOOP;\\nEND\\n$$;\\n```\\n\\nRun the procedure:\\n```sql\\nCALL increment_salary(\'Bernard Marx\', \'employees\');\\n```\\n\\nThis took 55 seconds to run on my laptop. I also tried it on the table without versioning enabled, at in this case it took 38 seconds. I ran it a couple more times on the table with versioning enabled, so that the versions would be distributed across multiple partitions. Now we have an **employees_history** table that\'s populated with many rows for Bernard.\\n\\n```sql\\nSELECT count(*) FROM employees_history WHERE name = \'Bernard Marx\';\\n```\\n```\\n count\\n--------\\n 300000\\n(1 row)\\n```\\n\\nLet\'s run the same type of query command we ran before, with `EXPLAIN ANALYZE`. I picked a timestamp that will not be found to ensure it\'s as slow as possible.\\n\\n```sql\\nEXPLAIN ANALYZE\\nSELECT salary\\nFROM employee_history_view\\nWHERE name = \'Bernard Marx\'\\nAND sys_period @> TIMESTAMP WITH TIME ZONE \'2023-09-28 15:28:25+00\'\\nLIMIT 1;\\n```\\n\\nSimplified query plan output:\\n```\\nLimit\\n  ->  Append\\n        ->  Bitmap Heap Scan on employees\\n              Recheck Cond: (name = \'Bernard Marx\'::text)\\n              Filter: (sys_period @> \'...\')\\n              Rows Removed by Filter: 1\\n              Heap Blocks: exact=1\\n              ->  Bitmap Index Scan on employees_pkey\\n                    Index Cond: (name = \'Bernard Marx\'::text)\\n\\n        ... Empty partitions omitted ...\\n\\n        ->  Seq Scan on employees_history_p2023_09_29_0030\\n              Filter: ((sys_period @> \'...\') AND (name = \'Bernard Marx\'::text))\\n              Rows Removed by Filter: 31\\n\\n        ->  Seq Scan on employees_history_p2023_09_29_0031\\n              Filter: ((sys_period @> \'...\') AND (name = \'Bernard Marx\'::text))\\n              Rows Removed by Filter: 99969\\n\\n        ... Empty partitions omitted ...\\n\\n        ->  Seq Scan on employees_history_p2023_09_29_0035\\n              Filter: ((sys_period @> \'...\') AND (name = \'Bernard Marx\'::text))\\n              Rows Removed by Filter: 97393\\n\\n        ->  Seq Scan on employees_history_p2023_09_29_0036\\n              Filter: ((sys_period @> \'...\') AND (name = \'Bernard Marx\'::text))\\n              Rows Removed by Filter: 102607\\n\\n        ... Empty partitions omitted ...\\n\\nPlanning Time: 12.427 ms\\nExecution Time: 262.706 ms\\n(47 rows)\\n```\\nThis query took 263 milliseconds. We notice this query needs to scan all partitions, because we are partitioning by `created_at`, and querying `sys_period`. We can improve the speed with indexes.\\n\\nIf this was a real workload, I doubt that employees\' salaries are being updated so frequently, or at least that\'s been the case in my personal experience. However, if it\'s a big company, then there could be a lot of employees. In that case, it would be best to add an index on the name (or more realistically, employee ID) in the **employees_history** table. Then, withing each partition it will find only rows for the employee being queryed using the index, then it would scan the remaining rows, probably typically zero, one, or two rows, to find the correct salary.\\n\\n## Expiring old versions\\n\\nEarlier in this blog, we configured **pg_partman** to partition in 1 minute increments, to expire partitions that are older than 15 minutes, and to check every 30 seconds. Every 30 seconds, any partition that is older that 15 minutes is deleted by the **pg_partman** background worker.\\n\\nWith this query, I can check how many rows and the total data size in each partition.\\n```sql\\n-- This query was produced by ChatGPT 4 with the prompt:\\n-- \\"How can I check the number of rows in each partition of employees_history?\\"\\nSELECT\\n    child.relname AS partition_name,\\n    pg_total_relation_size(child.oid) AS total_size,\\n    pg_relation_size(child.oid) AS data_size,\\n    pg_stat_user_tables.n_live_tup AS row_count\\nFROM\\n    pg_inherits\\nJOIN\\n    pg_class parent ON pg_inherits.inhparent = parent.oid\\nJOIN\\n    pg_class child ON pg_inherits.inhrelid = child.oid\\nLEFT JOIN\\n    pg_stat_user_tables ON child.oid = pg_stat_user_tables.relid\\nWHERE\\n    parent.relname=\'employees_history\'\\nORDER BY\\n    partition_name;\\n```\\n\\nIn order to check that old versions are being dropped, I ran the procedure to create a lot of salaray increments several times in a row.\\n\\nThen, running the above query, I find an output like this:\\n\\n\\n```\\n           partition_name           | total_size | data_size | row_count\\n------------------------------------+------------+-----------+-----------\\n employees_history_default          |      16384 |         0 |         0\\n employees_history_p2023_09_28_2204 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2205 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2206 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2207 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2208 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2209 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2210 |      32768 |      8192 |         4\\n employees_history_p2023_09_28_2211 |    9584640 |   7995392 |     68267\\n employees_history_p2023_09_28_2212 |    4489216 |   3719168 |     31733\\n employees_history_p2023_09_28_2213 |   13180928 |  11018240 |     94144\\n employees_history_p2023_09_28_2214 |     868352 |    688128 |      5856\\n employees_history_p2023_09_28_2215 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2216 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2217 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2218 |      16384 |         0 |         0\\n(16 rows)\\n```\\nIn this output, we can see that we have 1 partition for every minute, and a total of 15 partitions. I have old versions expiring after 10 minutes. I thought it\'s interesting to note that **pg_partman** is preemptively creating partitions for the future, in this case 5 minutes into the future.\\n\\nIf you refer to the original set up steps, I have configured `infinite_time_partitions = true`, and this means we will generate partitions even when we are not generating any data for them. I think this is the proper configuration since we also have a retention policy that will drop the old partitions. The concern of making infinite partitions as time passes, even if no data is being generated, is not applicable because old tables are being dropped.\\n\\nTo confirm data was being deleted, I sampled the above query over time, and we can see the large body of inserts moving up into the oldest available partitions, then falling outside of the retention policy and being deleted.\\n\\n```\\n\\n           partition_name           | total_size | data_size | row_count\\n------------------------------------+------------+-----------+-----------\\n employees_history_default          |      16384 |         0 |         0\\n employees_history_p2023_09_28_2207 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2208 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2209 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2210 |      32768 |      8192 |         4\\n employees_history_p2023_09_28_2211 |    9584640 |   7995392 |     68267\\n employees_history_p2023_09_28_2212 |    4489216 |   3719168 |     31733\\n employees_history_p2023_09_28_2213 |   13189120 |  11018240 |     94144\\n employees_history_p2023_09_28_2214 |     876544 |    688128 |      5856\\n employees_history_p2023_09_28_2215 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2216 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2217 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2218 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2219 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2220 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2221 |      16384 |         0 |         0\\n(16 rows)\\n\\n\\n           partition_name           | total_size | data_size | row_count\\n------------------------------------+------------+-----------+-----------\\n employees_history_default          |      16384 |         0 |         0\\n employees_history_p2023_09_28_2211 |    9584640 |   7995392 |     68267\\n employees_history_p2023_09_28_2212 |    4489216 |   3719168 |     31733\\n employees_history_p2023_09_28_2213 |   13189120 |  11018240 |     94144\\n employees_history_p2023_09_28_2214 |     876544 |    688128 |      5856\\n employees_history_p2023_09_28_2215 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2216 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2217 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2218 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2219 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2220 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2221 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2222 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2223 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2224 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2225 |      16384 |         0 |         0\\n(16 rows)\\n\\n\\n           partition_name           | total_size | data_size | row_count\\n------------------------------------+------------+-----------+-----------\\n employees_history_default          |      16384 |         0 |         0\\n employees_history_p2023_09_28_2212 |    4489216 |   3719168 |     31733\\n employees_history_p2023_09_28_2213 |   13189120 |  11018240 |     94144\\n employees_history_p2023_09_28_2214 |     876544 |    688128 |      5856\\n employees_history_p2023_09_28_2215 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2216 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2217 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2218 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2219 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2220 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2221 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2222 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2223 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2224 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2225 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2226 |      16384 |         0 |         0\\n(16 rows)\\n\\npostgres=# select count(*) from employees_history;\\n count\\n--------\\n 131733\\n(1 row)\\n\\n           partition_name           | total_size | data_size | row_count\\n------------------------------------+------------+-----------+-----------\\n employees_history_default          |      16384 |         0 |         0\\n employees_history_p2023_09_28_2214 |     876544 |    688128 |      5856\\n employees_history_p2023_09_28_2215 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2216 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2217 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2218 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2219 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2220 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2221 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2222 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2223 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2224 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2225 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2226 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2227 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2228 |      16384 |         0 |         0\\n(16 rows)\\n\\npostgres=# select count(*) from employees_history;\\n count\\n-------\\n  5856\\n(1 row)\\n\\n           partition_name           | total_size | data_size | row_count\\n------------------------------------+------------+-----------+-----------\\n employees_history_default          |      16384 |         0 |         0\\n employees_history_p2023_09_28_2215 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2216 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2217 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2218 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2219 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2220 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2221 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2222 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2223 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2224 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2225 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2226 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2227 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2228 |      16384 |         0 |         0\\n employees_history_p2023_09_28_2229 |      16384 |         0 |         0\\n(16 rows)\\n\\npostgres=# select count(*) from employees_history;\\n count\\n-------\\n     0\\n(1 row)\\n```\\n\\n## Thanks!\\n\\nIf you got this far, thank you for reading this! I hope that you are inspired to try out extensions on your own and see what they can do. The next time you have some problem to solve with your data, consider that maybe it could just be handled by a Postgres extension.\\n\\nIf you want to try extensions without any local setup, you should try Tembo Cloud at [cloud.tembo.io](https://cloud.tembo.io).\\n\\nJust use Postgres!"},{"id":"postgres-extension-in-rust-pgmq","metadata":{"permalink":"/blog/postgres-extension-in-rust-pgmq","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-09-28-pgmq-internals/index.md","source":"@site/blog/2023-09-28-pgmq-internals/index.md","title":"Anatomy of a Postgres extension written in Rust: pgmq","description":"In my previous submission to this space, I described my experience with pgmq while using the Python library. In this post, I\'ll share what I found after inspecting the code.","date":"2023-09-28T00:00:00.000Z","formattedDate":"September 28, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"pgmq","permalink":"/blog/tags/pgmq"},{"label":"rust","permalink":"/blog/tags/rust"},{"label":"pgrx","permalink":"/blog/tags/pgrx"},{"label":"extensions","permalink":"/blog/tags/extensions"}],"readingTime":11.11,"hasTruncateMarker":false,"authors":[{"name":"Binidxaba","title":"Community contributor","url":"https://github.com/binidxaba","email":"noreply@tembo.io","imageURL":"https://github.com/binidxaba.png","key":"rjzv"}],"frontMatter":{"slug":"postgres-extension-in-rust-pgmq","title":"Anatomy of a Postgres extension written in Rust: pgmq","authors":["rjzv"],"tags":["postgres","pgmq","rust","pgrx","extensions"],"image":"./pgmq-archive.png"},"prevItem":{"title":"Version History and Lifecycle Policies for Postgres Tables","permalink":"/blog/table-version-history"},"nextItem":{"title":"Postgres 16: The exciting and the unnoticed","permalink":"/blog/postgres-16"}},"content":"In my [previous submission](https://tembo.io/blog/pgmq-with-python) to this space, I described my experience with [pgmq](https://github.com/tembo-io/pgmq) while using the Python library. In this post, I\'ll share what I found after inspecting the code.\\n\\nSo, first, I\'ll describe the general structure of the project. Then, I\'ll explain what happens when we install the pgmq extension. Finally, I\'ll describe how some of its functions work.\\n\\nIn this post, I\'ll be using version v0.25.0, which you can find [here](https://github.com/tembo-io/pgmq/releases/tag/v0.25.0).\\n\\n\\n## Project structure\\nAfter cloning the appropriate tag, we can see that the repository contains the following files:\\n\\n```bash\\n$ ls -1\\nCargo.lock\\nCargo.toml\\nCONTRIBUTING.md\\ncore\\nDockerfile.build\\nexamples\\nimages\\nLICENSE\\nMakefile\\npgmq.control\\npgmq-rs\\nREADME.md\\nsql\\nsrc\\ntembo-pgmq-python\\ntests\\n```\\n\\nThe project uses [pgrx](https://github.com/pgcentralfoundation/pgrx). From pgrx\'s [README](https://github.com/pgcentralfoundation/pgrx/blob/develop/README.md), we know that the relevant files for the extension are `Cargo.toml`, `pgmq.control` and the `src` and `sql` directories:\\n\\n```bash\\n$ tree sql src\\nsql\\n\u251c\u2500\u2500 pgmq--0.10.2--0.11.1.sql\\n\u251c\u2500\u2500 pgmq--0.11.1--0.11.2.sql\\n...\\n\u251c\u2500\u2500 pgmq--0.8.0--0.8.1.sql\\n\u251c\u2500\u2500 pgmq--0.8.1--0.9.0.sql\\n\u2514\u2500\u2500 pgmq--0.9.0--0.10.2.sql\\nsrc\\n\u251c\u2500\u2500 api.rs\\n\u251c\u2500\u2500 errors.rs\\n\u251c\u2500\u2500 lib.rs\\n\u251c\u2500\u2500 metrics.rs\\n\u251c\u2500\u2500 partition.rs\\n\u251c\u2500\u2500 sql_src.sql\\n\u2514\u2500\u2500 util.rs\\n\\n0 directories, 7 files\\n```\\n\\n## Installing the pgmq extension\\n\\n:::note \\nThis section assumes that you have successfully installed the pre-requisites as described in [CONTRIBUTING.md](https://github.com/tembo-io/pgmq/blob/main/CONTRIBUTING.md)\\n:::\\n\\nTo build the pgmq extension, we can do the following:\\n\\n```bash\\ncargo build\\n```\\n\\nAlternatively, to build and install the pgmq extension, we can do:\\n\\n```bash\\ncargo pgrx install\\n```\\n\\nIn either case, we can see a shared library `pgmq.so` being created. The installation process also places the shared library in the `lib` directory of the postgres installation; and the sql files and the control file in the `extensions` directory. In my case:\\n\\n```\\n$ ls -1 /opt/postgres/share/extension/pgmq*\\n/opt/postgres/share/extension/pgmq--0.10.2--0.11.1.sql\\n...\\n/opt/postgres/share/extension/pgmq--0.9.0--0.10.2.sql\\n/opt/postgres/share/extension/pgmq.control\\n\\n$ ls -1 /opt/postgres/lib/pgmq*\\n/opt/postgres/lib/pgmq.so\\n```\\nTo test the extension, we can do:\\n\\n```bash\\ncargo pgrx run\\n```\\nand it\'ll start a `psql` prompt. In the prompt, we can execute the `create extension` statement to start using pgmq:\\n\\n```sql\\n-- List installed extensions\\n\\\\dx\\n\\n-- Enable pgmq\\ncreate extension pgmq;\\n\\n-- List installed extensions again\\n\\\\dx\\n```\\n\\nThe output will look something like:\\n```\\npgmq=# \\\\dx\\n                 List of installed extensions\\n  Name   | Version |   Schema   |         Description\\n---------+---------+------------+------------------------------\\n plpgsql | 1.0     | pg_catalog | PL/pgSQL procedural language\\n(1 row)\\n\\npgmq=# create extension pgmq;\\nCREATE EXTENSION\\n\\npgmq=# \\\\dx\\n                                     List of installed extensions\\n  Name   | Version |   Schema   |                             Description\\n---------+---------+------------+---------------------------------------------------------------------\\n pgmq    | 0.25.0  | public     | A lightweight message queue. Like AWS SQS and RSMQ but on Postgres.\\n plpgsql | 1.0     | pg_catalog | PL/pgSQL procedural language\\n(2 rows)\\n```\\n\\nWe can also list the available functions:\\n\\n```sql\\n-- List available functions under pgmq schema\\n\\\\df pgmq.*\\n```\\n\\n```\\npgmq=# \\\\df pgmq.*\\n                                                                         List of functions\\n Schema |          Name          |                                                                         Result data type                                                                         |                                                 Argument data types                                                  | Type \\n--------+------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+------\\n pgmq   | archive                | boolean                                                                                                                                                          | queue_name text, msg_id bigint                                                                                       | func\\n pgmq   | archive                | TABLE(archive boolean)                                                                                                                                           | queue_name text, msg_ids bigint[]                                                                                    | func\\n pgmq   | create                 | void                                                                                                                                                             | queue_name text                                                                                                      | func\\n pgmq   | create_non_partitioned | void                                                                                                                                                             | queue_name text                                                                                                      | func\\n pgmq   | create_partitioned     | void                                                                                                                                                             | queue_name text, partition_interval text DEFAULT \'10000\'::text, retention_interval text DEFAULT \'100000\'::text       | func\\n pgmq   | delete                 | boolean                                                                                                                                                          | queue_name text, msg_id bigint                                                                                       | func\\n pgmq   | delete                 | TABLE(delete boolean)                                                                                                                                            | queue_name text, msg_ids bigint[]                                                                                    | func\\n pgmq   | drop_queue             | boolean                                                                                                                                                          | queue_name text, partitioned boolean DEFAULT false                                                                   | func\\n pgmq   | list_queues            | TABLE(queue_name text, created_at timestamp with time zone)                                                                                                      |                                                                                                                      | func\\n pgmq   | metrics                | TABLE(queue_name text, queue_length bigint, newest_msg_age_sec integer, oldest_msg_age_sec integer, total_messages bigint, scrape_time timestamp with time zone) | queue_name text                                                                                                      | func\\n pgmq   | metrics_all            | TABLE(queue_name text, queue_length bigint, newest_msg_age_sec integer, oldest_msg_age_sec integer, total_messages bigint, scrape_time timestamp with time zone) |                                                                                                                      | func\\n pgmq   | pop                    | TABLE(msg_id bigint, read_ct integer, enqueued_at timestamp with time zone, vt timestamp with time zone, message jsonb)                                          | queue_name text                                                                                                      | func\\n pgmq   | purge_queue            | bigint                                                                                                                                                           | queue_name text                                                                                                      | func\\n pgmq   | read                   | TABLE(msg_id bigint, read_ct integer, enqueued_at timestamp with time zone, vt timestamp with time zone, message jsonb)                                          | queue_name text, vt integer, \\"limit\\" integer                                                                         | func\\n pgmq   | read_with_poll         | TABLE(msg_id bigint, read_ct integer, enqueued_at timestamp with time zone, vt timestamp with time zone, message jsonb)                                          | queue_name text, vt integer, \\"limit\\" integer, poll_timeout_s integer DEFAULT 5, poll_interval_ms integer DEFAULT 250 | func\\n pgmq   | send                   | bigint                                                                                                                                                           | queue_name text, message jsonb, delay integer DEFAULT 0                                                              | func\\n pgmq   | send_batch             | TABLE(msg_id bigint)                                                                                                                                             | queue_name text, messages jsonb[], delay integer DEFAULT 0                                                           | func\\n pgmq   | set_vt                 | TABLE(msg_id bigint, read_ct integer, enqueued_at timestamp with time zone, vt timestamp with time zone, message jsonb)                                          | queue_name text, msg_id bigint, vt_offset integer                                                                    | func\\n(18 rows)\\n```\\n\\nWith this, we can now explore the extension from the inside. And, if needed, recompile and reinstall the extension to play with it.\\n\\n\\n## The internals\\n\\nWe know that when an extension is created with pgrx, it generates a `lib.rs` file. Let us explore it.\\n\\nOne of the first thing we can see, is that the other five files in the `src/` directory are included:\\n\\n```rust\\npub mod api;\\npub mod errors;\\npub mod metrics;\\npub mod partition;\\npub mod util;\\n```\\n\\nAfter reviewing these files a little bit, we can notice that there\'s also some relevant code in another module, the one in `core/`. For example, in `src/partition.rs`:\\n\\n```rust\\nuse pgmq_core::{\\n    errors::PgmqError,\\n    query::{\\n        assign_archive, assign_queue, create_archive, create_archive_index, create_index,\\n        create_meta, grant_pgmon_meta, grant_pgmon_queue, grant_pgmon_queue_seq, insert_meta,\\n    },\\n    types::{PGMQ_SCHEMA, QUEUE_PREFIX},\\n    util::CheckedName,\\n};\\n```\\n\\nSo, at this point we know that we can find the source code in two places: `src/` and `core/`. \\n\\nIf we continue exploring `lib.rs`, we can see that a sql file (`sql_src.sql`) is executed when the extension is enabled:\\n\\n```rust\\nCREATE TABLE pgmq.meta (\\n    queue_name VARCHAR UNIQUE NOT NULL,\\n    is_partitioned BOOLEAN NOT NULL,\\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT now() NOT NULL\\n);\\n...\\n```\\n\\nWe can actually see that table with `psql`:\\n\\n```sql\\n-- List tables in the pgmq schema\\n\\\\dt pgmq.*\\n\\n-- List contents of pgmq.meta\\nselect * from pgmq.meta;\\n```\\n```\\npgmq-# \\\\dt pgmq.*\\n           List of relations\\n Schema | Name | Type  |  Owner\\n--------+------+-------+----------\\n public | meta | table | binidxaba\\n(1 row)\\n\\npgmq=# select * from pgmq.meta;\\n queue_name | created_at\\n------------+------------\\n(0 rows)\\n```\\n\\nThe following diagram shows what the pgmq schema looks like right after `CREATE EXTENSION` is executed:\\n\\n![after-create-extension](after-create-extension.png \\"after create extension\\")\\n\\nFrom this point, we can suspect that every time we create a queue, a new row is inserted into this table.\\n\\nLet us see what `pgmq.create()` does...\\n\\n\\n### pgmq.create()\\n\\nMost of the functions provided by pgmq are defined in `src/api.rs`. In that file, we can find the function `pgmq_create(queue_name: &str)`, and if we chase the call sequence, we can discover that the interesting function is `init_queue(name: &str)` in `core/src/query.rs`:\\n\\n```rust\\npub fn init_queue(name: &str) -> Result<Vec<String>, PgmqError> {\\n    let name = CheckedName::new(name)?;\\n    Ok(vec![\\n        create_queue(name)?,\\n        assign_queue(name)?,\\n        create_index(name)?,\\n        create_archive(name)?,\\n        assign_archive(name)?,\\n        create_archive_index(name)?,\\n        insert_meta(name)?,\\n        grant_pgmon_queue(name)?,\\n    ])\\n}\\n```\\nThis function generates several sql statements that are later executed in `pgmq_create_non_partitioned` using an [`Spi` client](https://docs.rs/pgx/latest/pgx/spi/struct.SpiClient.html).\\n\\nI\'ll skip the details, but the sql statements basically do:\\n1. Create a table `pgmq.q_<queue_name>`.\\n2. Assign the table to the pqmg extension.\\n3. Create an index on the `pgmq.q_<queue_name>` table.\\n4. Create a table `pgmq.a_<queue_name>`.\\n5. Assign the table to the pgmq extension.\\n6. Create an index on the `pgmq.a_<queue_name>` table.\\n7. Insert a row on the `pgmq.meta` table.\\n8. Grant privileges to `pg_monitor`.\\n\\nWe can see the effects of this in psql using the following lines:\\n```sql\\n-- Create a queue\\nselect pgmq.create(\'my_queue\');\\n\\n-- List tables\\n\\\\dt pgmq.*\\n\\n-- List indexes\\n\\\\di pgmq.*\\n\\n-- See the contents of pgmq_meta\\nselect * from pgmq.meta;\\n```\\n\\nThe output will show something like:\\n```\\npgmq=# select pgmq_create(\'my_queue\');\\n create\\n--------\\n\\n(1 row)\\n\\npgmq=# \\\\dt pgmq.*;\\n           List of relations\\n Schema |    Name    | Type  |  Owner\\n--------+------------+-------+-----------\\n pgmq   | a_my_queue | table | binidxaba\\n pgmq   | meta       | table | binidxaba\\n pgmq   | q_my_queue | table | binidxaba\\n(3 rows)\\n\\npgmq=# \\\\di pgmq.*\\n                         List of relations\\n Schema |           Name           | Type  |   Owner   |   Table\\n--------+--------------------------+-------+-----------+------------\\n pgmq   | a_my_queue_pkey          | index | binidxaba | a_my_queue\\n pgmq   | archived_at_idx_my_queue | index | binidxaba | a_my_queue\\n pgmq   | meta_queue_name_key      | index | binidxaba | meta\\n pgmq   | q_my_queue_pkey          | index | binidxaba | q_my_queue\\n pgmq   | q_my_queue_vt_idx        | index | binidxaba | q_my_queue\\n(5 rows)\\n\\npgmq=# select * from pgmq.meta;\\n queue_name | is_partitioned |          created_at           \\n ------------+----------------+-------------------------------\\n  my_queue   | f              | 2023-09-18 23:35:38.163096-06\\n  (1 row)\\n```\\n\\nThe following diagram shows what the pgmq schema looks like at this point:\\n\\n![complete](complete.png \\"after create queue\\")\\n\\nFor the queue `my_queue`, we can see the underlying table and the corresponding archive table. Each table has an index associated with the primary key. The `pgmq.q_my_queue` table also has an index on the `vt` column, and `pgmq.a_my_queue` has an index on the `archived_at` column.\\n\\nWe can suspect that the `pgmq.q_my_queue` table is used in the send and read operations. Let us look at those two functions.\\n\\n\\n### pgmq.send()\\n\\nWe can explore the send operation in a similar way. The relevant SQL is straightforward. It just inserts a new row in the the underlying table:\\n\\n```sql\\nINSERT INTO {PGMQ_SCHEMA}.{QUEUE_PREFIX}_{name} (vt, message)\\nVALUES {values}\\nRETURNING msg_id; \\n```\\n\\n![pgmq-send](pgmq-send.png \\"what send does\\")\\n\\n:::note \\nAt this point, we can see the following pattern in the pgmq project: \\n  - the exposed SQL functions are defined in `src/api.rs`, and \\n  - the underlying SQL statements are defined in `core/src/query.rs`\\n:::\\n\\n\\n### pgmq.read()\\n\\n\\nSo, let\'s see. If I were the one programming `pgmq.read()`, I would perhaps do something like \\"get the first `{limit}` rows from the queue table whose `{vt}` has already expired, and for those rows, also update the visibility timeout to `now() + {vt}`.\\" Naively, maybe something like:\\n\\n```sql\\nupdate pgmq.q_my_queue\\nSET      \\n    vt = clock_timestamp() + interval \'10 seconds\',                                            \\n    read_ct = read_ct + 1                                                                      \\nWHERE  \\n    msg_id in (select msg_id from pgmq.q_my_queue where vt <= clock_timestamp()                                                      \\n        ORDER BY msg_id ASC                                                                            \\n        LIMIT 1);  \\n```\\n\\nIn reality, `pgmq.read` is more interesting than that :sweat_smile:. It performs the following DML:\\n\\n```sql\\nWITH cte AS\\n    (\\n        SELECT msg_id\\n        FROM {PGMQ_SCHEMA}.{QUEUE_PREFIX}_{name}\\n        WHERE vt <= clock_timestamp()\\n        ORDER BY msg_id ASC\\n        LIMIT {limit}\\n        FOR UPDATE SKIP LOCKED\\n    )\\nUPDATE {PGMQ_SCHEMA}.{QUEUE_PREFIX}_{name} t\\nSET\\n    vt = clock_timestamp() + interval \'{vt} seconds\',\\n    read_ct = read_ct + 1\\nFROM cte\\nWHERE t.msg_id=cte.msg_id\\nRETURNING *;\\n```\\n\\n![pgmq-read](pgmq-read.png \\"what read does\\")\\n\\nFirstly, in pgmq\'s version, there is a CTE (Common Table Expression) to obtain the first `{limit}` message IDs whose `vt` has expired. (It would be interesting to discuss why pgmq developers used a CTE, but we can explore that in another post.)\\n\\nThere are two crucial things to notice in the CTE. One is the `order by` clause that ensures the FIFO ordering. The other one is the `FOR UPDATE SKIP LOCKED` clause, claiming the rows no one else has claimed. This part is essential because it ensures correctness in the case of concurrent  `pgmq.read()` operations. \\n\\nThe next step in the DML is to update the corresponding rows with a new vt value by adding the supplied `{vt}` to the current timestamp. Additionally, the `read_ct` value is incremented by 1. What is the use of this counter? In general, we can suspect that there is a problem processing a given message if it has a high `read_ct` value because users usually archive the message after successfully processing it. So, ideally, a message is only read once. \\n\\n\\n\\n### pgmq.archive()\\n\\nThe next stage in the lifecycle of a message is archiving it. For that, pgmq uses the following insert statement:\\n\\n```sql\\nWITH archived AS (\\n    DELETE FROM {PGMQ_SCHEMA}.{QUEUE_PREFIX}_{name}\\n    WHERE msg_id = ANY($1)\\n    RETURNING msg_id, vt, read_ct, enqueued_at, message\\n)\\nINSERT INTO {PGMQ_SCHEMA}.{ARCHIVE_PREFIX}_{name} (msg_id, vt, read_ct, enqueued_at, message)\\nSELECT msg_id, vt, read_ct, enqueued_at, message\\nFROM archived\\nRETURNING msg_id;\\n```\\n\\nEssentially, it deletes the message with the provided `msg_id` from the queue table, and then the message is placed in the corresponding archive table.\\n\\n![pgmq-archive](pgmq-archive.png \\"what archive does\\")\\n\\nOne interesting thing to notice is that `pgmq.archive()` can be used to archive a batch of messages too:\\n\\n```sql\\nselect pgmq.archive(\'my_queue\', ARRAY[3, 4, 5]);\\n```\\n\\n```\\npgmq=# select pgmq.archive(\'my_queue\', ARRAY[3, 4, 5]);\\n pgmq_archive\\n--------------\\n t\\n t\\n t\\n(3 rows)\\n\\n```\\n\\n\\nThat is achieved in pgrx by declaring two functions using the [same name in the `pg_extern`](https://github.com/pgcentralfoundation/pgrx/blob/047b1d1fc9e9c4007c871e226fa81e294f8bf5e6/pgrx-macros/src/lib.rs#L462) derive macro as follows:\\n\\n```rust\\n#[pg_extern(name = \\"archive\\")]\\nfn pgmq_archive(queue_name: &str, msg_id: i64) -> Result<Option<bool>, PgmqExtError> {\\n//...\\n}\\n\\n#[pg_extern(name = \\"archive\\")]\\nfn pgmq_archive_batch(\\n    queue_name: &str,\\n    msg_ids: Vec<i64>,\\n) -> Result<TableIterator<\'static, (name!(archive, bool),)>, PgmqExtError> {\\n//...\\n}\\n```\\n\\n\\n### pgmq.drop\\\\_queue()\\n\\nFinally, let\'s talk about `pgmq.drop_queue()`. It essentially executes multiple statements:\\n\\n1. Unassign the `pgmq.q_<queue_name>` table from the extension.\\n2. Unassign the `pgmq.a_<queue_name>` table from the extension.\\n3. Drop the table `pgmq.q_<queue_name>`.\\n4. Drop the table `pgmq.a_<queue_name>`.\\n5. Delete the corresponding row from the `pgmq.meta` table.\\n\\nNothing surprising in this one, and with it, we conclude our tour.\\n\\n\\n## Conclusion\\n\\nIn this post, we explored how the pgrx tool is used to generate the pgmq extension. We explored how the metadata objects are created and how they are used in the basic send, read and archive operations. At least from an explorer perspective, the internals of the extension are currently easy to read and understand.\\n\\nI invite everyone to explore how the other pgmq functions work. You can explore the code at https://github.com/tembo-io/pgmq. And you can learn more about pgrx at: https://github.com/pgcentralfoundation/pgrx."},{"id":"postgres-16","metadata":{"permalink":"/blog/postgres-16","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-09-20-postgres-16/index.md","source":"@site/blog/2023-09-20-postgres-16/index.md","title":"Postgres 16: The exciting and the unnoticed","description":"In case you missed it, Postgres 16 came out last week - and this year it arrived earlier than the last few years. There are many features that I\u2019ve been looking forward to for the last few months and I\u2019m excited to see them get into the hands of users. Before we dive into the specific features of this release, let\u2019s discuss what a Postgres major release actually means.","date":"2023-09-20T00:00:00.000Z","formattedDate":"September 20, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"}],"readingTime":8.83,"hasTruncateMarker":false,"authors":[{"name":"Samay Sharma","title":"CTO Tembo","url":"https://github.com/samay-sharma","email":"noreply@tembo.io","imageURL":"https://github.com/samay-sharma.png","key":"samay"}],"frontMatter":{"slug":"postgres-16","title":"Postgres 16: The exciting and the unnoticed","authors":["samay"],"tags":["postgres"]},"prevItem":{"title":"Anatomy of a Postgres extension written in Rust: pgmq","permalink":"/blog/postgres-extension-in-rust-pgmq"},"nextItem":{"title":"Enter the matrix: the four types of Postgres extensions","permalink":"/blog/four-types-of-extensions"}},"content":"In case you missed it, Postgres 16 came out last week - and this year it arrived earlier than the last few years. There are many features that I\u2019ve been looking forward to for the last few months and I\u2019m excited to see them get into the hands of users. Before we dive into the specific features of this release, let\u2019s discuss what a Postgres major release actually means.\\n\\n![postgres-16](postgres-16.png \\"PG16\\")\\n\\n\\n## Postgres Releases\\n\\nThe PostgreSQL Global Development Group releases a new _major_ [version](https://www.postgresql.org/support/versioning/) every year with new features. \\n\\nIn addition, Postgres releases minor versions of each major release every 3 months or so with bug fixes and security fixes. No new features are released in minor versions, and that\u2019s what makes major version releases so exciting as it\u2019s the culmination of about a year\u2019s worth of development work on the project. \\n\\nWhile there\'s a case to be made for faster release of features, Postgres prides itself on stability and this release cadence provides enough time for features to be proposed, reviewed, committed and tested before they get shipped.\\n\\n\\n## Should I upgrade to Postgres 16?\\n\\nIf you are building a new application, yes, I would recommend that you start with the latest major version of Postgres. This will guarantee the latest and greatest features, and a continuous flow of minor releases that fix bugs and improve the security of your database.\\n\\nIf you are upgrading an existing system, there are more factors to consider. The general advice is to **upgrade minor versions** always - because they contain security and bug fixes and the risk of not upgrading is higher. \\n\\nHowever, for major versions, you will need to consider the tradeoffs as the major versions usually change the internal format of system tables and data files. That means, you can\u2019t just use previous versions of the data directory \u2014 you\u2019ll need to use `pg_dump` / `pg_restore` or [pg_upgrade](https://www.postgresql.org/docs/current/pgupgrade.html) to upgrade. In addition, depending on the features you are using and the Postgres release, manual changes to your code or queries may also be required.\\n\\nObviously, another important factor if you are using a managed service provider is when they provide support for Postgres 16. At [Tembo Cloud](https://cloud.tembo.io/), we\u2019ve already started working on supporting Postgres 16 and expect it to be available in a few weeks.\\n\\n\\n## What\u2019s most exciting about Postgres 16?\\n\\nPostgres 16 delivers exciting features in all aspects of the database ranging from performance improvements, monitoring enhancements, better security and privilege handling, replication improvements, new server features and commands and a lot more. \\n\\nIf you\u2019re interested in the complete list of features, you can read the detailed [release notes](https://www.postgresql.org/docs/16/release-16.html). Below, I\u2019ll talk about the aspects of this release which excite me the most and we will talk about a few not-so-talked about features which lay the groundwork for more exciting features in Postgres 17.\\n\\n\\n### Logical replication improvements\\n\\n[Logical replication](https://www.postgresql.org/docs/current/logical-replication.html) is a feature I\u2019ve always been interested in, as it allows you to expand the capabilities of Postgres by moving data between different Postgres databases or from Postgres to other databases. It finds interesting use cases in: replicating selectively from one database to another, replicating across Postgres versions, online migrations and allowing consolidation from multiple databases.\\n\\nThis release, arguably the most exciting logical replication feature, is allowing [logical replication from standby servers](https://pganalyze.com/blog/5mins-postgres-16-logical-decoding). Prior to this feature, you could only create a logical replication slot on the primary which meant adding more replicas would add more load on the primary. With Postgres 16, secondaries also have the ability to create replication slots allowing for more distribution of that load. What\u2019s more is that the replication slots on the secondary are persisted even when the standby is promoted to the primary. This means that subscribers won\u2019t be affected even during a failover! You can read more about this feature in Bertrand\u2019s [blog post](https://bdrouvot.github.io/2023/04/19/postgres-16-highlight-logical-decoding-on-standby/).\\n\\nThe short of it is that now you can do this on a Postgres 16 standby:\\n\\n```\\npostgres@standby=# select pg_is_in_recovery();\\n pg_is_in_recovery\\n-------------------\\n t\\n(1 row)\\n\\npostgres@standby=# SELECT * FROM pg_create_logical_replication_slot(\'active_slot\', \'test_decoding\', false, true);\\n  slot_name  |    lsn\\n-------------+------------\\n active_slot | 0/C0053A78\\n(1 row)\\n```\\n\\nOn Postgres 15, the same thing would have errored out:\\n```\\npostgres@standby=# SELECT * FROM pg_create_logical_replication_slot(\'active_slot\', \'test_decoding\', false, true);\\nERROR:  logical decoding cannot be used while in recovery\\n```\\n\\nIn addition to this, there are a number of other logical replication performance improvements. This includes faster initial table sync using [binary format](https://www.postgresql.org/message-id/flat/TYCPR01MB8373B593010467315C2BA8EBED229%40TYCPR01MB8373.jpnprd01.prod.outlook.com#be8109723de40d3724730713175517ab), use of [btree indexes](https://www.postgresql.org/message-id/flat/CACawEhX6TvX%2Bj8EpcpCKvnMGao8Gcp8W43Sgc87pg9o6-Xbf2Q%40mail.gmail.com#81e7ec5c7732e7492c9e28d0f38df657) during logical replication apply when tables don\u2019t have a primary key (previously the table would be scanned sequentially) and parallel application of large transactions (~25-40% speedups).\\n\\n\\n### Monitoring improvements\\n\\nThe other bucket of features which intrigued me are the monitoring enhancements. While Postgres provides a number of statistics tables with monitoring information, I believe more can be done to provide actionable insights to users. As an example, Lukas pointed out several interesting gaps in Postgres monitoring in his [PGCon 2020 talk](https://www.pgcon.org/events/pgcon_2020/sessions/session/132/slides/49/Whats%20Missing%20for%20Postgres%20Monitoring.pdf).\\n\\nComing back to this release, [`pg_stat_io`](https://www.postgresql.org/docs/16/monitoring-stats.html#MONITORING-PG-STAT-IO-VIEW) has to be the most useful piece of information added to the Postgres stats views in Postgres 16. It allows you to understand the I/O done by Postgres at a more granular level, broken down by `backend_type` and `context`. This means you can calculate a more accurate cache hit ratio by ignoring the I/O done by VACUUM, differentiate between `extends` and `flushes`, and separate out bulk operations while deciding which configurations to tune. Melanie talks about this and much more in her [talk](https://www.youtube.com/watch?v=rCzSNdUOEdg) and this [blog post](https://www.depesz.com/2023/02/27/waiting-for-postgresql-16-add-pg_stat_io-view-providing-more-detailed-io-statistics/) approaches how you would use this as a DBA.\\n\\nHere is an example of the statistics you can see in `pg_stat_io`:\\n\\n```\\n$ SELECT * FROM pg_stat_io ;\\n    backend_type     \u2502   io_object   \u2502 io_context \u2502  reads  \u2502 writes  \u2502 extends \u2502 op_bytes \u2502 evictions \u2502 reuses  \u2502 fsyncs \u2502          stats_reset\\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n autovacuum launcher \u2502 relation      \u2502 bulkread   \u2502       0 \u2502       0 \u2502  [NULL] \u2502     8192 \u2502         0 \u2502       0 \u2502 [NULL] \u2502 2023-02-27 13:25:39.725072+01\\n ...\\n autovacuum worker   \u2502 relation      \u2502 bulkread   \u2502       0 \u2502       0 \u2502  [NULL] \u2502     8192 \u2502         0 \u2502       0 \u2502 [NULL] \u2502 2023-02-27 13:25:39.725072+01\\n ...\\n client backend      \u2502 temp relation \u2502 normal     \u2502       0 \u2502       0 \u2502       0 \u2502     8192 \u2502         0 \u2502  [NULL] \u2502 [NULL] \u2502 2023-02-27 13:25:39.725072+01\\n background worker   \u2502 relation      \u2502 bulkread   \u2502  268221 \u2502  268189 \u2502  [NULL] \u2502     8192 \u2502         0 \u2502  268189 \u2502 [NULL] \u2502 2023-02-27 13:25:39.725072+01\\n ...\\n checkpointer        \u2502 relation      \u2502 normal     \u2502  [NULL] \u2502   32121 \u2502  [NULL] \u2502     8192 \u2502    [NULL] \u2502  [NULL] \u2502   3356 \u2502 2023-02-27 13:25:39.725072+01\\n standalone backend  \u2502 relation      \u2502 bulkread   \u2502       0 \u2502       0 \u2502  [NULL] \u2502     8192 \u2502         0 \u2502       0 \u2502 [NULL] \u2502 2023-02-27 13:25:39.725072+01\\n ...\\n startup             \u2502 relation      \u2502 vacuum     \u2502       0 \u2502       0 \u2502       0 \u2502     8192 \u2502         0 \u2502       0 \u2502 [NULL] \u2502 2023-02-27 13:25:39.725072+01\\n walsender           \u2502 relation      \u2502 bulkread   \u2502       0 \u2502       0 \u2502  [NULL] \u2502     8192 \u2502         0 \u2502       0 \u2502 [NULL] \u2502 2023-02-27 13:25:39.725072+01\\n ...\\n walsender           \u2502 temp relation \u2502 normal     \u2502       0 \u2502       0 \u2502       0 \u2502     8192 \u2502         0 \u2502  [NULL] \u2502 [NULL] \u2502 2023-02-27 13:25:39.725072+01\\n...\\n```\\n\\nIn addition to this, there are other improvements including the addition of `last_seq_scan` and `last_idx_scan` on `pg_stat_*` tables which allow you to understand index usage better and figure out when plans for a query might have changed.\\n\\n\\n### Special mentions\\n\\nLike I said, each release comes with many improvements - and I could not outline them all in a blog post (and if I did, nobody would read it!). But I do want to mention a few other items in Postgres 16 that I won\u2019t be able to dive deeper into but are interesting as well.\\n\\n\\n\\n* Load balancing with multiple hosts in `libpq`: This feature allows to balance the load across Postgres read replicas directly within `libpq` (which is the foundational Postgres client library) without having to use another load balancer. You can read [this blog post](https://mydbops.wordpress.com/2023/05/07/postgresql-16-brings-load-balancing-support-in-libpq-psql/) on how this new feature is implemented and can be used.\\n* Performance: I won\u2019t repeat what\u2019s in the release notes but there\u2019s a long list of performance improvements in this release. There\u2019s support for more parallelism on `FULL` and `OUTER` `JOINs` and on more aggregates, greater usage of incremental sorts, window function optimizations and even an upto 300% performance improvement in `COPY`.\\n* `VACUUM` improvements: Last thing I\u2019d mention is improvements to `VACUUM` which include freezing performance improvements, ability to increase (or decrease) shared buffer usage by `VACUUM`, and faster loading of `VACUUM` configs.\\n\\n\\n## Laying the groundwork for an exciting Postgres 17 (and beyond)\\n\\nAll of the features mentioned above can immediately add a lot of value to your Postgres usage, but there are new features which lay the groundwork for powerful features in future releases. I\u2019ll quickly touch upon three items that I believe are noteworthy:\\n\\n\\n\\n* Direct IO and Async IO in Postgres: In Postgres 16, several building blocks for implementing direct and [asynchronous IO](https://www.youtube.com/watch?v=CD0w3gWBF7s) for Postgres were committed. This includes reduced contention on the relation extension lock and addition of Direct IO as a developer only option via the `debug_io_direct` setting. This important but hard work has been ongoing for several releases and Postgres 17 will likely be the first release where users will be able to utilize these features.\\n* Moving towards Active Active replication: A feature was committed in Postgres 16 to allow logical replication to avoid replication loops, which is when a transaction gets replicated from source to target and back. Postgres 16 allows subscribers to process only changes which have no origin which allows you to prevent these loops. Bi-directional active-active replication is still very complicated and requires solving a lot of problems, but this feature tackles one of those important sub-problems.\\n* Migration of the build system to Meson: This might have slipped under your radar but in this release, Postgres added support for a new build system which is expected to replace the Autoconf and Windows based build systems. Why, you might ask? Andres makes a compelling case for it in [this thread](https://www.postgresql.org/message-id/flat/20211012083721.hvixq4pnh2pixr3j%40alap3.anarazel.de) if you\u2019re interested but some reasons include faster build times, simpler dependency management and moving towards a common build system across Linux and Windows.\\n\\n\\n## Postgres continues to deliver\\n\\nWith every new release, Postgres becomes more and more compelling and adds great features that improve the experience of its users. I recommend you check out the [press release](https://www.postgresql.org/about/news/postgresql-16-released-2715/), and [the release notes](https://www.postgresql.org/docs/16/release-16.html) to see everything coming along with this new release.\\n\\nAnd if you want to try out the full power of Postgres including its powerful extension ecosystem on a managed service, try out [Tembo Cloud](https://cloud.tembo.io/)."},{"id":"four-types-of-extensions","metadata":{"permalink":"/blog/four-types-of-extensions","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-09-14-four-types-of-extensions/index.md","source":"@site/blog/2023-09-14-four-types-of-extensions/index.md","title":"Enter the matrix: the four types of Postgres extensions","description":"Before working for a Postgres company, I had never used extensions.","date":"2023-09-14T00:00:00.000Z","formattedDate":"September 14, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"extensions","permalink":"/blog/tags/extensions"},{"label":"trunk","permalink":"/blog/tags/trunk"}],"readingTime":12.05,"hasTruncateMarker":false,"authors":[{"name":"Steven Miller","title":"Founding Engineer","url":"https://github.com/sjmiller609","email":"noreply@tembo.io","imageURL":"https://github.com/sjmiller609.png","key":"steven"}],"frontMatter":{"slug":"four-types-of-extensions","title":"Enter the matrix: the four types of Postgres extensions","authors":["steven"],"tags":["postgres","extensions","trunk"]},"prevItem":{"title":"Postgres 16: The exciting and the unnoticed","permalink":"/blog/postgres-16"},"nextItem":{"title":"Tembo Stacks: Making Postgres the Everything Database","permalink":"/blog/tembo-stacks-intro"}},"content":"Before working for a Postgres company, I had never used extensions.\\n\\nNow, I\'m part of a team working to fully automate turning on any extension. I didn\'t find great resources to explain the process of turning on an extension, and why it varies between different extensions.\\n\\n**I want to share my mental model for the different types of extensions, how to know what type of extension you\'re working with, and how to get it turned on.**\\n\\n## Turn on an extension\\n\\n![one-does-not-simply](./images/one-does-not-simply.png)\\n\\n**What\'s traditionally involved:**\\n\\n- Find the extension you want\\n- Figure out how to build it\\n- Sometimes, installation of dependencies (for example with *apt-get* or *yum*)\\n- Sometimes, installation of other extensions (*goto* \u2018figure out how to build it\u2019)\\n- Install your extension\\n- Sometimes, load a library\\n- Sometimes, provide extension-specific configurations\\n- Sometimes, run the `CREATE EXTENSION` command\\n\\nBuilding and installing extensions is well covered by other resources. In this blog, I want to focus on steps to get an extension up and running after it\'s installed, and how **I believe that all extensions fit into four mostly-tidy categories.**\\n\\n### Terminology\\n\\nExtensions consist of **SQL** and / or **libraries**.\\n\\nA **library** simply means compiled code, for example written in [C](https://www.postgresql.org/docs/current/xfunc-c.html) or [Rust](https://github.com/pgcentralfoundation/pgrx).\\n\\n**[SQL objects](https://www.postgresql.org/docs/current/extend-extensions.html#:~:text=A%20useful%20extension%20to%20PostgreSQL,package%20to%20simplify%20database%20management.)**, let\'s just call it SQL, are extensions of SQL, for example new functions and data types. These are often implemented by a library, but can also be implemented in other ways, for example using a procedural language like [PL/pgSQL](https://www.postgresql.org/docs/current/plpgsql.html).\\n\\n**Hooks:** A Postgres feature informally called *hooks* can be used to connect into Postgres\' existing functionality. Hooks allow for overwriting default Postgres functionality, or calling back into an extension\'s code at the appropriate time. For example, one type of hook can modify Postgres start up behavior to launch a background worker, and a different type of hook can be used to redirect queries to a different table.\\n:::note\\nSometimes extensions are instead referred to as \'modules\', but I like to simply refer to everything as an \'extension\', but feel free to @ me on X to tell me why that\'s wrong ([@sjmiller609](https://twitter.com/sjmiller609)).\\n:::\\n\\n## Enter the matrix\\n\\n![im-in-matrix](./images/im-in-matrix.gif)\\n\\nA big part of what I have been working on is fully automating enabling any extension. In order to do that, we have to understand exactly how extensions vary. We can break it down into a 2x2 matrix by defining two boolean categories.\\n\\n**Requires [LOAD](https://www.postgresql.org/docs/current/sql-load.html)** true or false and **requires [CREATE EXTENSION](https://www.postgresql.org/docs/current/sql-createextension.html)** true or false:\\n\\n|                             | Requires `CREATE EXTENSION`                             | Does not require `CREATE EXTENSION`                           |\\n|-----------------------------|---------------------------------------------------------|---------------------------------------------------------------|\\n| **Requires `LOAD`**         | Extensions that use SQL and their libraries have hooks  | Extensions that do not use SQL, may or may not have hooks     |\\n| **Does not require `LOAD`** | SQL-only extensions, and SQL + libraries without hooks  | Output plugins                                                |\\n\\n*By categorizing an extension into this 2x2 matrix, we can know how to turn it on.*\\n\\n### LOAD\\n\\n [LOAD](https://www.postgresql.org/docs/current/sql-load.html) is a command that tells Postgres to *load* a library, meaning make the code accessible to Postgres by loading the compiled code on disk into memory. If a library has hooks, **performing a load will activate the hooks**.\\n\\n**Requires `LOAD`: true** means you have to do one of the following steps to load a library:\\n- **[LOAD](https://www.postgresql.org/docs/current/sql-load.html)**: using the `LOAD` command directly loads a library for the current connection only\\n- **[session_preload_libraries](https://www.postgresql.org/docs/15/runtime-config-client.html#GUC-LOCAL-PRELOAD-LIBRARIES:~:text=at%20session_preload_libraries%20instead.-,session_preload_libraries,-(string))**: configuration, specifies which libraries to LOAD for new connections\\n- **[shared_preload_libraries](https://www.postgresql.org/docs/15/runtime-config-client.html#GUC-LOCAL-PRELOAD-LIBRARIES:~:text=pooling%20is%20used.-,shared_preload_libraries,-(string))**: configuration, specifies which libraries to LOAD at server start, and therefore requires a restart\\n\\n:::note\\nEven though code is loaded in other ways during `CREATE EXTENSION`, that is not **requires `LOAD`: true** under this definition. I mean that the user must do something other than `CREATE EXTENSION` to load in libraries. Also, we are conflating [local_preload_libraries](https://www.postgresql.org/docs/15/runtime-config-client.html#GUC-LOCAL-PRELOAD-LIBRARIES:~:text=load%20that%20module.-,local_preload_libraries,-(string)) with `session_preload_libraries` to simplify things in this blog post.\\n:::\\n\\nFor example, if you installed the extension [auto explain](https://pgt.dev/extensions/auto_explain), then you may have a library file called `auto_explain.so` in your library directory, which can be found with [pg_config --pkglibdir](https://pgpedia.info/d/dynamic_library_path.html). Libraries are not always named exactly the same as the extension.\\n\\n```\\n$ trunk install auto_explain\\n\\nUsing pkglibdir: /var/lib/postgresql/data/tembo/15/lib\\n\\n[+] auto_explain.so => /var/lib/postgresql/data/tembo/15/lib\\n```\\n\\n```\\n$ ls $(pg_config --pkglibdir) | grep auto_explain\\nauto_explain.so\\n```\\n\\nAuto explain can be loaded into your session like `LOAD \'auto_explain\';`. This command will always match exactly the name of the library file, less the file type, in this example `.so`. With a [couple of configurations](https://www.postgresql.org/docs/current/auto-explain.html#AUTO-EXPLAIN-EXAMPLE), now this extension will automatically log the [EXPLAIN ANALYZE](https://www.postgresql.org/docs/current/sql-explain.html) output for long-running queries.\\n\\n```\\npostgres=# LOAD \'auto_explain\';\\nLOAD\\n```\\n\\nHowever, the `LOAD` command is not typically used directly, and many extensions require you do not load them in this way. Instead, typically the Postgres configuration [shared_preload_libraries](https://pgpedia.info/s/shared_preload_libraries.html) is used instead.\\n```\\npostgres=# LOAD \'pg_cron\';\\nERROR:  pg_cron can only be loaded via shared_preload_libraries\\nHINT:  Add pg_cron to the shared_preload_libraries configuration variable in postgresql.conf.\\n```\\n\\nThe best reason to use `LOAD` directly is for debugging. It can be nice to `LOAD` on-demand while troubleshooting.\\n\\n:::info\\n**What to do when an extension requires load:**\\n\\nExtensions that **requires `LOAD`: true** can always be configured in `shared_preload_libraries`, but this configuration requires a restart to take effect. Some extensions can be loaded without a restart using `LOAD` directly, but in this case it\'s usually better to use the `session_preload_libraries` configuration, and [reload the Postgres configuration](https://pgpedia.info/p/pg_reload_conf.html) with `SELECT pg_reload_conf();`. You should run `LOAD` directly when you are intentionally loading for only the current connection.\\n:::\\n\\n### CREATE EXTENSION\\n\\nWhen you run [CREATE EXTENSION](https://www.postgresql.org/docs/current/sql-createextension.html), this basically just runs an extension\'s SQL script. The script will typically create new SQL objects such as functions, data types, operators and so on.\\n\\n`CREATE EXTENSION` looks at the extension\'s **control file**, which is installed to the `extension` directory of **sharedir**.\\n\\n```\\n$ trunk install pg_jsonschema;\\n\\nUsing pkglibdir: \\"/var/lib/postgresql/data/tembo/15/lib\\"\\nUsing sharedir: \\"/var/lib/postgresql/data/tembo\\"\\n\\n[+] pg_jsonschema.so => /var/lib/postgresql/data/tembo/15/lib\\n[+] extension/pg_jsonschema--0.1.4.sql => /var/lib/postgresql/data/tembo\\n[+] extension/pg_jsonschema.control => /var/lib/postgresql/data/tembo\\n```\\n\\n**sharedir** can be located with `pg_config --sharedir`\\n```\\n$  ls $(pg_config --pkglibdir) | grep pg_jsonschema\\npg_jsonschema.so\\n\\n$ ls $(pg_config --sharedir)/extension | grep pg_jsonschema\\npg_jsonschema--0.1.4.sql\\npg_jsonschema.control\\n```\\n\\nThe information in a control file is used to determine what start up or upgrade scripts to run. We\'ll cover upgrades in-depth in a future blog, so let\'s focus on first-time enabling. For example, in the above installation output, we notice a file `pg_jsonschema--0.1.4.sql`. Postgres knows to run this because the name of the control file matches the name of the script suffixed by the `default_version` defined in the control file.\\n\\n```\\n$ cat $(pg_config --sharedir)/extension/pg_jsonschema.control\\ncomment = \'JSON schema validation on json and jsonb data types\'\\ndefault_version = \'0.1.4\'\\nmodule_pathname = \'$libdir/pg_jsonschema\'\\nrelocatable = false\\nsuperuser = true\\n```\\n\\nWhen running `CREATE EXTENSION`, the extension name always matches exactly the name of a control file, less the `.control` file type.\\n```\\npostgres=# CREATE EXTENSION pg_jsonschema;\\nCREATE EXTENSION\\n```\\n\\nI mentioned that a start up script creates new SQL, including new functions. For example in the case of [pg_jsonschema](https://pgt.dev/extensions/pg_jsonschema), the start up script `pg_jsonschema--0.1.4.sql` includes the following SQL to create a new function called `jsonb_matches_schema`. Even though we have a library file, we don\'t need `LOAD` because `CREATE FUNCTION` is another way to load code from a file. This is an example of **requires `LOAD`: false**, **requires `CREATE EXTENSION`: true**.\\n\\n[CREATE FUNCTION ... AS \'obj_file\' documentation](https://www.postgresql.org/docs/current/sql-createfunction.html)\\n> obj_file is the name of the shared library file containing the compiled [code]\\n\\n```\\nCREATE FUNCTION \\"jsonb_matches_schema\\"(\\n        \\"schema\\" json,\\n        \\"instance\\" jsonb\\n) RETURNS bool\\nIMMUTABLE STRICT\\nLANGUAGE c\\nAS \'MODULE_PATHNAME\', \'jsonb_matches_schema_wrapper\';\\n```\\n\\n:::info\\nYou can always know whether or not an extension requires `CREATE EXTENSION` by the presence of a control file in `$(pg_config --sharedir)/extension`\\n:::\\n\\n### Hooks that require a restart\\n\\nAn extension is in the category **requires `CREATE EXTENSION`: true** and **requires `LOAD`: true** if the extension has libraries that use hooks which require a restart and it has a control file.\\n\\nYou will be able to identify this is the case when the extension\'s documentation mentions both `CREATE EXTENSION` and `shared_preload_libraries`. Sometimes an error message or hint is provided if you run `CREATE EXTENSION` before loaded the library, or if you try to run `LOAD` directly, but you can\'t count on that.\\n\\nFor example, in the case of both `pg_cron` and `pg_partman`, there are a background workers. These are examples of extensions using hooks in the start up process of Postgres. So, in both of these cases the user is expected to configure `shared_preload_libraries` to start the background worker, then run `CREATE EXTENSION` on a cluster where that background worker is already running.\\n\\n### LOAD is needed when there isn\'t a control file\\n\\nIn the case of auto_explain, it uses hooks that do not require a restart. In this case, there is no control file and no extra SQL objects to be created. So `LOAD` is required simply because we have to load it into memory somehow. To demonstrate, it is technically possible to make a control file for auto_explain to allow for `CREATE EXTENSION` behavior instead of `LOAD`:\\n\\n**auto_explain.control:**\\n```\\ncomment = \'auto explain\'\\ndefault_version = \'0.0.1\'\\nmodule_pathname = \'$libdir/auto_explain\'\\nrelocatable = false\\nsuperuser = true\\n```\\n\\n**auto_explain--0.0.1.sql**\\n```\\nLOAD \'auto_explain\';\\n```\\n:::caution\\nIn practice, do not use `LOAD` in an extension start up script to activate hooks. `LOAD` is only applicable for the current connection.\\n\\n:::\\n\\n```\\npostgres=# CREATE EXTENSION auto_explain;\\nCREATE EXTENSION\\n\\npostgres=# \\\\dx\\n                     List of installed extensions\\n      Name       | Version |   Schema   |         Description\\n-----------------+---------+------------+------------------------------\\n auto_explain    | 0.0.1   | public     | auto explain\\n plpgsql         | 1.0     | pg_catalog | PL/pgSQL procedural language\\n(2 rows)\\n\\npostgres=# SET auto_explain.log_min_duration = 0;\\nSET\\npostgres=# SET auto_explain.log_analyze = true;\\nSET\\n```\\nAfter running the above, now my subsequent queries have their `EXPLAIN ANALYZE` logged.\\n\\nSo, if that could work, **why not just have control files for all extensions?**\\n\\n**Having a control file requires version upgrade handling.**\\n\\nWhen you have a control file, you also have to write upgrade scripts for every new version. In the case of pg_cron, we can find all these files in **sharedir**. When enabling version 1.5, it will run `pg_cron--1.0.sql`, then each migration script up to 1.5.\\n```\\npg_cron--1.0--1.1.sql\\npg_cron--1.0.sql\\npg_cron--1.1--1.2.sql\\npg_cron--1.2--1.3.sql\\npg_cron--1.3--1.4.sql\\npg_cron--1.4-1--1.5.sql\\npg_cron--1.4--1.4-1.sql\\npg_cron.control\\n```\\n\\nSince that\'s not really applicable on auto_explain, because it\'s just logging outputs and there is nothing to migrate or handle between versions, it\'s just cleaner to not have a control file. Upgrading auto_explain only involves replacing the library, then loading it again.\\n\\n:::info\\nUpgrade logic is not applicable for extensions that do not require `CREATE EXTENSION`. These cases just involve re-loading a new version of the library.\\n:::\\n\\n\\n### You don\'t load hooks during CREATE EXTENSION\\n\\nIt made sense to me for activating hooks that require a restart they have to be configured in `shared_preload_libraries`. But for extensions that do not require a restart, it\'s not obvious why the hooks can\'t just be loaded during the `CREATE EXTENSION` start up script like I just demonstrated is possible with auto_explain.\\n\\n**Even though it\'s technically possible to `LOAD` hooks during `CREATE EXTENSION`, it\'s a bad idea.**\\n\\nFirst of all, when using the `LOAD` command directly, it\'s only applicable to the current connection. So, in the above example with auto explain, the queries are only logged in the connection where I ran `CREATE EXTENSION`. To apply to all connections without a restart, it would need to go into `session_preload_libraries`. It is technically possible to do that inside of `CREATE EXTENSION` by doing `ALTER SYSTEM SET session_preload_libraries` then `SELECT pg_reload_conf()` in your start up script, but it is not a good approach for `CREATE EXTENSION` to automatically perform a configuration update. First of all it would confuse a user to change a config on the fly, and secondly there is currently no concept to automatically merge multi-value, comma-separated configurations like `session_preload_libraries`.\\n\\n\\n:::info\\nThe 2x2 matrix makes it easier to understand how to enable an extension.\\n\\nJust ask yourself \\"do I need to run `CREATE EXTENSION`?\\" determined by presence of a control file, and \\"do I need to do a `LOAD`?\\" determined by any mention of `LOAD`, `shared_preload_libraries`, or `session_preload_libraries` in the extension\'s documentation or an error message.\\n\\nIn all cases of needing a `LOAD`, you can get away with setting it in `shared_preload_libraries`. You can optimize to avoid restarts in some cases.\\n:::\\n\\n\\n### Output plugins\\n\\nThere are some extensions, for example [wal2json](https://pgt.dev/extensions/wal2json) that require neither `CREATE EXTENSION` or `LOAD`. In all known cases so far, these are [output plugins](https://www.postgresql.org/docs/current/logicaldecoding-output-plugin.html). I think it\'s more of a stretch to call these \'extensions\', but since they provide additional functionality to Postgres, that counts in my book.\\n\\nIn the case of output plugins, the library is loaded when a replication slot is created:\\n\\n[Postgresql documentation](https://www.postgresql.org/docs/current/logicaldecoding-example.html)\\n```\\nSELECT * FROM pg_create_logical_replication_slot(\'regression_slot\', \'test_decoding\', false, true);\\n```\\n\\n## Installing extensions\\n\\nSome of the above examples use the free and open source [Trunk project](https://pgt.dev) that Tembo created, which allows us to skip the build process. It also installs extension dependencies, and provides metadata about other dependencies. When I\'m trying out extensions, I am starting from one of Tembo\u2019s container images to handle the system dependencies installation.\\n\\n**I wrote [this guide](https://tembo.io/docs/tembo-cloud/try-extensions-locally) for trying out extensions locally**. If you have any issues just reach out on our [community Slack channel](https://join.slack.com/t/tembocommunity/shared_invite/zt-20v3m8pwz-pPjeFaWSM~Bt3KUqDXff2A) and we can help.\\n\\n*Perhaps it\'s not so bad after all...*\\n\\n![files-in-computer](./images/files-in-computer.gif)\\n\\n## Automate everything\\n\\nWe want to make it possible to automatically install and turn on any Postgres extension. For this reason, we are seeking to qualify all known extensions by these two dimensions: requires `CREATE EXTENSION` true or false, and requires `LOAD` true or false.\\n\\nTo enable the community, that metadata is being published on [Trunk](https://pgt.dev). On [Tembo Cloud](https://cloud.tembo.io), we leverage that information to automatically enable extensions. Currently, we\'ve got this working for over 150 extensions.\\n\\n## Dear experts, tell me how I\'m wrong (seriously!)\\n\\nI\'m serious that I want you to tell me where this is incorrect! If you\'re a Postgres extensions expert, or maybe just know a thing or two about extensions that seems to conflict with something in this blog, please reach out on X [@sjmiller609](https://twitter.com/sjmiller609) and let me know. Even if it\'s just minor correction or subjective information, I\'d love to hear from you. I also want to hear if there is an easier mental model than this. I hope this blog can serve as a minimal yet comprehensive explanation of what it takes to get extensions turned on.\\n\\nAnother way to contribute is to **click the \\"Edit this page\\" link below**, and suggest changes. I will happily accept improvements to this blog."},{"id":"tembo-stacks-intro","metadata":{"permalink":"/blog/tembo-stacks-intro","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-09-06-tembo-stacks/index.md","source":"@site/blog/2023-09-06-tembo-stacks/index.md","title":"Tembo Stacks: Making Postgres the Everything Database","description":"For years, Postgres has been considered among the most popular and advanced open source OLTP databases. Interestingly, 20 years ago, it was a competition between Postgres and MySQL and this debate is still going on today. \ud83d\ude42 Over the last few years, Postgres added more features and its ecosystem added more extensions and now the comparison list keeps growing. After adding support for jsonb in 9.4, Postgres began to be compared with MongoDB as users started using Postgres to store unstructured data. With Citus, people started using Postgres over other distributed SQL technologies for use cases such as multi-tenancy and analytics. And this phenomenon continues to grow with pgvector, timescaledb and others.","date":"2023-09-06T00:00:00.000Z","formattedDate":"September 6, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"tembo","permalink":"/blog/tags/tembo"}],"readingTime":5.01,"hasTruncateMarker":false,"authors":[{"name":"Samay Sharma","title":"CTO Tembo","url":"https://github.com/samay-sharma","email":"noreply@tembo.io","imageURL":"https://github.com/samay-sharma.png","key":"samay"}],"frontMatter":{"slug":"tembo-stacks-intro","title":"Tembo Stacks: Making Postgres the Everything Database","authors":["samay"],"tags":["postgres","tembo"]},"prevItem":{"title":"Enter the matrix: the four types of Postgres extensions","permalink":"/blog/four-types-of-extensions"},"nextItem":{"title":"Optimizing Postgres\'s Autovacuum for High-Churn Tables","permalink":"/blog/optimizing-postgres-auto-vacuum"}},"content":"For years, Postgres has been considered among the most popular and advanced open source OLTP databases. Interestingly, 20 years ago, it was a competition between Postgres and MySQL and this debate is still going on [today](https://news.ycombinator.com/item?id=35906604). \ud83d\ude42 Over the last few years, Postgres added more features and its ecosystem added more extensions and now the comparison list keeps growing. After adding support for [jsonb](https://www.postgresql.org/docs/release/9.4.0/) in 9.4, Postgres began to be compared with MongoDB as users started using Postgres to store unstructured data. With [Citus](https://github.com/citusdata/citus), people started using Postgres over other distributed SQL technologies for use cases such as multi-tenancy and analytics. And this phenomenon continues to grow with [pgvector](https://github.com/pgvector/pgvector), [timescaledb](https://github.com/timescale/timescaledb) and others.\\n\\n\\n# Use Postgres for Everything\\n\\nPostgres has evolved to where people have started talking about using \u201cPostgres for everything.\u201d There\u2019s strong agreement that it\u2019s wise to use Postgres until you find a strong reason for why you can\u2019t use it anymore - either a very specialized use case, or scale which can\u2019t be achieved by Postgres. This topic was well-discussed on [hacker news](https://news.ycombinator.com/item?id=33934139) thanks to a [blog](https://www.amazingcto.com/postgres-for-everything/) by a friend, Steven Schmidt.\\n\\nAnd to clarify, when I say Postgres, I mean the entire Postgres ecosystem - this includes Postgres, its extensions and ecosystem tooling for high-availabilty, backups, connection pooling, etc. All of these together turn Postgres into a powerful data platform which you can use to run diverse workloads. However, it\u2019s easier said than done.\\n\\n\\n## Making \u201cPostgres for Everything\u201d real\\n\\nAdvanced users may know the recipe which turns Postgres into a distributed database, a vector database, an OLAP database or a search store, but this is still inaccessible to most developers. Over years of working with Postgres users, I\u2019ve realized that even tuning Postgres for OLTP (which is a fairly well known use case) is very hard for most developers. For example: OLTP deployments often have badly chosen indexes, over / under provisioned hardware, insufficiently tuned autovacuum etc. even though they are running on managed services which do the heavy lifting for them.\\n\\nSo, expecting developers to be able to figure out how to use Postgres for all these use cases when there are other databases focused on user experience for one particular use case is unrealistic. Even though developers know that dealing with 5 different databases is a pain in the medium to long run, when they are starting, they care about getting off the ground fast. So, to expect everybody to use Postgres for everything, we need to make it possible for every user to have easy access to the recipes which turn Postgres into the database for the data service of their choice.\\n\\n\\n# Enter Tembo Stacks\\n\\nTembo stacks are pre-built use-case-specific Postgres deployments which are optimized and tuned to serve a specific workload. They aim to be a replacement for other databases which you actually don\u2019t need, but you are considering because you don\u2019t know how to solve that problem using Postgres. They help you avoid the pains associated with learning, managing and deploying other database systems. Some examples of Stacks are: OLTP, Message Queue, Data Warehouse, Enterprise LLM and Document Store.\\n\\n\\n## Defining a stack\\n\\nA stack is a recipe of how to run an optimized Postgres for a workload, expressed as a spec file. The spec includes the following components:\\n\\n\\n\\n* Docker Base Image containing a particular version of Postgres\\n* Curated set of extensions which turn Postgres into best-in-class for that workload.\\n* Hardware profile which is best suited for that workload\\n* Postgres configs optimized according to hardware and workload\\n* Use-case specific metrics, alerts and recommendations\\n* On-instance sidecar - Kubernetes Services to Deploy a containerized application near Postgres to expand capabilities while minimizing network latency\\n\\nLet\u2019s look at an example of a stack spec for a [Message queue](https://github.com/tembo-io/tembo-stacks/blob/main/tembo-operator/src/stacks/templates/message_queue.yaml) stack - an [SQS](https://aws.amazon.com/sqs/) / [RabbitMQ](https://www.rabbitmq.com/) replacement based on Postgres.\\n\\nIt has the following components:\\n\\n\\n\\n* A standard Postgres 15 base image\\n* Curated extensions: [pgmq](https://github.com/tembo-io/pgmq), [pg_partman](https://github.com/pgpartman/pg_partman) and [pg_stat_statements](https://www.postgresql.org/docs/current/pgstatstatements.html)\\n* Optimized Postgres configs: Aggressive autovacuum settings, reduced random_page_cost and Checkpoint and WAL configs tuned for a high throughput Postgres instance\\n* CPU::Memory ratio recommendations for hardware\\n* Message queue specific metrics like queue length, oldest message age, newest message age\\n* (Coming soon) pg_bouncer\\n\\nThis and many other such stack specifications are [open source](https://github.com/tembo-io/tembo-stacks/tree/main/tembo-operator/src/stacks/templates) and can be used to deploy a stack locally on a self-managed instance or fully managed on Tembo Cloud. This is a reflection of one of our core values - to always put developers first. We open source our stacks to create the best development experience: locally and on the cloud, and to invite community feedback and collaboration.\\n\\n\\n## Deploying a stack on Tembo Cloud\\n\\n[Tembo Cloud](https://cloud.tembo.io/) is a dev-first, fully-extensible, fully-managed, secure, and scalable Postgres service. It has the ability to take this stack spec and deploy a Postgres instance which is built using this stack spec. With Tembo Cloud, you can get all the benefits of a managed service like: backups, high availability, scaling of storage and compute, metrics and alerts with an easy to use UI and CLI. And you get access to an ever growing list of [extensions](https://pgt.dev/) which you can add to your stack or bring your own extensions to deploy on Tembo Cloud.\\n\\n\\n![image](image.png)\\n\\n\\n# Evolving Postgres to be the data platform for everything\\n\\nWe know we\u2019ve embarked on a challenging journey to turn Postgres into the data platform for Everything. But, we strongly believe that with the power of Postgres and its ecosystem, it\u2019s possible to replace most deployments of esoteric databases with just a flavor of Postgres and save developers a lot of time and effort.\\n\\nWe\u2019ll be building many stacks, benchmarking them against \u201ccompetitive\u201d solutions, and making sure Postgres grows to tackle these workloads. We\u2019ll have to optimize Postgres, support a wide variety of extensions, write several new extensions to close feature and performance gaps with other databases and also evolve Postgres. But, we have no doubt that we, along with the Postgres community can make this happen!"},{"id":"optimizing-postgres-auto-vacuum","metadata":{"permalink":"/blog/optimizing-postgres-auto-vacuum","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-08-31-tuning-autovacuum/index.md","source":"@site/blog/2023-08-31-tuning-autovacuum/index.md","title":"Optimizing Postgres\'s Autovacuum for High-Churn Tables","description":"Database systems use various techniques to ensure transactionality and performance. For Postgres, this is called MVCC (Multi-Version Concurrency Control). MVCC allows Postgres to provide great performance even when multiple clients could be working with the same table concurrently.","date":"2023-08-31T00:00:00.000Z","formattedDate":"August 31, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"}],"readingTime":9.64,"hasTruncateMarker":false,"authors":[{"name":"Adam Hendel","title":"Founding Engineer","url":"https://github.com/ChuckHend","email":"noreply@tembo.io","imageURL":"https://github.com/chuckhend.png","key":"adam"}],"frontMatter":{"slug":"optimizing-postgres-auto-vacuum","title":"Optimizing Postgres\'s Autovacuum for High-Churn Tables","authors":["adam"],"tags":["postgres"]},"prevItem":{"title":"Tembo Stacks: Making Postgres the Everything Database","permalink":"/blog/tembo-stacks-intro"},"nextItem":{"title":"Using pgmq with Python","permalink":"/blog/pgmq-with-python"}},"content":"Database systems use various techniques to ensure transactionality and performance. For Postgres, this is called MVCC (Multi-Version Concurrency Control). MVCC allows Postgres to provide great performance even when multiple clients could be working with the same table concurrently.\\n\\nIt is useful to be aware of Postgres\u2019 MVCC implementation to understand how Postgres manages a table\u2019s physical storage. Internally, Postgres refers to rows as \u201ctuples\u201d. And as a baseline, there are two big ideas to keep in mind about how Postgres implements changes to rows in a table:\\n\\n- An UPDATE operation in Postgres is equivalent to a DELETE of the previous tuple, plus an INSERT of the new one.\\n- A DELETE operation in Postgres does not cause the data to be removed from physical storage. It only causes it to be marked as deleted.\\n\\nThis is why Postgres has the [autovacuum](https://www.postgresql.org/docs/current/routine-vacuuming.html#AUTOVACUUM) process: It is the automatic process in charge of cleaning up and optimizing table storage for Postgres. You can follow this blog post with a local test environment of Postgres. I will demonstrate with code how MVCC and VACUUM features work, and how to tune the Auto-vacuum process.\\n\\n## Why does it matter?\\n\\nVacuum is not just about cleaning up storage space. In environments where data undergoes constant change, Postgres tables often experience an excessive amount of Inserts, Updates, and Deletes. This activity can lead to table bloat. Table bloat happens when a table\u2019s physical footprint far exceeds the size of the data that it actually holds.\\n\\nTable bloat is a condition that if not managed, will likely hamper performance of our database. And so, this takes us back to the autovacuum process: to extract its maximum benefits, you may need to fine-tune its settings.\\n\\nPostgres\u2019s default autovacuum settings are pretty good. If you\u2019re like me, it could have been years into your postgres journey before having a negative experience with bloat. However, when the time came I found it challenging to understand and tune these configuration settings. That\u2019s why we will study them safely in a dev environment.\\n\\n## Unraveling the Mystery of Bloat & Vacuum\'s Role\\n\\nPostgreSQL\'s mechanism for handling bloat is unique due to its adherence to MVCC. Contrary to immediate space reclamation after data is deleted or becomes obsolete, Postgres tags these rows as \\"dead\\", or \u201cdead tuples\u201d. However, even though they are dead they still occupy disk space and will degrade the performance of your queries. Many queries will continue to scan through these tuples, despite their \u201cdead\u201d status. The auto-vacuum steps in here, ensuring that these rows are removed and both the table and its associated indexes are streamlined for performance. You can\u2019t scan the dead tuples if they no longer exist!\\n\\nTo illustrate, let us create some bloat and see how it affects a rowcount query:\\n\\nCreate a table we can easily manipulate, and let\u2019s disable autovacuum so we can observe the consequences.\\n\\n```sql\\nCREATE TABLE bencher (\\n  record_id bigserial,\\n  updated_at timestamp with time zone\\n);\\nALTER TABLE bencher SET (autovacuum_enabled = false);\\nSELECT pg_reload_conf();\\n```\\n\\nNow insert 10 million rows of example data.\\n\\n```sql\\nINSERT INTO bencher(updated_at)\\nSELECT now()\\nFROM generate_series(1, 10000000);\\n```\\n\\nWe just created the table, and have only inserted records, so there are currently no dead tuples.\\n\\n```sql\\nSELECT\\n    schemaname,\\n    relname,\\n    n_dead_tup,\\n    n_live_tup\\nFROM pg_stat_user_tables\\nWHERE relname = \'bencher\';\\n\\n schemaname | relname | n_dead_tup | n_live_tup \\n------------+---------+------------+------------\\n public     | bencher |          0 |   10000000\\n\\n```\\n\\nLet\u2019s see how long it takes to get a rowcount with no bloat.\\n\\n```sql\\n\\\\timing\\n\\nselect * from bencher where record_id = 5000000;\\nrecord_id | updated_at\\n-----------+-------------------------------\\n5000000 | 2023-08-30 14:42:05.584778+00\\n(1 row)\\n\\nTime: 191.006 ms\\n```\\n\\nGreat, 191ms. Slow, yes but we have no indices because we\u2019re demonstrating bloat. Now lets create a bunch of dead tuples. This can take a minute or so.\\n\\n```sql\\nDO $$\\nDECLARE\\ni integer;\\nBEGIN\\nFOR i IN 1..5 LOOP\\n  UPDATE bencher SET updated_at = now();\\nEND LOOP;\\nEND $$;\\n```\\n\\nNow, lets see how long it takes to fetch the same record:\\n\\n```sql\\nselect * from bencher where record_id = 5000000;\\nrecord_id | updated_at\\n-----------+-------------------------------\\n5000000 | 2023-08-30 14:42:58.964919+00\\n(1 row)\\n\\nTime: 283.728 ms\\n```\\n\\nIt\u2019s getting closer to 300ms now. Let\u2019s check how many dead tuples are on the table.\\n\\n```sql\\nSELECT\\n    schemaname,\\n    relname,\\n    n_dead_tup,\\n    n_live_tup\\nFROM pg_stat_user_tables\\nWHERE relname = \'bencher\';\\n\\n\\nschemaname | relname | n_dead_tup | n_live_tup\\n------------+---------+------------+------------\\npublic | bencher | 50000000 | 10000000\\n```\\n\\nNow let\u2019s manually clean up the dead tuples and restore our query performance.\\n\\n```sql\\nVACUUM FULL bencher;\\n```\\n\\nAnd check the dead tuple count, there are no dead tuples.\\n\\n```sql\\nSELECT\\n    schemaname,\\n    relname,\\n    n_dead_tup,\\n    n_live_tup\\nFROM pg_stat_user_tables\\nWHERE\\n    n_dead_tup > 0 --needed to avoid division by zero\\nand relname = \'bencher\';\\n schemaname | relname | n_dead_tup | n_live_tup \\n------------+---------+------------+------------\\n public     | bencher |          0 |    10000000\\n```\\n\\nFinally, let\u2019s retrieve the record. Performance restored!\\n\\n```sql\\nselect * from bencher where record_id = 500000;\\nrecord_id | updated_at\\n-----------+-------------------------------\\n500000 | 2023-08-30 14:42:58.964919+00\\n(1 row)\\n\\nTime: 194.101 ms\\n```\\n\\n## Setting up a benchmarking environment\\n\\nThe rest of the examples will be run on Postgres in [Tembo Cloud](https://cloud.tembo.io). We\u2019ll use 8 vcore and 16Gb of memory and execute all the `psql` and `pgbench` commands from an EC2 instance within the same region as Postgres.\\n\\nLet\u2019s set up a script that will create an absurdly large amount of churn on our table and be able to execute it with `pgbench`. For every iteration, let\u2019s insert a row to our \u201cbencher\u201d table. Then, let\u2019s read and update a single record. Finally, let\u2019s delete the same record. This will create a situation similar to many queue implementations ([like PGMQ](https://github.com/tembo-io/pgmq)), where there are at least 2 transactions for every 1 insert. Additionally, the total record count on the table will typically be low - for every record we insert, we also delete one.\\n\\nThis creates a situation where a table consists of primarily dead tuples!\\n\\n```sql\\n-\u2013 churn.sql\\nDO $$\\nDECLARE\\nrec_id INT;\\nBEGIN\\n    INSERT INTO bencher(updated_at)\\nSELECT now();\\n\\n-- read and update a row\\nWITH cte AS\\n(\\n    SELECT record_id\\n    FROM bencher\\n    WHERE updated_at < now()\\n    ORDER BY record_id ASC\\n    LIMIT 1\\nFOR UPDATE SKIP LOCKED\\n)\\n\\n\\nUPDATE bencher\\nSET\\n    updated_at = now()\\nWHERE record_id in (select record_id from cte)\\nRETURNING record_id INTO rec_id;\\n\\n\\n-- Delete the row with the returned ID\\nDELETE\\nFROM bencher\\nWHERE record_id = rec_id;\\nEND $$;\\n```\\n\\nSet Postgres to all the default vacuum configurations;\\n\\n```sql\\nALTER SYSTEM SET autovacuum_vacuum_scale_factor = 0.2;\\nALTER SYSTEM SET autovacuum_vacuum_cost_delay = \'20ms\';\\nALTER SYSTEM SET autovacuum_vacuum_cost_limit = \'-1\';\\nALTER SYSTEM SET vacuum_cost_limit = 200;\\nALTER SYSTEM SET autovacuum_naptime = \'1min\';\\nSELECT pg_reload_conf();\\n```\\n\\nLet\u2019s run a benchmark to get a baseline. We will reuse this benchmark through the process.\\n\\n```sh\\npgbench \'postgresql://postgres:pw@host:5432\' -c 100 -j 1 -P 1 -T 300 -r -f churn.sql\\n```\\n\\n![default](default.png \\"default\\")\\n\\nAverage latency is about 3.4 ms. We are benchmarking an expensive set of queries, and you\u2019ve probably noticed the sawtooth pattern in the plot and a high standard deviation relative to the latency. This is a symptom of bloat accumulating on our table. Query latency grows until the vacuum process clears dead tuples, and then grows once again. This also has an inverse impact on transactions per second (TPS). Ideally we can reduce and provide some stability to latency.\\n\\n## Balancing Vacuum Delay for Optimal System Performance\\n\\nVacuuming is indispensable. However, it is not free and if left unchecked, it can burden your system. The balance lies in `autovacuum_vacuum_cost_delay` and `autovacuum_vacuum_cost_limit`. `autovacuum_vacuum_cost_delay` is the amount of time that the autovacuum process will halt processing when the `autovacuum_vacuum_cost_limit` is reached. Imagine this series of events - a table reaches 10% bloat, meaning 10% of the tuples are dead. When the 10% threshold is reached, the autovacuum worker begins to work and starts accruing cost. When that cost reaches `autovacuum_vacuum_cost_limit`, it will pause for the duration specified by `autovacuum_vacuum_cost_delay`, and then continue working until it is complete.\\n\\nModifying these can craft the perfect balance between seamless vacuuming and system efficiency. Let\u2019s increase the cost limit to the max, and reduce the delay by  half. This will let the autovacuum process run longer and pause for a shorter period of time when it does reach the cost limit, to ideally reduce bloat faster and reduce query latency.\\n\\n```sql\\nALTER SYSTEM SET autovacuum_vacuum_cost_delay = \'10ms\';\\nALTER SYSTEM SET autovacuum_vacuum_cost_limit = 10000;\\nSELECT pg_reload_conf();\\n```\\n\\n![delay_cost_limit](delay_limit.png \\"delay-cost-limit\\")\\n\\nWe have a slight reduction in average latency, but we can still see that the obviously grows in latency over time and decrease in TPS. It clears roughly every 60 seconds.\\n\\n## Fine-Tuning Auto Vacuum Scale Factors\\n\\nIn the previous example, we manually vacuumed our table. But postgres gives us an automated way to configure the vacuum process. One of the most critical parameters is the `autovacuum_vacuum_scale_factor`; it denotes the portion of the table size that, when surpassed by \\"dead\\" rows, prompts a vacuum action. For tables that see frequent data changes, it might be beneficial to lessen this value.\\n\\n```sql\\nALTER SYSTEM SET autovacuum_vacuum_scale_factor = 0.1;\\nSELECT pg_reload_conf();\\n```\\n\\n![scale-factor](scale_factor.png \\"scale-factor\\")\\n\\nReducing the scale factor had minimal impact on our result, so allowing the autovacuum to trigger sooner did not help. We can see that the period of the sawtooth pattern is still about 60 seconds, which means there we are probably limited by `autovacuum_naptime`, which we\'ll talk about next.\\n\\n## A Quick Siesta for Your System\\n\\nThe autovacuum_naptime parameter in Postgres specifies the minimum delay between autovacuum runs on any given database. The default (which we set earlier) is `1min`. Generally, depending on just how high-churn your workloads are, it might be necessary to decrease this value, whereas a longer interval could be suited for environments that are not churning at such a high rate. But our table has a crazy amount of churn.\\n\\nWe want to reduce the height of the latency peaks. One way to do this is to make the vacuum more aggressive and tell it to run sooner. We tried to influence that by setting the `autovacuum_vacuum_scale_factor`, but we can also lower the `autovacuum_naptime` value, which will also allow it to run sooner. Let\u2019s cut it in half.\\n\\n```sql\\nALTER SYSTEM SET autovacuum_naptime = \'30s\';\\nSELECT pg_reload_conf();\\n```\\n\\n![naptime](naptime.png \\"naptime\\")\\n\\nAllowing the autovacuumer to run more frequently reduced our average latency and increase TPS. However, we\u2019re still seeing a noticeable sawtooth pattern and high standard deviation of latency. Let\u2019s completely disable the cost limitations to the vacuum process, let it have as much compute as it needs.\\n\\n```sql\\nALTER SYSTEM SET autovacuum_vacuum_cost_delay = \'0\';\\nSELECT pg_reload_conf();\\n```\\n\\n![disable-cost](disable_cost.png \\"disable-cost\\")\\n\\nFinally, reduce naptime to 10s\\n\\n```sql\\nALTER SYSTEM SET autovacuum_naptime = \'10s\';\\nSELECT pg_reload_conf();\\n```\\n\\n![naptime-final](naptime_final.png \\"naptime-final\\")\\n\\nOverall, we\u2019ve iterated on autovacuum settings and reduced the average latency from 3.4ms to 2.8ms and stddev from 0.8ms to 0.7ms, which helped increase TPS from 4.3k to about 5.3k.\\n\\nConfiguring the autovacuum settings can be a lot of fun and the appreciated values are wildly dependent on the workload. We covered the absurdly high churn use case on a single-table today, which is very similar to what we see when running applications using PGMQ. Vacuum is complicated and can be [tuned differently](https://www.enterprisedb.com/blog/postgresql-vacuum-and-analyze-best-practice-tips) when considering multiple tables with different workloads. Other OLTP use cases will call for different settings, and OLAP workloads may be less influenced by the vacuum settings . Follow us, and sign up for [Tembo Cloud](https://cloud.tembo.io) because we will surely be writing about these other topics soon.\\n\\n## More on this topic\\n\\nWatch the video on [Optimizing autovacuum: PostgreSQL\u2019s vacuum cleaner by Samay Sharma](https://www.youtube.com/watch?v=D832gi8Qrv4)."},{"id":"pgmq-with-python","metadata":{"permalink":"/blog/pgmq-with-python","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-08-24-pgmq_with_python/index.md","source":"@site/blog/2023-08-24-pgmq_with_python/index.md","title":"Using pgmq with Python","description":"In my recent search for something interesting to do with Rust, I discovered that people write postgres extensions using pgrx.","date":"2023-08-24T00:00:00.000Z","formattedDate":"August 24, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"pgmq","permalink":"/blog/tags/pgmq"},{"label":"python","permalink":"/blog/tags/python"}],"readingTime":4.34,"hasTruncateMarker":false,"authors":[{"name":"Binidxaba","title":"Community contributor","url":"https://github.com/binidxaba","email":"noreply@tembo.io","imageURL":"https://github.com/binidxaba.png","key":"rjzv"}],"frontMatter":{"slug":"pgmq-with-python","title":"Using pgmq with Python","authors":["rjzv"],"tags":["postgres","pgmq","python"]},"prevItem":{"title":"Optimizing Postgres\'s Autovacuum for High-Churn Tables","permalink":"/blog/optimizing-postgres-auto-vacuum"},"nextItem":{"title":"Introducing pg_later: Asynchronous Queries for Postgres, Inspired by Snowflake","permalink":"/blog/introducing-pg-later"}},"content":"In my recent search for something interesting to do with Rust, I discovered that people write postgres extensions using [pgrx](https://github.com/pgcentralfoundation/pgrx).\\n\\nI found that very cool, and while looking for some real-world examples to study and dissect, I came across [pgmq](https://tembo.io/blog/introducing-pgmq): \\"A lightweight message queue. Like AWS SQS and RSMQ but on Postgres.\\" So, I decided to give it a shot so that perhaps in the future, I can contribute to the project ;)\\n\\nWhen reviewing the [repository](https://github.com/tembo-io/pgmq), I noticed a [Python client](https://github.com/tembo-io/pgmq/tree/main/tembo-pgmq-python) to interact with pgmq and began to play with it.\\n\\nLet me quickly describe how easy it was for me to use it.\\n\\n\\n## Setting up the environment\\n\\nThe first step was to start Postgres with a docker container. You can check the [README](https://github.com/tembo-io/pgmq) for detailed instructions, but in summary, just run:\\n\\n```console\\ndocker run -d --name postgres -e POSTGRES_PASSWORD=postgres -p 5432:5432 quay.io/tembo/pgmq-pg:latest\\n```\\n\\nA quick test to make sure that Postgres is running:\\n\\n```console\\npsql postgres://postgres:postgres@0.0.0.0:5432/postgres\\n```\\n\\nAfter that, I simply installed the pgmq Python client in my virtual environment:\\n\\n```console\\npip install tembo-pgmq-python\\n```\\n\\nThat\'s all the setup that I needed.\\n\\n\\n## The PGMQueue class\\n\\nTo use the library, we need to instantiate a PGMQueue object and from that object we can call the methods described in the following table:\\n\\n\\n| SQL function                             | PGMQueue method                                                           | Description                                                                |\\n| ---------------------------------------- | ------------------------------------------------------------------------- | -------------------------------------------------------------------------- |\\n| pgmq\\\\_create(queue)                     | create\\\\_queue(self, queue: str)   | Creates a new queue with the name queue.                                 |\\n| pgmq\\\\_send(queue, message)            | send(self, queue: str, message: dict, delay: Optional[int] = None) | Appends a message to the queue.                                        |\\n| pgmq\\\\_read(queue, vt, num\\\\_messages) | read(self, queue: str, vt: Optional[int] = None) | Reads num\\\\_messages from queue and sets the visibility timeout to vt. |\\n| pgmq\\\\_archive(queue, msg\\\\_id)          | archive(self, queue: str, msg\\\\_id: int) | Archives the message with msg\\\\_id.                                        |\\n| pgmq\\\\_pop(queue)                        | pop(self, queue: str) | Pop the next message in the queue.                                       |\\n| pgmq\\\\_delete(queue, msg\\\\_id)           | delete(self, queue: str, msg\\\\_id: int) | Deletes the message with msg\\\\_id from the queue.                        |\\n| pgmq\\\\_drop\\\\_queue(queue)                 | Not available yet | Drops the queue. | \\n\\nNext, let me show you how to implement a simple producer/consumer setup using the methods above.\\n\\n\\n## Implementing a Producer\\n\\nIn summary, the required steps are:\\n1. Import the Messages and PGMQueue classes.\\n2. Instantiate a PGMQueue object.\\n3. Create a queue.\\n4. Send N messages via the queue in a loop.\\n\\n```py\\nfrom tembo_pgmq_python import Message, PGMQueue\\n\\nqueue = PGMQueue(**connection_info)\\nqueue.create_queue(test_queue)\\n...\\n\\nfor x in range(num_messages):\\n    ...\\n    msg_id = queue.send(test_queue, test_message)\\n    ...\\n```\\n\\nThe PGMQueue constructor is the one that receives the connection information. For the Postgres instance initiated by the docker container, the connection details are:\\n\\n```py\\nqueue = PGMQueue(host=\\"localhost\\",\\n                port=5432,\\n                username=\\"postgres\\",\\n                password=\\"postgres\\",\\n                database=\\"postgres\\")\\n```\\n\\n\\n## Implementing a Consumer\\n\\nIn short, the code should basically do:\\nImport the Messages and PGMQueue classes.\\nConsume the messages from the queue in a loop.\\n\\n```py\\nfrom tembo_pgmq_python import Message, PGMQueue\\n\\n...\\nqueue = PGMQueue(**connection_info)\\n...\\n\\nwhile True:\\n    ...\\n    message: Message = queue.pop(queue_name)\\n    ...\\n```\\n\\n## Harnessing Producer and Consumer\\n\\nFor simplicity, I used a simple shell script to initiate my experiment:\\n\\n```shell\\n#/bin/bash\\n\\n# Spawn one producer\\npython3 producer.py > /tmp/producer.out &\\nsleep 2\\n\\n\\n# Spawn 5 consumers\\nfor i in $(seq 1 5)\\ndo\\n  python3 consumer.py > /tmp/consumer_${i}.out &\\ndone\\n\\n# Wait for everyone to finish\\nwait\\n```\\n\\nThe script basically starts 1 producer and 5 consumers in the background. The output is saved in the `/tmp` directory.\\n\\nAnd that was it...\\n\\nFrom this point, you can explore the other available methods.\\n\\n\\n## Some final words...\\n\\nIt was a pleasant surprise how easy it was to create this example: only a couple of shell commands and a couple of short Python scripts. The PGMQueue methods were very intuitive and straightforward. Personally, my next step is to understand how it works internally. But that\'s a topic for the future :)\\n\\nI invite everyone to explore this project at: https://github.com/tembo-io/pgmq. Give it a star and also check out the other available clients for [Rust](https://github.com/tembo-io/pgmq/tree/main/core) and [Go](https://github.com/craigpastro/pgmq-go).\\n\\n\\n## Appendix\\n\\nHere is the complete code if you want to give it a try (or see it in [this repository](https://github.com/binidxaba/pgmq-with-python)):\\n\\n```py\\n\\"\\"\\"\\nThis is the Producer\'s code\\n\\"\\"\\"\\n\\nimport random\\nimport string\\nfrom tembo_pgmq_python import Message, PGMQueue\\n\\n\\nif __name__ == \'__main__\':\\n    host = \\"localhost\\"\\n    port = 5432\\n    username = \\"postgres\\"\\n    password = \\"postgres\\"\\n    database = \\"postgres\\"\\n\\n    num_messages = 100000\\n    test_queue = \\"bench_queue_sample\\"\\n\\n    queue = PGMQueue(host=host,\\n                    port=port,\\n                    username=username,\\n                    password=password,\\n                    database=database)\\n\\n    try:\\n        #queue.drop_queue(test_queue)\\n        queue.create_queue(test_queue)\\n\\n        for x in range(num_messages):\\n\\n            payload = \'\'.join(random.choices(string.ascii_uppercase + string.digits, k = 10))\\n            msg = {\\"payload\\": payload}\\n            msg_id = queue.send(test_queue, msg)\\n\\n            if (x+1) % 1000 == 0:\\n                print(\\"Sent {} messages\\".format(x + 1))\\n\\n    except Exception as e:\\n        print(f\\"{e}\\")\\n```\\n\\n```py\\n\\"\\"\\"\\nThis is the Consumer\'s code\\n\\"\\"\\"\\n\\nimport random\\nimport time\\nfrom tembo_pgmq_python import Message, PGMQueue\\n\\n\\nif __name__ == \'__main__\':\\n    host = \\"localhost\\"\\n    port = 5432\\n    username = \\"postgres\\"\\n    password = \\"postgres\\"\\n    database = \\"postgres\\"\\n\\n    no_message_timeout = 0\\n    test_queue = \\"bench_queue_sample\\"\\n\\n    queue = PGMQueue(host=host, port=port, username=username, password=password, database=database)\\n\\n    while no_message_timeout < 5:\\n        try:\\n            message: Message = queue.pop(test_queue)  # type: ignore\\n            print(\\"Consumed message: {}\\".format(message.message[\\"payload\\"]))\\n            no_message_timeout = 0\\n\\n        except IndexError:\\n            no_message_timeout += 1\\n            print(\\"No more messages for {no_message_timeout} consecutive reads\\")\\n            time.sleep(0.500)\\n\\n```"},{"id":"introducing-pg-later","metadata":{"permalink":"/blog/introducing-pg-later","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-08-16-introducing-pg-later/index.md","source":"@site/blog/2023-08-16-introducing-pg-later/index.md","title":"Introducing pg_later: Asynchronous Queries for Postgres, Inspired by Snowflake","description":"We\u2019re working on asynchronous query execution in Postgres and have packaged the work up in an extension we\u2019re calling pglater. If you\u2019ve used Snowflake\u2019s asynchronous queries, then you might already be familiar with this type of feature. Submit your queries to Postgres now, and come back later and get the query\u2019s results.","date":"2023-08-16T00:00:00.000Z","formattedDate":"August 16, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"announcement","permalink":"/blog/tags/announcement"},{"label":"async","permalink":"/blog/tags/async"}],"readingTime":4.095,"hasTruncateMarker":false,"authors":[{"name":"Adam Hendel","title":"Founding Engineer","url":"https://github.com/ChuckHend","email":"noreply@tembo.io","imageURL":"https://github.com/chuckhend.png","key":"adam"}],"frontMatter":{"slug":"introducing-pg-later","title":"Introducing pg_later: Asynchronous Queries for Postgres, Inspired by Snowflake","authors":["adam"],"tags":["postgres","announcement","async"]},"prevItem":{"title":"Using pgmq with Python","permalink":"/blog/pgmq-with-python"},"nextItem":{"title":"Introducing PGMQ: Simple Message Queues built on Postgres","permalink":"/blog/introducing-pgmq"}},"content":"We\u2019re working on asynchronous query execution in Postgres and have packaged the work up in an extension we\u2019re calling [pg_later](https://github.com/tembo-io/pg_later). If you\u2019ve used [Snowflake\u2019s asynchronous queries](https://docs.snowflake.com/developer-guide/python-connector/python-connector-example#examples-of-asynchronous-queries), then you might already be familiar with this type of feature. Submit your queries to Postgres now, and come back later and get the query\u2019s results.\\n\\nVisit [pg_later](https://github.com/tembo-io/pg_later)\u2019s Github repository and give it a star!\\n\\n![elephant-tasker](elephant.png \\"elephant-tasker\\")\\n\\n## Why async queries?\\n\\nImagine that you\u2019ve initiated a long-running maintenance job. You step away while it is executing, only to come back and discover it was interrupted hours ago due to your laptop shutting down. You don\u2019t want this to happen again, so you spend some time googling or asking your favorite LLM how to run the command in the background with screen or tmux. Having asynchronous query support from the beginning would have saved you a bunch of time and headache!\\n\\nAsynchronous processing is a useful development pattern in software engineering. It has advantages such as improved resource utilization, and unblocking of the main execution thread.\\n\\nSome examples where async querying can be useful are:\\n\\n* For a DBA running ad-hoc maintenance.\\n* Developing in interactive environments such as a Jupyter notebook. Rather than submit a long-running query only to have your notebook hang idly and then crash, you can use asynchronous tasks to avoid blocking your Notebook, or simply come back later to check on your task.\\n* Having a long-running analytical query, for example fulfilling an ad-hoc request like seeing how many new users signed up each day for the past month. You can submit that query and have it run in the background while you continue other work.\\n\\n## Extending Postgres with async features\\n\\nAt Tembo, we\u2019ve built a similar feature for Postgres and published it as an extension called **pg_later**. With **pg_later**, you can dispatch a query to your Postgres database and, rather than waiting for the results, your program can return and retrieve the results at your convenience.\\n\\nA common example is manually executing VACUUM on a table. Typically one might execute VACUUM in one session, and then use another session to check the status of the VACUUM job via `pg_stat_progress_vacuum`. pg_later gives you the power to do that in a single session. You can use it to queue up any long-running analytical or administrative task on your Postgres database.\\n\\n## Stacking Postgres Extensions\\n\\n**pg_later** is built on top of [PGMQ](https://tembo.io/blog/introducing-pgmq), another one of Tembo\'s open source extensions. Once a user submits a query, **pg_later** seamlessly enqueues the request in a Postgres-managed message queue. This mechanism then processes the query asynchronously, ensuring no unnecessary wait times or hold-ups.\\n\\nThe **pg_later** [background worker](https://www.postgresql.org/docs/current/bgworker.html) picks up the query from the queue and executes it. The results are persisted by being written to a table as [JSONB](https://www.postgresql.org/docs/current/functions-json.html) and can be easily retrieved using the **pg_later** API. You can simply reference the unique job id given upon query submission, and retrieve the result set, or query the table directly. By default, the results are retained forever, however we are building retention policies as a feature into **pg_later**.\\n\\n![diagram](diagram.png \\"diagram\\")\\n\\n## Using pg_later\\n\\nTo get started, check out our project\u2019s [README](https://github.com/tembo-io/pg_later/blob/main/README.md) for a guide on installing the extension.\\n\\nFirst, you need to initialize the extension. This handles the management of PGMQ objects like a job queue and some metadata tables.\\n\\n```sql\\nselect pglater.init();\\n```\\n\\nYou\'re now set to dispatch your queries. Submit the query using pglater.exec, and be sure to take note of the **job_id** that is returned. In this case, it\u2019s the first job so the **job_id** **is 1**.\\n\\n```sql\\nselect pglater.exec(\\n  \'select * from pg_available_extensions limit 2\'\\n) as job_id;\\n```\\n\\n```text\\n job_id \\n--------\\n     1\\n(1 row)\\n```\\n\\nAnd whenever you\'re ready, your results are a query away:\\n\\n```sql\\nselect pglater.fetch_results(1);\\n```\\n\\n```text\\n{\\n  \\"query\\": \\"select * from pg_available_extensions limit 2\\",\\n  \\"job_id\\": 1,\\n  \\"result\\": [\\n    {\\n      \\"name\\": \\"pg_later\\",\\n      \\"comment\\": \\"pg_later:  Run queries now and get results later\\",\\n      \\"default_version\\": \\"0.0.6\\",\\n      \\"installed_version\\": \\"0.0.6\\"\\n    },\\n    {\\n      \\"name\\": \\"pgmq\\",\\n      \\"comment\\": \\"Distributed message queues\\",\\n      \\"default_version\\": \\"0.10.1\\",\\n      \\"installed_version\\": \\"0.10.1\\"\\n    }\\n  ],\\n  \\"status\\": \\"success\\"\\n}\\n```\\n\\n## Up next\\n\\n`pg_later` is a new project and still under development. A few features that we are excited to build:\\n\\n* Status and progress of in-flight queries\\n* Security and permission models for submitted queries\\n* Cursor support for finished jobs (fetch results row by row)\\n* Kill a query that is in the queue or is currently in flight\\n* Support for transactions\\n* Configurable concurrency levels for background works to increase the throughput of jobs\\n* Push notifications for completed and failed jobs\\n* Retention policies for completed jobs\\n\\nGive us a [star](https://github.com/tembo-io/pg_later) and try out pg_later by running the example in the README. If you run into issues, please create an [issue](https://github.com/tembo-io/pg_later/issues). We would greatly welcome contributions to the project as well.\\n\\nPlease stay tuned for a follow up post on benchmarking PGMQ vs leading open source message queues."},{"id":"introducing-pgmq","metadata":{"permalink":"/blog/introducing-pgmq","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-08-03-introducing-pgmq/index.md","source":"@site/blog/2023-08-03-introducing-pgmq/index.md","title":"Introducing PGMQ: Simple Message Queues built on Postgres","description":"We\u2019ve released PGMQ, a packaged extension for message queues on Postgres.","date":"2023-08-03T00:00:00.000Z","formattedDate":"August 3, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"announcement","permalink":"/blog/tags/announcement"},{"label":"queues","permalink":"/blog/tags/queues"}],"readingTime":4.345,"hasTruncateMarker":false,"authors":[{"name":"Adam Hendel","title":"Founding Engineer","url":"https://github.com/ChuckHend","email":"noreply@tembo.io","imageURL":"https://github.com/chuckhend.png","key":"adam"}],"frontMatter":{"slug":"introducing-pgmq","title":"Introducing PGMQ: Simple Message Queues built on Postgres","authors":["adam"],"tags":["postgres","announcement","queues"]},"prevItem":{"title":"Introducing pg_later: Asynchronous Queries for Postgres, Inspired by Snowflake","permalink":"/blog/introducing-pg-later"},"nextItem":{"title":"Tembo Manifesto","permalink":"/blog/tembo-manifesto"}},"content":"We\u2019ve released [PGMQ](https://github.com/tembo-io/pgmq), a packaged extension for message queues on Postgres. \\n\\nPeople have been implementing queues on Postgres in many different ways and we\u2019re excited about combining lessons learned from those projects into a simple, feature-rich extension. \\n\\nSome exciting features of the project include:\\n\\n* Guaranteed exactly-once delivery of messages within a visibility timeout\\n* Optional archival of messages retention for replayability and retention\\n* Familiar SQL interface\\n* Single and Batch read of messages\\n* Client SDKs in both Rust and Python for an ORM-like feel\\n\\n## The need for message queues\\n\\nMessage queues are a very common architectural feature to manage operational pipelines, particularly within the context of asynchronous tasks and distributed systems. There are products in the market that support message queues (Kafka, RSMQ, RabbitMQ, SQS); however, when adopting one of these technologies, you increase your cognitive load, required skills and production support overhead.\\n\\n## Building your queues on Postgres\\n\\nAs a Postgres startup, we had the same issue, and we decided to build our own Message Queue based on Postgres. We are not the first: others have implemented queues on Postgres, and many have written about it including [Dagster](https://dagster.io/blog/skip-kafka-use-postgres-message-queue), [CrunchyData](https://www.crunchydata.com/blog/message-queuing-using-native-postgresql), and [PostgresFM](https://postgres.fm/episodes/queues-in-postgres) dedicated an entire podcast episode to them.\\n\\nAt Tembo, we needed a job queue to manage tasks between our control-plane and data-plane in our managed cloud offering. Our control-plane publishes tasks like `create postgres cluster`, and `update postgres cluster`. \\n\\nTo keep our architecture simple and reduce technology sprawl, we built a Postgres extension so that we could run queues for our cloud and more easily share the implementation with the community.\\n\\n## Queues Implemented with best practices\\n\\nPGMQ was implemented on Postgres and follows industry best practices. One of the most important practices is the use of Postgres\u2019s [SKIP LOCKED](https://www.2ndquadrant.com/en/blog/what-is-select-skip-locked-for-in-postgresql-9-5/), which is similar to `NOWAIT` in other databases. `SKIP LOCKED` helps ensure that consumers don\'t hang and `FOR UPDATE` ensures messages are not duplicated on read. PGMQ also supports partitioning, which is particularly beneficial for large queues and can be used to efficiently archive / expire old messages.\\n\\nPGMQ also provides exactly once delivery semantics within a visibility timeout. Similar to Amazon\u2019s SQS and RSMQ, PGMQ consumers set the period of time during which Postgres will prevent all consumers from receiving and processing a message. This is done by the consumer on read, and once the visibility timeout expires the message becomes available for consumption once again. That way, if a consumer crashes, there is no data loss. This effectively means at-least-once delivery semantics once the first visibility timeout has expired.\\n\\n![vt](vt.png \\"VisibilityTimeout\\")\\n\\n## Using PGMQ\\n\\nTo get started, check out our project\u2019s [README](https://github.com/tembo-io/pgmq/blob/main/README.md#installation) for a guide on installing the extension.\\n\\nYou can create a new queue by simply calling\\n\\n```sql\\nSELECT pgmq_create(\'my_queue\');\\n```\\n\\nThen, pgmq_send() a couple messages to the queue. The message id is returned from the send() function.\\n\\n```sql\\nSELECT * from pgmq_send(\'my_queue\', \'{\\"foo\\": \\"bar1\\"}\');\\nSELECT * from pgmq_send(\'my_queue\', \'{\\"foo\\": \\"bar2\\"}\');\\n```\\n\\n```text\\n pgmq_send\\n-----------\\n         1\\n(1 row)\\n\\n pgmq_send\\n-----------\\n         2\\n(1 row)\\n```\\n\\nRead `2` messages from the queue. Make them invisible for `30` seconds. If the messages are not deleted or archived within 30 seconds, they will become visible again and can be read by another consumer.\\n\\n```sql\\nSELECT * from pgmq_read(\'my_queue\', 30, 2);\\n```\\n\\n```text\\n msg_id | read_ct |              vt               |          enqueued_at          |    message\\n--------+---------+-------------------------------+-------------------------------+---------------\\n      1 |       1 | 2023-02-07 04:56:00.650342-06 | 2023-02-07 04:54:51.530818-06 | {\\"foo\\":\\"bar1\\"}\\n      2 |       1 | 2023-02-07 04:56:00.650342-06 | 2023-02-07 04:54:51.530818-06 | {\\"foo\\":\\"bar2\\"}\\n```\\n\\nIf the queue is empty, or if all messages are currently invisible, no rows will be returned.\\n\\n```sql\\nSELECT * from pgmq_read(\'my_queue\', 30, 1);\\n```\\n\\n```text\\n msg_id | read_ct | vt | enqueued_at | message\\n--------+---------+----+-------------+---------\\n```\\n\\n`Archiving` removes the message from the queue and inserts it to the queue\u2019s archive table. This provides you with an opt-in retention mechanism for messages, and is an excellent way to debug applications.\\n\\nArchive the message with id 2.\\n\\n```sql\\nSELECT * from pgmq_archive(\'my_queue\', 2);\\n```\\n\\nThen inspect the message on the archive table.\\n\\n```sql\\nSELECT * from pgmq_my_queue_archive;\\n```\\n\\n```text\\n msg_id | read_ct |         enqueued_at          |          deleted_at           |              vt               |     message     \\n--------+---------+------------------------------+-------------------------------+-------------------------------+-----------------\\n      2 |       1 | 2023-04-25 00:55:40.68417-05 | 2023-04-25 00:56:35.937594-05 | 2023-04-25 00:56:20.532012-05 | {\\"foo\\": \\"bar2\\"}```\\n```\\n\\nAlternatively, you can delete a message forever.\\n\\n```sql\\nSELECT * from pgmq_send(\'my_queue\', \'{\\"foo\\": \\"bar3\\"}\');\\n```\\n\\n```text\\n pgmq_send\\n-----------\\n         3\\n(1 row)\\n```\\n\\n```sql\\nSELECT pgmq_delete(\'my_queue\', 3);\\n```\\n\\n```text\\n pgmq_delete\\n-------------\\n t\\n ```\\n\\n## Getting involved\\n\\nGive us a [star](https://github.com/tembo-io/pgmq) and try out PGMQ by cloning the [repo](https://github.com/tembo-io/pgmq) and following the example in the README. Please use Github issues if you run into any issues or have any feedback. \\n\\nWe\u2019ve also built client side libraries in [Rust](https://github.com/tembo-io/pgmq/tree/main/core) and [Python](https://github.com/tembo-io/pgmq/tree/main/tembo-pgmq-python), which will give you an ORM-like experience.\\n\\nYou can also try PGMQ on [Tembo Cloud](https://tembo.io/) as part of our [Message Queue Stack](https://tembo.io/docs/stacks/message-queue). Tembo Cloud\u2019s Message Queue Stack is powered by PGMQ, but also ships with Postgres configurations optimized for message queue workloads. We\u2019re also working on adding metrics and data visualizations specific to message queues.\\n\\n## Interested in learning more? \\n\\nStay tuned for our upcoming post [pg_later](https://github.com/tembo-io/pg_later), an extension we built on top of PGMQ as well as benchmarks comparing PGMQ to [SQS](https://aws.amazon.com/sqs/) and [Redis](https://redis.com/)."},{"id":"tembo-manifesto","metadata":{"permalink":"/blog/tembo-manifesto","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-07-05-tembo-manifesto/index.md","source":"@site/blog/2023-07-05-tembo-manifesto/index.md","title":"Tembo Manifesto","description":"tembo brand","date":"2023-07-05T00:00:00.000Z","formattedDate":"July 5, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"announcement","permalink":"/blog/tags/announcement"}],"readingTime":5.18,"hasTruncateMarker":false,"authors":[{"name":"Ry Walker","title":"Founder/CEO Tembo","url":"https://github.com/ryw","email":"noreply@tembo.io","imageURL":"https://github.com/ryw.png","key":"ryw"}],"frontMatter":{"slug":"tembo-manifesto","title":"Tembo Manifesto","authors":["ryw"],"tags":["postgres","announcement"]},"prevItem":{"title":"Introducing PGMQ: Simple Message Queues built on Postgres","permalink":"/blog/introducing-pgmq"},"nextItem":{"title":"Introducing Tembo","permalink":"/blog/introducing-tembo"}},"content":"![tembo brand](./tembo_brand.png)\\n\\n## $100 billion+\\n\\nThat\'s the expected size of the global database market in the coming years. The amount of data being generated, stored, and leveraged is growing exponentially, as is the need for applications to operate at a global scale. We\'re producing more data, we need it faster, and the uses for it are more and more complex by the day. \\n\\nThere\'s an *incredible* opportunity in the making here. But first, a little context. \\n\\n## The Database Market\\n\\nEnterprises typically store their data across various databases, generally grouped into transactional and analytical systems. There\'s roughly 10x more transactional than analytical data, mostly in Oracle, MySQL, and Postgres, and there have been some sizable shifts to this landscape in recent years:\\n\\n1. First, we saw some analytical workloads move to the cloud. This was the big data era, resulting in the rise of platforms like Snowflake and Databricks.\\n2. Next, some transactional workloads moved to streaming and real-time data. This required transactional and analytical processing platforms to be managed by app developers, not database admins.\\n3. Finally, application infrastructure has been abstracted, allowing developers to build and scale applications more efficiently. Services like Vercel and Netlify have streamlined the development lifecycle, but tend to build on top of databases rather than dealing with the databases themselves.\\n\\nThe net result? Exactly what we see around us\u2014an ever-expanding menagerie of innovative apps and services built upon an increasingly complex database ecosystem. The modern data stack is more complicated and expensive than it\'s ever been, and with the normalization of AI, the trend is accelerating. \\n\\nHow can we cope? \\n\\n## Postgres as a Platform\\n\\nDevelopers would love an open-source, multi-functional data platform to simplify their lives and work\u2014as long as it doesn\'t restrict their freedom. Companies want reliability, flexibility, and a pathway out of the spiraling costs and vendor lock-in attempts that they\'re currently saddled with. At face value, there\'s an obvious solution that ticks all the necessary boxes:\\n\\n**Postgres.**\\n\\nPostgres, the world\'s favorite database with millions of deployments, features a liberal OSS license and a large community. It efficiently manages SQL and JSON queries across diverse workloads due to its growing, decade-old ecosystem of add-ons and extensions. It is popular for its open-source, standards-compliant, extensible nature, and ACID compliance, making it a reliable, cost-effective system. It handles low latency, high throughput analytical cases, offering HTAP-lite capabilities through window functions and foreign data wrappers. Even better, it\u2019s extensibility has resulted in a wide ecosystem of add-ons and plugins for GIS data, image processing, vector databases, and more, with some extensions evolving into companies like CitusDB and Timescale. In short, everything you\u2019d want and then some.\\n\\n![Postgres is most admired and desired](postgres_is_admired_and_desired_stackoverflow.png)\\n\\n*Source: [Stack Overflow Developer Survey 2023](https://survey.stackoverflow.co/2023/#section-admired-and-desired-databases)*\\n\\nProblem solved, right? Not so fast. \\n\\n## Overwhelmment\\n\\n*(It\'s a word now. Go with it.)*\\n\\nCompanies are typically hesitant to adopt databases due to costs, complexity, and risk. The effort required to build, configure, and optimize a new system often makes the transition value negligible at best. For these companies (especially large enterprises that spend billions per year on a fragmented database architecture), Postgres and it\'s assimilation of database innovations *should* be an ideal solution. Open source, extensible, free from vendor lock in and ever-increasing costs\u2014it should be a no brainer. \\n\\n\u201cWhat\'s the holdup?\u201d you might say. Well, Postgres is... complicated. \\n\\nTo create a self-managed cluster of Postgres clusters, DBAs have to consider infrastructure, environment, security, data management, backups, and workload-specific tuning. Further, maintaining and scaling Postgres involves meeting high availability requirements, managing data storage, updating schemas, optimizing query performance, and managing failover protection and caching. Lastly, extensions exist to support additional functionality in Postgres but they are hard to discover, evaluate, certify and deploy.\\n\\nPut simply, Postgres is the solution.. *if* you can tame it and make it manageable. \\n\\n## Our Vision\\n\\nThis is where Tembo comes in. Tembo is the managed cloud to run Postgres and its entire ecosystem\u2014extensions, applications, tools, and more\u2014all within a single unified platform. It simplifies deploying Postgres with a virtualized runtime experience, enabling one-click migrations and access to the Postgres ecosystem. Developers can control the data model lifecycle, and deploy to multiple zones and clouds. Advanced options will include autoscaling, hybrid transactional and analytical processing (HTAP), and edge caching.\\n\\nAdditionally, Tembo invests in the Postgres extension ecosystem, aiming to standardize and simplify the use and creation of extensions. By unbundling and decoupling the database into services and abstraction layers, Tembo enables new simplicity and capability.\\n\\n## Tembo Cloud\\n\\nIt\'s Postgres the way it *should* be. With Tembo, you don\'t have to be a database expert to build an expert database. We are building a dev-first, fully-extensible, fully-managed, secure, and scalable Postgres service. Available on all clouds and bare metal providers, Tembo Cloud provides the largest curated library of easily-installed extensions, allowing our customers to expand their use cases of Postgres.\\n\\n![Organization home](org_home.png)\\n\\n![Instance home](instance_home.png)\\n\\n## Tembo Stacks\\n\\nIt\'s \u201cPostgres for Everything,\u201d and we mean it. Stacks accelerate your development by enabling you to quickly create and deploy custom-built \\"flavors\\" of Postgres + extensions that are tailor made for key enterprise needs. No need to spin up new databases and endure the pain and associated sprawl\u2014Stacks enable you to replace external, non-Postgres data services. \\n\\n## What Does This Mean?\\n\\nNo matter who you are or what you\'re trying to build, three things are true about Tembo:\\n\\n1. **True Managed Open Source**: You don\'t have to settle for a complex web of OSS data services **or** a restrictive, locked-in, expensive managed contract with one of the large cloud providers. Tembo is committed to making true open-source Postgres manageable and accessible.\\n2. **Developer Centric**: You *can* have the flexibility and control you\'ve dreamed of. Tembo is made by developers, for developers; we give you fast deployment, automatic migration, and a clear path to genuine value. We win when you win.\\n3. **Future Proof**: Postgres is the world\'s most developer beloved database. It isn\'t going anywhere, and with the constantly growing ecosystem of extensions and applications, it\'s only getting better. With Tembo, you get all the potential of that ever-growing network, right at the click of a button.\\n\\n**Tembo: It\'s everything you need to build anything on Postgres.**"},{"id":"introducing-tembo","metadata":{"permalink":"/blog/introducing-tembo","editUrl":"https://github.com/tembo-io/website/blob/main/blog/2023-04-18-introducing-tembo/index.md","source":"@site/blog/2023-04-18-introducing-tembo/index.md","title":"Introducing Tembo","description":"pink nodes","date":"2023-04-18T00:00:00.000Z","formattedDate":"April 18, 2023","tags":[{"label":"postgres","permalink":"/blog/tags/postgres"},{"label":"announcement","permalink":"/blog/tags/announcement"}],"readingTime":2.135,"hasTruncateMarker":false,"authors":[{"name":"Ry Walker","title":"Founder/CEO Tembo","url":"https://github.com/ryw","email":"noreply@tembo.io","imageURL":"https://github.com/ryw.png","key":"ryw"}],"frontMatter":{"slug":"introducing-tembo","title":"Introducing Tembo","authors":["ryw"],"tags":["postgres","announcement"]},"prevItem":{"title":"Tembo Manifesto","permalink":"/blog/tembo-manifesto"}},"content":"![pink nodes](./db_pink_nodes.png)\\n\\nHello, world :) We are thrilled to announce the launch of our new startup, Tembo, and our mission to build a game-changing managed Postgres service.\\n\\n## Background\\n\\nWhile wrapping up my involvement in my previous company, I learned about the rich Postgres extension ecosystem, and was drawn to an idea that these extensions represent a big opportunity to expand the use cases for Postgres\u2014maybe even collapse the modern data stack into a Postgres-centric platform, and eliminate a lot of unnecessary and expensive data movement.The vast majority of developers and companies primarily use \\"Vanilla\\" Postgres. This is partly because the current user experience of discovering, installing, and gaining proficiency in Postgres extensions is subpar\u2014especially compared to other ecosystems. This has been voiced by many community members. As such, most existing managed services offer only a sliver of the potential value that extensions could provide.\\n\\n## We\'ve started a company\\n\\nTembo\u2019s mission is to continuously improve the dev experience of Postgres, especially in the area of expanded use cases that rely on community extensions.Postgres is the most loved database by a long shot, and all Postgres users (including me) owe a huge debt of gratitude to all the companies and people who have invested into Postgres over the past 26 years. Our goal is to serve the Postgres community in an increasingly valuable manner.Our mission is to empower developers and businesses worldwide with the full power of Postgres and its extensions ecosystem, enabling users to use Postgres for a growing number of use cases.\\n\\n## We\'ve raised some money\\n\\nWe have raised a $6.5M seed round led by Venrock, with participation from CincyTech and Wireframe Ventures, and other influential tech angel investors, and we\'ve already got a team of 10 working on our SaaS product and related open-source projects.\\nWe\'ve been building a product\\n\\nWe are building what we hope will be a world-class managed Postgres service:\\n\\n* Use any extension, even custom or private extensions\\n* Provide UI for popular extensions, and a framework to include the community in those efforts\\n* Stacks - packaging of extensions, tools and configurations, to make it easy to get started using Postgres for specific new use cases\\n* An intelligent advisor that helps you to better use the features that you\u2019re already using in Postgres\\n\\n![create cluster](./create_cluster.jpg)\\n\\n![org home](./org_home.jpg)\\n\\nThe Tembo team is incredibly excited to share our platform with you as we enter our closed beta period, and support your journey towards data-driven success. \\n\\nWe invite you to try [Tembo Cloud](https://cloud.tembo.io), to access the future of database management."}]}')}}]);