"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8237],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>g});var r=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=r.createContext({}),p=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=p(e.components);return r.createElement(s.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),d=p(n),m=a,g=d["".concat(s,".").concat(m)]||d[m]||u[m]||o;return n?r.createElement(g,i(i({ref:t},c),{},{components:n})):r.createElement(g,i({ref:t},c))}));function g(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[d]="string"==typeof e?e:a,i[1]=l;for(var p=2;p<o;p++)i[p]=n[p];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}m.displayName="MDXCreateElement"},28375:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>l,toc:()=>p});var r=n(87462),a=(n(67294),n(3905));const o={sidebar_position:5},i="Tembo Enterprise LLM",l={unversionedId:"tembo-stacks/enterprise-llm",id:"tembo-stacks/enterprise-llm",title:"Tembo Enterprise LLM",description:"Machine learning significantly enhances the features and capabilities of applications. The Enterprise Machine Learning Stack is a Postgres cluster with the latest machine learning and large language model toolchains pre-installed and enabled.",source:"@site/docs/tembo-stacks/enterprise-llm.md",sourceDirName:"tembo-stacks",slug:"/tembo-stacks/enterprise-llm",permalink:"/docs/tembo-stacks/enterprise-llm",draft:!1,editUrl:"https://github.com/tembo-io/website/blob/main/docs/tembo-stacks/enterprise-llm.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5},sidebar:"tutorialSidebar",previous:{title:"Tembo OLAP",permalink:"/docs/tembo-stacks/olap"},next:{title:"Tembo Message Queue",permalink:"/docs/tembo-stacks/message-queue"}},s={},p=[{value:"Extensions",id:"extensions",level:2},{value:"Getting started",id:"getting-started",level:2},{value:"Setup",id:"setup",level:3},{value:"Create a vectorize job",id:"create-a-vectorize-job",level:3},{value:"Manually trigger embedding generation",id:"manually-trigger-embedding-generation",level:3},{value:"Search for similar products",id:"search-for-similar-products",level:3},{value:"Stopping the job",id:"stopping-the-job",level:3},{value:"Tuning performance with indexes",id:"tuning-performance-with-indexes",level:3}],c={toc:p},d="wrapper";function u(e){let{components:t,...n}=e;return(0,a.kt)(d,(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"tembo-enterprise-llm"},"Tembo Enterprise LLM"),(0,a.kt)("p",null,"Machine learning significantly enhances the features and capabilities of applications. The Enterprise Machine Learning Stack is a Postgres cluster with the latest machine learning and large language model toolchains pre-installed and enabled."),(0,a.kt)("h2",{id:"extensions"},"Extensions"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://pgt.dev/extensions/postgresml"},"postgresml")," - ",(0,a.kt)("inlineCode",{parentName:"li"},"pgml")," allows you to train and run machine learning models in Postgres. It supports a variety of models and algorithms, including linear regression, logistic regression, decision tree, random forest, and k-means clustering. It also provides hooks into HuggingFace for downloading and consuming pre-trained models and transformers."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://pgt.dev/extensions/pgvector"},"pgvector")," - ",(0,a.kt)("inlineCode",{parentName:"li"},"pgvector")," is a vector similarity search engine for Postgres. It is typically used for storing embeddings and then conducting vector search on that data."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://pgt.dev/extensions/pg_embedding"},"pg_embedding")," - an alternative to ",(0,a.kt)("inlineCode",{parentName:"li"},"pgvector")," that provides similar functionality."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("a",{parentName:"li",href:"https://pgt.dev/extensions/vectorize"},"pg_vectorize")," - an orchestration layer for embedding generation and store, vector search and index maintenance. It provides a simple interface for generating embeddings from text, storing them in Postgres, and then searching for similar vectors using ",(0,a.kt)("inlineCode",{parentName:"li"},"pgvector"),".")),(0,a.kt)("p",null,"The extensions listed above are all very flexible and support many use cases. Visit their documentation pages for additional details."),(0,a.kt)("h2",{id:"getting-started"},"Getting started"),(0,a.kt)("p",null,"We will build a simple vector search application using ",(0,a.kt)("inlineCode",{parentName:"p"},"pg_vectorize"),", which is powered by ",(0,a.kt)("a",{parentName:"p",href:"https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key"},"OpenAI")," and ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/pgvector/pgvector"},"pgvector"),"."),(0,a.kt)("h3",{id:"setup"},"Setup"),(0,a.kt)("p",null,"First, you will need to acquire an API key from ",(0,a.kt)("a",{parentName:"p",href:"https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key"},"OpenAI"),"."),(0,a.kt)("p",null,"Then, connect to your Tembo cluster:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sql"},"psql 'postgresql://postgres:<your-password>@<your-host>:5432/postgres'\n")),(0,a.kt)("p",null,"Create a table using the example dataset."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE products AS\nSELECT * FROM vectorize.example_products;\n")),(0,a.kt)("p",null,"The table contains products along with their descriptions. Our application will allow us to easily search the table."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT * FROM products limit 2;\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-text"}," product_id | product_name |                      description                       |        last_updated_at\n------------+--------------+--------------------------------------------------------+-------------------------------\n          1 | Pencil       | Utensil used for writing and often works best on paper | 2023-07-26 17:20:43.639351-05\n          2 | Laptop Stand | Elevated platform for laptops, enhancing ergonomics    | 2023-07-26 17:20:43.639351-05\n")),(0,a.kt)("h3",{id:"create-a-vectorize-job"},"Create a vectorize job"),(0,a.kt)("p",null,"Create a job to vectorize the products table. Give the job a name, we'll call it ",(0,a.kt)("inlineCode",{parentName:"p"},"product_search")," in this example. Specify the table's primary key (",(0,a.kt)("inlineCode",{parentName:"p"},"product_id"),") and the columns that we want to search (",(0,a.kt)("inlineCode",{parentName:"p"},"product_name")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"description"),")."),(0,a.kt)("p",null,"Provide the OpenAI API key for the job."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT vectorize.table(\n    job_name => 'product_search',\n    \"table\" => 'products',\n    primary_key => 'product_id',\n    columns => ARRAY['product_name', 'description'],\n    args => '{\"api_key\": \"your-openai-key\"}'\n);\n")),(0,a.kt)("p",null,"By default, this job will run to generate and update embeddings every minute based on the ",(0,a.kt)("inlineCode",{parentName:"p"},"last_updated_at")," column. This update process is triggered by a ",(0,a.kt)("inlineCode",{parentName:"p"},"pg_cron"),", which is setup for your automatically by ",(0,a.kt)("inlineCode",{parentName:"p"},"pg_vectorize"),". If there are updates to the ",(0,a.kt)("inlineCode",{parentName:"p"},"products")," table, the next job run will subsequently update the embeddings accordingly."),(0,a.kt)("p",null,"By default, this will add two columns to your table: ",(0,a.kt)("inlineCode",{parentName:"p"},"<job_name>_embeddings")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"<job_name>_updated_at"),"."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT column_name\nFROM information_schema.columns\nWHERE table_schema = 'public'\nAND table_name   = 'products';\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-text"},"        column_name\n---------------------------\n product_id\n product_name\n description\n last_updated_at\n product_search_embeddings  <--- embeddings\n product_search_updated_at  <--- embeddings last updated at\n")),(0,a.kt)("h3",{id:"manually-trigger-embedding-generation"},"Manually trigger embedding generation"),(0,a.kt)("p",null,"We can manually trigger the update, or wait for the cron job to do it."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sql"},"-- manually trigger the refresh\nSELECT vectorize.job_execute('product_search');\n")),(0,a.kt)("h3",{id:"search-for-similar-products"},"Search for similar products"),(0,a.kt)("p",null,"Now that we have generated embeddings for our products, we can search for similar products using the extension."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT * FROM vectorize.search(\n    job_name => 'product_search',\n    return_col => 'product_name',\n    query => 'accessories for mobile devices',\n    api_key => 'your-openai-key\"',\n    num_results => 3\n);\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-text"},'                                          search_results\n--------------------------------------------------------------------------------------------------\n {"value": "Phone Charger", "column": "product_name", "similarity_score": 0.8530797672121025}\n {"value": "Tablet Holder", "column": "product_name", "similarity_score": 0.8284493388477342}\n {"value": "Bluetooth Speaker", "column": "product_name", "similarity_score": 0.8255034841826178}\n')),(0,a.kt)("p",null,"Great! Our query returned the top 3 most similar products to our query, along with the score for each product."),(0,a.kt)("h3",{id:"stopping-the-job"},"Stopping the job"),(0,a.kt)("p",null,(0,a.kt)("inlineCode",{parentName:"p"},"pg_vectorize")," will continuously update the embeddings for your table. If you want to stop the job, you can do so by running:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sql"},"UPDATE cron.job\nSET active = false\nWHERE job_name = 'product_search';\n")),(0,a.kt)("p",null,"You can reenable the job by running:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sql"},"UPDATE cron.job\nSET active = true\nWHERE job_name = 'product_search';\n")),(0,a.kt)("h3",{id:"tuning-performance-with-indexes"},"Tuning performance with indexes"),(0,a.kt)("p",null,"When you have tens of thousands of rows, your query performance will improve by adding an index. See the ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/pgvector/pgvector#indexing"},"pgvector documentation")," to tune the index for your use case."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE INDEX ON products USING ivfflat (product_search_embeddings vector_cosine_ops) WITH (lists = 100);\n")))}u.isMDXComponent=!0}}]);